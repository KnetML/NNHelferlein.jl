<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · NNHelferlein.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="NNHelferlein.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">NNHelferlein.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Layers"><span>Layers</span></a></li><li><a class="tocitem" href="#Fully-connected-layers"><span>Fully connected layers</span></a></li><li><a class="tocitem" href="#Convolutional"><span>Convolutional</span></a></li><li><a class="tocitem" href="#Recurrent"><span>Recurrent</span></a></li><li><a class="tocitem" href="#Others"><span>Others</span></a></li><li class="toplevel"><a class="tocitem" href="#Data-providers"><span>Data providers</span></a></li><li><a class="tocitem" href="#Tabular-data"><span>Tabular data</span></a></li><li><a class="tocitem" href="#Image-data"><span>Image data</span></a></li><li class="toplevel"><a class="tocitem" href="#Training"><span>Training</span></a></li><li class="toplevel"><a class="tocitem" href="#Evaluation"><span>Evaluation</span></a></li><li class="toplevel"><a class="tocitem" href="#ImageNet-tools"><span>ImageNet tools</span></a></li><li class="toplevel"><a class="tocitem" href="#Other-utils"><span>Other utils</span></a></li></ul></li><li><a class="tocitem" href="../license/">License</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Chains"><a class="docs-heading-anchor" href="#Chains">Chains</a><a id="Chains-1"></a><a class="docs-heading-anchor-permalink" href="#Chains" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.NeuNet" href="#NNHelferlein.NeuNet"><code>NNHelferlein.NeuNet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">abstract type NeuNet end</code></pre><p>Mother type for DNN hierarchy with implementation for a chain of layers.</p><p><strong>Signatures:</strong></p><pre><code class="language-none">(n::NeuNet)(x) = (for l in n.layers; x = l(x); end; x)
(m::NeuNet)(d::Knet.Data) = mean( m(x,y) for (x,y) in d)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/nets.jl#L6-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Classifier" href="#NNHelferlein.Classifier"><code>NNHelferlein.Classifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct Classifier &lt;: NeuNet</code></pre><p>Classifyer with nll loss.</p><p><strong>Signatures:</strong></p><pre><code class="language-none">(m::Classifier)(x,y) = nll(m(x), y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/nets.jl#L21-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Regressor" href="#NNHelferlein.Regressor"><code>NNHelferlein.Regressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct Regressor</code></pre><p>Regression network with square loss.</p><p><strong>Signatures:</strong></p><pre><code class="language-none">(m::Regression)(x,y) = sum(abs2.( m(x) - y))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/nets.jl#L36-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Chain" href="#NNHelferlein.Chain"><code>NNHelferlein.Chain</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct Chain</code></pre><p>Simple wrapper tu chain layers afer each other.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/nets.jl#L51-L55">source</a></section></article><h1 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h1><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>Layer</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Fully-connected-layers"><a class="docs-heading-anchor" href="#Fully-connected-layers">Fully connected layers</a><a id="Fully-connected-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Fully-connected-layers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Dense" href="#NNHelferlein.Dense"><code>NNHelferlein.Dense</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct Dense  &lt;: Layer</code></pre><p>Default Dense layer.</p><p><strong>Constructors:</strong></p><ul><li><code>Dense(w, b, actf)</code>: default constructor</li><li><code>Dense(i::Int, j::Int; actf=sigm)</code>: layer of j neurons with       i inputs.</li><li><code>Dense(h5::HDF5.File, group::String; trainable=false, actf=sigm)</code>:</li><li><code>Dense(h5::HDF5.File, kernel::String, bias::String;       trainable=false, actf=sigm)</code>: layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/layers.jl#L15-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Embed" href="#NNHelferlein.Embed"><code>NNHelferlein.Embed</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct Embed &lt;: Layer</code></pre><p>Simple type for an embedding layer to embed a virtual onehot-vector into a smaller number of neurons by linear combination. The onehot-vector is virtual, because not the vector, but only the index of the &quot;one&quot; in the vector has to be provided as Integer value (or a minibatch of integers).</p><p><strong>Fields:</strong></p><ul><li>w</li><li>actf</li></ul><p><strong>Constructors:</strong></p><ul><li><code>Embed(i,j; actf=identity) = new(param(j,i), actf):</code> with   input size i, output size j and default activation function idendity.</li></ul><p><strong>Signatures:</strong></p><ul><li><code>(l::Embed)(x) = l.actf.(w[:,x])</code> default embedding of input vector x.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/layers.jl#L243-L263">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Predictions" href="#NNHelferlein.Predictions"><code>NNHelferlein.Predictions</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct Predictions &lt;: Layer</code></pre><p>Simple wrapper around a Dense layer without activation function that can used as output layer (because all loss-functions of the package assume raw output activations).</p><p><strong>Constructors:</strong></p><ul><li><code>Predictions(i::Int, j:Int)</code>: with   input size i, output size j activation function idendity.</li><li><code>Predictions(h5::HDF5.File, group::String; trainable=false)</code>:</li><li><code>Predictions(h5::HDF5.File, kernel::String, bias::String;              trainable=false)</code>: with   an hdf5-object and group name of the output layer.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/layers.jl#L272-L286">source</a></section></article><h2 id="Convolutional"><a class="docs-heading-anchor" href="#Convolutional">Convolutional</a><a id="Convolutional-1"></a><a class="docs-heading-anchor-permalink" href="#Convolutional" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Conv" href="#NNHelferlein.Conv"><code>NNHelferlein.Conv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct Conv  &lt;: Layer</code></pre><p>Default Conv layer.</p><p><strong>Constructors:</strong></p><ul><li><code>Conv(w, b, padding, actf)</code>: default constructor</li><li><code>Conv(w1::Int, w2::Int,  i::Int, o::Int; actf=relu; kwargs...)</code>: layer with   o kernels of size (w1,w2) for an input of i layers.</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>:</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>: layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>padding=0</code>: the number of extra zeros implicitly concatenated       at the start and end of each dimension.</li><li><code>stride=1</code>: the number of elements to slide to reach the next filtering window.</li><li><code>dilation=1</code>: dilation factor for each dimension.</li><li><code>...</code> See the Knet documentation for Details:       https://denizyuret.github.io/Knet.jl/latest/reference/#Convolution-and-Pooling.       All keywords to the Knet function <code>conv4()</code> are supported.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/layers.jl#L66-L88">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.DeConv" href="#NNHelferlein.DeConv"><code>NNHelferlein.DeConv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct DeConv  &lt;: Layer</code></pre><p>Default deconvolution layer.</p><p><strong>Constructors:</strong></p><ul><li><code>DeConv(w, b, actf, kwargs...)</code>: default constructor</li><li><code>Conv(w1::Int, w2::Int,  i::Int, o::Int; actf=relu, kwargs...)</code>: layer with   o kernels of size (w1,w2) for an input of i channels.</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>:</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>: layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>padding=0</code>: the number of extra zeros implicitly concatenated       at the start and end of each dimension (applied to the output).</li><li><code>stride=1</code>: the number of elements to slide to reach the next filtering window       (applied to the output).</li><li><code>...</code> See the Knet documentation for Details:       https://denizyuret.github.io/Knet.jl/latest/reference/#Convolution-and-Pooling.       All keywords to the Knet function <code>deconv4()</code> are supported.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/layers.jl#L157-L180">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>Pool</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.UnPool" href="#NNHelferlein.UnPool"><code>NNHelferlein.UnPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct UnPool &lt;: Layer</code></pre><p>Unpooling layer.</p><p><strong>Constructors:</strong></p><ul><li><code>UnPool(;kwargs...)</code>: user-defined unpooling</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/layers.jl#L196-L203">source</a></section></article><h2 id="Recurrent"><a class="docs-heading-anchor" href="#Recurrent">Recurrent</a><a id="Recurrent-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrent" title="Permalink"></a></h2><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>RSeqClassifier</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Others"><a class="docs-heading-anchor" href="#Others">Others</a><a id="Others-1"></a><a class="docs-heading-anchor-permalink" href="#Others" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Flat" href="#NNHelferlein.Flat"><code>NNHelferlein.Flat</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct Flat &lt;: Layer</code></pre><p>Default flatten layer.</p><p><strong>Constructors:</strong></p><ul><li><code>Flat()</code>: with no options.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/layers.jl#L211-L218">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.PyFlat" href="#NNHelferlein.PyFlat"><code>NNHelferlein.PyFlat</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct PyFlat &lt;: Layer</code></pre><p>Flatten layer with optional Python-stype flattening (row-major). This layer can be used if pre-trained weight matrices from tensorflow are applied after the flatten layer.</p><p><strong>Constructors:</strong></p><ul><li><code>PyFlat(; python=true)</code>: if true, row-major flatten is performed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/layers.jl#L225-L234">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Softmax" href="#NNHelferlein.Softmax"><code>NNHelferlein.Softmax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct Softmax &lt;: Layer</code></pre><p>Simple softmax layer to compute softmax probabilities.</p><p><strong>Constructors:</strong></p><ul><li><code>Softmax()</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/layers.jl#L298-L305">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Dropout" href="#NNHelferlein.Dropout"><code>NNHelferlein.Dropout</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct Dropout &lt;: Layer</code></pre><p>Dropout layer. Implemented with help of Knet&#39;s dropout() function that evaluates AutoGrad.recording() to detect if in training or inprediction. Dropouts are applied only if prediction.</p><p><strong>Constructors:</strong></p><ul><li><code>Dropout(p)</code> with the dropout rate <em>p</em>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/layers.jl#L311-L321">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.BatchNorm" href="#NNHelferlein.BatchNorm"><code>NNHelferlein.BatchNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct BatchNorm &lt;: Layer</code></pre><p>Batchnormalisation layer. Implemented with help of Knet&#39;s batchnorm() function that evaluates AutoGrad.recording() to detect if in training or in prediction. In training the moments are updated to record the running averages; in prediction the moments are applied, but not modified.</p><p><strong>Constructors:</strong></p><ul><li><code>Batchnom()</code> will initialise the moments with <code>Knet.bnmoments()</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/layers.jl#L329-L340">source</a></section></article><h1 id="Data-providers"><a class="docs-heading-anchor" href="#Data-providers">Data providers</a><a id="Data-providers-1"></a><a class="docs-heading-anchor-permalink" href="#Data-providers" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.DataLoader" href="#NNHelferlein.DataLoader"><code>NNHelferlein.DataLoader</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">abstract type DataLoader</code></pre><p>Mother type for minimatch iterators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/types.jl#L2-L6">source</a></section></article><h2 id="Tabular-data"><a class="docs-heading-anchor" href="#Tabular-data">Tabular data</a><a id="Tabular-data-1"></a><a class="docs-heading-anchor-permalink" href="#Tabular-data" title="Permalink"></a></h2><p>Tabular data is normally provided in table form (csv, ods) row-wise, i.e. one sample per row. The helper functions can read the tables and generate Knet compatible iterators of minibatches.</p><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.dataframe_read" href="#NNHelferlein.dataframe_read"><code>NNHelferlein.dataframe_read</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">dataframe_read(fname)</code></pre><p>Read a data table from an CSV-file with one sample per row and return a DataFrame with the data. (ODS-support is removed because of frequent PyCall compatibility issues of the OdsIO package).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/dataframes.jl#L5-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.dataframe_minibatches" href="#NNHelferlein.dataframe_minibatches"><code>NNHelferlein.dataframe_minibatches</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">dataframe_minibatches(data::DataFrames.DataFrame; size=256, ignore=[], teaching=:y, regression=true)</code></pre><p>Make Knet-conform minibatches from a dataframe with one sample per row.</p><ul><li><code>ignore</code>: defines a list of column names to be ignored</li><li><code>teaching</code>: defines the column with teaching input. Default is &quot;:y&quot;</li><li><code>regression</code>: if <code>true</code>, the teaching input is interpreted as               scalar value (converted to Float32); otherwise               teaching input is used as class labels (converted               to UInt8).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/dataframes.jl#L42-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.dataframe_split" href="#NNHelferlein.dataframe_split"><code>NNHelferlein.dataframe_split</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function dataframe_split(df::DataFrames.DataFrame; teaching=:y, fr=0.2, balanced=true)</code></pre><p>Split data, organised row-wise in a DataFrame into train and valid sets.</p><p><strong>Arguments:</strong></p><ul><li><code>df</code>: data</li><li><code>teaching</code>: name or index of column with teaching input (y)</li><li><code>fr</code>: fraction of data to be used for validation</li><li><code>balanced</code>: if <code>true</code>, result datasets will be balanced by oversampling.             Returned datasets will be bigger as expected             but include the same numbers of samples for each class.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/dataframes.jl#L73-L85">source</a></section></article><h2 id="Image-data"><a class="docs-heading-anchor" href="#Image-data">Image data</a><a id="Image-data-1"></a><a class="docs-heading-anchor-permalink" href="#Image-data" title="Permalink"></a></h2><p>Images as data should be provided in directories with the directory names denoting the class labels. The helpers read from the root of a directory tree in which the first level of sub-dirs tell the class label. All images in the tree under a class label are read as instances of the respective class. The following tree will generate the classes <code>daisy</code>, <code>rose</code> and <code>tulip</code>:</p><pre><code class="language-none">image_dir/
├── daisy
│   ├── 01
│   │   ├── 01
│   │   ├── 02
│   │   └── 03
│   ├── 02
│   │   ├── 01
│   │   └── 02
│   └── others
├── rose
│   ├── big
│   └── small
└── tulip</code></pre><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.ImageLoader" href="#NNHelferlein.ImageLoader"><code>NNHelferlein.ImageLoader</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct ImageLoader &lt;: DataLoader
    dir
    i_paths
    i_classes
    classes
    batchsize
    shuffle
    train
    aug_pipl
    pre_proc
    pre_load
    i_images
end</code></pre><p>Iterable image loader to provide minibatches of images as 4-d-arrays (x,y,rgb,mb).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/images.jl#L102-L119">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.mk_image_minibatch" href="#NNHelferlein.mk_image_minibatch"><code>NNHelferlein.mk_image_minibatch</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function mk_image_minibatch(dir, batchsize; split=false, fr=0.2,
                            balanced=false, shuffle=true, train=true,
                            aug_pipl=nothing, pre_proc=nothing)</code></pre><p>Return one or two iterable image-loader-objects that provides minibatches of images. For training each minibatch is a tupel <code>(x,y)</code> with x: 4-d-array with the minibatch of data and y: vector of class IDs as Int.</p><p><strong>Arguments:</strong></p><ul><li><code>dir</code>: base-directory of the image dataset. The first level of       sub-dirs are used as class names.</li><li><code>batchsize</code>: size of minibatches</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>split</code>: return two iterators for training and validation</li><li><code>fr</code>: split fraction</li><li><code>balanced</code>: return balanced data (i.e. same number of instances       for all classes). Balancing is achieved via oversampling</li><li><code>shuffle</code>: if true, shuffle the images everytime the iterator       restarts</li><li><code>train</code>: if true, minibatches with (x,y) Tuples are provided,       if false only x (for prediction)</li><li><code>pre_load</code>: if <code>true</code> all images are loaded in advance;       otherwise images are loaded on demand durng training.       (option is <em>not implemented yet!</em>)</li><li><code>aug_pipl</code>: augmentation pipeline for Augmentor.jl. Augmentation       is performed before the pre_proc-function is applied</li><li><code>pre_proc</code>: function with preprocessing       and augmentation algoritms of type x = f(x). In contrast       to the augmentation that modifies images, is <code>pre_proc</code>       working on Arrays{Float32}.</li><li><code>pre_load=false</code>: read all images from disk once when populating the       loader (requires loads of memory, but speeds up training).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/images.jl#L7-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.get_class_labels" href="#NNHelferlein.get_class_labels"><code>NNHelferlein.get_class_labels</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function get_class_labels(d::DataLoader)</code></pre><p>Extracts a list of class labels from a DataLoader.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/images.jl#L93-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.image2array" href="#NNHelferlein.image2array"><code>NNHelferlein.image2array</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function image2array(img)</code></pre><p>Take an image and return a 3d-array for RGB and a 2d-array for grayscale images with the colour channels as last dimension.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/images.jl#L324-L329">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.array2image" href="#NNHelferlein.array2image"><code>NNHelferlein.array2image</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function array2image(arr)</code></pre><p>Take a 3d-array with colour channels as last dimension or a 2d-array and return an array of RGB or of Gray as Image.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/images.jl#L348-L353">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.array2RGB" href="#NNHelferlein.array2RGB"><code>NNHelferlein.array2RGB</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function array2RGB(arr)</code></pre><p>Take a 3d-array with colour channels as last dimension or a 2d-array and return alwasy an array of RGB as Image.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/images.jl#L373-L378">source</a></section></article><h1 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.tb_train!" href="#NNHelferlein.tb_train!"><code>NNHelferlein.tb_train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function tb_train!(mdl, opti, trn, vld=nothing; epochs=1,
                  lr_decay=nothing, lrd_freq=1, l2=0.0,
                  eval_size=0.2, eval_freq=1,
                  acc_fun=nothing,
                  mb_loss_freq=100,
                  cp_freq=nothing, cp_dir=&quot;checkpoints&quot;,
                  tb_dir=&quot;logs&quot;, tb_name=&quot;run&quot;,
                  tb_text=&quot;&quot;&quot;Description of tb_train!() run.&quot;&quot;&quot;,
                  opti_args...)</code></pre><p>Train function with TensorBoard integration. TB logs are written with the TensorBoardLogger.jl package. The model is updated (in-place) and the trained model is returned.</p><p><strong>Arguments:</strong></p><ul><li><code>mdl</code>: model; i.e. forward-function for the net</li><li><code>opti</code>: Knet-stype optimiser iterator</li><li><code>trn</code>: training data; iterator to provide (x,y)-tuples with       minibatches</li><li><code>vld</code>: validation data; iterator to provide (x,y)-tuples with       minibatches. Set to <code>nothing</code>, if not defined.</li></ul><p><strong>Keyword arguments:</strong></p><p><strong>Optimiser:</strong></p><ul><li><code>epochs=1</code>: number of epochs to train</li><li><code>lr_decay=nothing</code>: Leraning rate decay if not <code>nothing</code>:       factor (&lt;1) to reduce the       lr every epoch as <code>lr  *= lr_decay</code>.</li><li><code>lrd_freq=1</code>: frequency of learning rate decay steps. Default is       to modify the lr after every epoch</li><li><code>l2=0.0</code>: L2 regularisation; implemented as weight decay per       parameter</li><li><code>opti_args...</code>: optional keyword arguments for the optimiser can be specified       (i.e. <code>lr</code>, <code>gamma</code>, ...).</li></ul><p><strong>Model evaluation:</strong></p><ul><li><code>eval_size=0.2</code>: fraction of validation data to be used for calculating       loss and accuracy for train and validation data during training.</li><li><code>eval_freq=1</code>: frequency of evaluation; default=1 means evaluation is       calculated after each epoch. With eval_freq=10 eveluation is       calculated 10 times per epoch.</li><li><code>acc_fun=nothing</code>: function to calculate accuracy. The function       is called with 2 arguments: <code>fun(predictions, teaching)</code> where       <code>predictions</code> is the output of a model call and a matrix and       <code>teaching</code> is the teaching input (y).       For classification tasks, <code>accuracy</code> from the Knet package is       a good choice. For regression a correlation or mean error       may be used (i.e. <code>acc_fun=(x,y)-&gt;sum(abs, x.-y)</code>).</li><li><code>mb_loss_freq=100</code>: frequency of training loss reporting. default=100       means that 100 loss-values per epoch will be logged to TensorBoard.       If mb<em>loss</em>freq is greater then the number of minibatches,       loss is logged for each minibatch.</li><li><code>cp_freq=nothing</code>: frequency of model checkpoints written to disk.       Default is <code>nothing</code>, i.e. no checkpoints are written.       To write the model after each epoch with       name <code>model</code> use freq=1; to write every 2 epochs freq=0.5.</li><li><code>cp_dir=&quot;checkpoints&quot;</code>: directory for checkpoints</li></ul><p><strong>TensorBoard:</strong></p><p>TensorBoard log-directory is created from 3 parts: <code>tb_dir/tb_name/&lt;current date time&gt;</code>.</p><ul><li><code>tb_dir=&quot;logs&quot;</code>: root directory for tensorborad logs.</li><li><code>tb_name=&quot;run&quot;</code>: name of training run. <code>tb_name</code> will be used as       directory name and should not include whitespace</li><li><code>tb_text</code>:  description       to be included in the TensorBoard log as <em>text</em> log.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/train.jl#L1-L69">source</a></section></article><h1 id="Evaluation"><a class="docs-heading-anchor" href="#Evaluation">Evaluation</a><a id="Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluation" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.predict" href="#NNHelferlein.predict"><code>NNHelferlein.predict</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function predict(mdl, x; softmax=false)</code></pre><p>Return the prediction for x.</p><p><strong>Arguments:</strong></p><p><code>mdl</code>: executable network model <code>x</code>: tensor, minibatch or iterator providing minibatches         of input data <code>softmax</code>: if true and if model is a <code>::Classifier</code> the prediction         softmax probabilities are rezrned instead of raw         activations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/train.jl#L365-L377">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.predict_top5" href="#NNHelferlein.predict_top5"><code>NNHelferlein.predict_top5</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function predict_top5(mdl, x; top_n=5, classes=nothing)</code></pre><p>Run the model mdl for data in x and prints the top 5 predictions as softmax probabilities.</p><p><strong>Arguments:</strong></p><p><code>top_n</code>: print top <em>n</em> hits instead of <em>5</em> <code>classes</code> may be a list of human readable class labels.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/train.jl#L334-L343">source</a></section></article><h1 id="ImageNet-tools"><a class="docs-heading-anchor" href="#ImageNet-tools">ImageNet tools</a><a id="ImageNet-tools-1"></a><a class="docs-heading-anchor-permalink" href="#ImageNet-tools" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.preproc_imagenet" href="#NNHelferlein.preproc_imagenet"><code>NNHelferlein.preproc_imagenet</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function preproc_imagenet(img)</code></pre><p>Image preprocessing for pre-trained ImageNet examples. Preprocessing includes</p><ul><li>bring RGB colour values into a range 0-255</li><li>standardise of colour values by substracting mean colour values   (103.939, 116.779, 123.68) from RGB</li><li>changing colour channel sequence from RGB to BGR</li></ul><p>Resize is <strong>not</strong> done, because this may be part of the augmentation pipeline.</p><p><strong>Examples:</strong></p><p>The function can be used with the image loader; for prediction with a trained model as:</p><pre><code class="language-julia">pipl = CropRatio(ratio=1.0) |&gt; Resize(224,224)
images = mk_image_minibatch(&quot;./example_pics&quot;, 16;
                    shuffle=false, train=false,
                    aug_pipl=pipl,
                    pre_proc=preproc_imagenet)</code></pre><p>And for training something like:</p><pre><code class="language-julia">pipl = Either(1=&gt;FlipX(), 1=&gt;FlipY(), 2=&gt;NoOp()) |&gt;
       Rotate(-5:5) |&gt;
       ShearX(-5:5) * ShearY(-5:5) |&gt;
       RCropSize(224,224)

dtrn, dvld = mk_image_minibatch(&quot;./example_pics&quot;, 16;
                    split=true, fr=0.2, balanced=false,
                    shuffle=true, train=true,
                    aug_pipl=pipl,
                    pre_proc=preproc_imagenet)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/imagenet.jl#L2-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.predict_imagenet" href="#NNHelferlein.predict_imagenet"><code>NNHelferlein.predict_imagenet</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function predict_imagenet(mdl, x; top_n=5)</code></pre><p>Predict the ImageNet-class of images from the predefined list of class labels.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/imagenet.jl#L77-L82">source</a></section></article><h1 id="Other-utils"><a class="docs-heading-anchor" href="#Other-utils">Other utils</a><a id="Other-utils-1"></a><a class="docs-heading-anchor-permalink" href="#Other-utils" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.crop_array" href="#NNHelferlein.crop_array"><code>NNHelferlein.crop_array</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function crop_array(x, crop_sizes)</code></pre><p>Crop a n-dimensional array to the given size. Cropping is always centered (i.e. a margin is removed).</p><p><strong>Arguments:</strong></p><ul><li><code>x</code>: n-dim AbstractArray</li><li><code>crop_sizes</code>: Tuple of target sizes to which the array is cropped.       Allowed values are Int or <code>:</code>. If <code>crop_sizes</code> defines less       dims as x has, the remaining dims will not be cropped (assuming <code>:</code>).       If a demanded crop size is bigger as the actual size of x,       it is ignored.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/util.jl#L23-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.init0" href="#NNHelferlein.init0"><code>NNHelferlein.init0</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function init0(siz)</code></pre><p>Initialise a vector or array of size <code>siz</code> with zeros. If a GPU is detected type of the returned value is <code>KnetArray{Float32}</code>, otherwise <code>Array{Float32}</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/6afec6abca9605ec89a1b044e97c32bd92f5a110/src/util.jl#L61-L67">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../license/">License »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 2 February 2021 08:25">Tuesday 2 February 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
