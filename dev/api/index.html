<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · NNHelferlein.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="NNHelferlein.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">NNHelferlein.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Layers"><span>Layers</span></a></li><li><a class="tocitem" href="#Fully-connected-layers"><span>Fully connected layers</span></a></li><li><a class="tocitem" href="#Convolutional"><span>Convolutional</span></a></li><li><a class="tocitem" href="#Recurrent"><span>Recurrent</span></a></li><li><a class="tocitem" href="#Others"><span>Others</span></a></li><li><a class="tocitem" href="#Attention-Mechanisms"><span>Attention Mechanisms</span></a></li><li><a class="tocitem" href="#Layers-for-transformers"><span>Layers for transformers</span></a></li><li class="toplevel"><a class="tocitem" href="#Data-providers"><span>Data providers</span></a></li><li><a class="tocitem" href="#Tabular-data"><span>Tabular data</span></a></li><li><a class="tocitem" href="#Image-data"><span>Image data</span></a></li><li><a class="tocitem" href="#Text-data"><span>Text data</span></a></li><li class="toplevel"><a class="tocitem" href="#Training"><span>Training</span></a></li><li class="toplevel"><a class="tocitem" href="#Evaluation"><span>Evaluation</span></a></li><li class="toplevel"><a class="tocitem" href="#ImageNet-tools"><span>ImageNet tools</span></a></li><li class="toplevel"><a class="tocitem" href="#Other-utils"><span>Other utils</span></a></li><li><a class="tocitem" href="#Utils-for-transformers"><span>Utils for transformers</span></a></li><li><a class="tocitem" href="#Utils-for-array-manipulation"><span>Utils for array manipulation</span></a></li><li><a class="tocitem" href="#Utils-for-fixing-types-in-GPU-context"><span>Utils for fixing types in GPU context</span></a></li></ul></li><li><a class="tocitem" href="../license/">License</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><p>API doc of all exported functions are listed here:</p><h1 id="Chains"><a class="docs-heading-anchor" href="#Chains">Chains</a><a id="Chains-1"></a><a class="docs-heading-anchor-permalink" href="#Chains" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.DNN" href="#NNHelferlein.DNN"><code>NNHelferlein.DNN</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">abstract type DNN end</code></pre><p>Mother type for DNN hierarchy with implementation for a chain of layers.</p><p><strong>Signatures:</strong></p><pre><code class="language-Julia hljs">(m::DNN)(x) = (for l in m.layers; x = l(x); end; x)
(m::DNN)(x,y) = m(x,y)
(m::DNN)(d::Knet.Data) = mean( m(x,y) for (x,y) in d)
(m::DNN)(d::Tuple) = mean( m(x,y) for (x,y) in d)
(m::DNN)(d::NNHelferlein.DataLoader) = mean( m(x,y) for (x,y) in d)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/nets.jl#L6-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.print_network" href="#NNHelferlein.print_network"><code>NNHelferlein.print_network</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function print_network(mdl::DNN)</code></pre><p>Print a network summary of any model of Type <code>DNN</code>. If the model has a field <code>layers</code>, the summary of all included layers will be printed recursively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/nets.jl#L83-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Classifier" href="#NNHelferlein.Classifier"><code>NNHelferlein.Classifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Classifier &lt;: DNN</code></pre><p>Classifyer with nll loss.</p><p><strong>Signatures:</strong></p><pre><code class="nohighlight hljs">(m::Classifier)(x,y) = nll(m(x), y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/nets.jl#L28-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Regressor" href="#NNHelferlein.Regressor"><code>NNHelferlein.Regressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Regressor</code></pre><p>Regression network with square loss.</p><p><strong>Signatures:</strong></p><pre><code class="nohighlight hljs">(m::Regression)(x,y) = sum(abs2.( m(x) - y))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/nets.jl#L46-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Chain" href="#NNHelferlein.Chain"><code>NNHelferlein.Chain</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Chain</code></pre><p>Simple wrapper to chain layers and execute them one after another.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/nets.jl#L63-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.VAE" href="#NNHelferlein.VAE"><code>NNHelferlein.VAE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct VAE</code></pre><p>Type for a generic variational autoencoder.</p><p><strong>Constructor:</strong></p><pre><code class="nohighlight hljs">VAE(encoder, decoder)</code></pre><p>Separate predefinded chains (ideally, but not necessarily of type <code>Chain</code>)  for encoder and decoder must be specified. The VAE needs the 2 parameters mean and variance to define the distribution of each code-neuron in the bottleneck-layer. In consequence the encoder outputmust be 2 times  the size of the decoder input (in case of dense layers: if encoder output is a 8-value vector, 4 codes are defined and the decoder input is a 4-value vector; in case of convolutional layers the number of encoder output channels must be 2 times the number of the encoder input channels - see the examples). </p><p><strong>Signatures:</strong></p><pre><code class="nohighlight hljs">(vae::VAE)(x)
(vae::VAE)(x,y)</code></pre><p>Called with one argument, predict will be executed;  with two arguments (args x and y should be identical for the autoencoder) the loss will be returned.    </p><p><strong>Details:</strong></p><p>The loss is calculated as the sum of element-wise error squares plus the <em>Kullback-Leibler-Divergence</em> to adapt the distributions of the bottleneck codes:</p><p class="math-container">\[\mathcal{L} = \frac{1}{2} \sum_{i=1}^{n_{outputs}} (t_{i}-o_{i})^{2} - 
               \frac{1}{2} \sum_{j=1}^{n_{codes}}(1 + ln\sigma_{c_j}^{2}-\mu_{c_j}^{2}-\sigma_{c_j}^{2}) \]</p><p>Output of the autoencoder is cropped to the size of input before loss calculation (and before prediction); i.e. the output has always the same dimensions as the input, even if the last layer generates a bigger shape.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/nets.jl#L138-L175">source</a></section></article><h1 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Layer" href="#NNHelferlein.Layer"><code>NNHelferlein.Layer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">abstract type Layer end</code></pre><p>Mother type for layers hierarchy.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L7-L11">source</a></section></article><h2 id="Fully-connected-layers"><a class="docs-heading-anchor" href="#Fully-connected-layers">Fully connected layers</a><a id="Fully-connected-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Fully-connected-layers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Dense" href="#NNHelferlein.Dense"><code>NNHelferlein.Dense</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Dense  &lt;: Layer</code></pre><p>Default Dense layer.</p><p><strong>Constructors:</strong></p><ul><li><code>Dense(w, b, actf)</code>: default constructor</li><li><code>Dense(i::Int, j::Int; actf=sigm)</code>: layer of j neurons with       i inputs.</li><li><code>Dense(h5::HDF5.File, group::String; trainable=false, actf=sigm)</code>:</li><li><code>Dense(h5::HDF5.File, kernel::String, bias::String;       trainable=false, actf=sigm)</code>: layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L16-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Linear" href="#NNHelferlein.Linear"><code>NNHelferlein.Linear</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Linear  &lt;: Layer</code></pre><p>Almost standard dense layer, but functionality inspired by the TensorFlow-layer:</p><ul><li>capable to work with input tensors of any number of dimensions</li><li>default activation function <code>indetity</code></li><li>optionally without biases.</li></ul><p>The shape of the input tensor is preserved; only the size of the first dim is changed from in to out.</p><p><strong>Constructors:</strong></p><ul><li><code>Linear(i::Int, j::Int; bias=true, actf=identity)</code> weher <code>i</code> is fan-in       and <code>j</code> is fan-out.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>bias=true</code>: if false biases are fixed to 0.0</li><li><code>actf=identity</code>: activation function.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L71-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Embed" href="#NNHelferlein.Embed"><code>NNHelferlein.Embed</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Embed &lt;: Layer</code></pre><p>Simple type for an embedding layer to embed a virtual onehot-vector into a smaller number of neurons by linear combination. The onehot-vector is virtual, because not the vector, but only the index of the &quot;one&quot; in the vector has to be provided as Integer value (or a minibatch of integers).</p><p><strong>Fields:</strong></p><ul><li>w</li><li>actf</li></ul><p><strong>Constructors:</strong></p><ul><li><code>Embed(v,d; actf=identity):</code> with   vocab size v, embedding depth d and default activation function idendity.</li></ul><p><strong>Signatures:</strong></p><ul><li><code>(l::Embed)(x) = l.actf.(w[:,x])</code> default embedding of input tensor x.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L366-L386">source</a></section></article><h2 id="Convolutional"><a class="docs-heading-anchor" href="#Convolutional">Convolutional</a><a id="Convolutional-1"></a><a class="docs-heading-anchor-permalink" href="#Convolutional" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Conv" href="#NNHelferlein.Conv"><code>NNHelferlein.Conv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Conv  &lt;: Layer</code></pre><p>Default Conv layer.</p><p><strong>Constructors:</strong></p><ul><li><code>Conv(w, b, padding, actf)</code>: default constructor</li><li><code>Conv(w1::Int, w2::Int,  i::Int, o::Int; actf=relu; kwargs...)</code>: layer with   o kernels of size (w1,w2) for an input of i layers.</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>:</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>: layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>padding=0</code>: the number of extra zeros implicitly concatenated       at the start and end of each dimension.</li><li><code>stride=1</code>: the number of elements to slide to reach the next filtering window.</li><li><code>dilation=1</code>: dilation factor for each dimension.</li><li><code>...</code> See the Knet documentation for Details:       https://denizyuret.github.io/Knet.jl/latest/reference/#Convolution-and-Pooling.       All keywords to the Knet function <code>conv4()</code> are supported.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L119-L141">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.DeConv" href="#NNHelferlein.DeConv"><code>NNHelferlein.DeConv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct DeConv  &lt;: Layer</code></pre><p>Default deconvolution layer.</p><p><strong>Constructors:</strong></p><ul><li><code>DeConv(w, b, actf, kwargs...)</code>: default constructor</li><li><code>Conv(w1::Int, w2::Int,  i::Int, o::Int; actf=relu, kwargs...)</code>: layer with   o kernels of size (w1,w2) for an input of i channels.</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>:</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>: layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>padding=0</code>: the number of extra zeros implicitly concatenated       at the start and end of each dimension (applied to the output).</li><li><code>stride=1</code>: the number of elements to slide to reach the next filtering window       (applied to the output).</li><li><code>...</code> See the Knet documentation for Details:       https://denizyuret.github.io/Knet.jl/latest/reference/#Convolution-and-Pooling.       All keywords to the Knet function <code>deconv4()</code> are supported.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L236-L259">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Pool" href="#NNHelferlein.Pool"><code>NNHelferlein.Pool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Pool &lt;: Layer</code></pre><p>Pooling layer.</p><p><strong>Constructors:</strong></p><ul><li><code>Pool(;kwargs...)</code>: max pooling; without kwargs, 2x2-pooling       is performed.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>window=2</code>: pooling window size (same for both directions)</li><li><code>...</code>: See the Knet documentation for Details:       https://denizyuret.github.io/Knet.jl/latest/reference/#Convolution-and-Pooling.       All keywords to the Knet function <code>pool</code> are supported.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L197-L211">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.UnPool" href="#NNHelferlein.UnPool"><code>NNHelferlein.UnPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct UnPool &lt;: Layer</code></pre><p>Unpooling layer.</p><p><strong>Constructors:</strong></p><ul><li><code>UnPool(;kwargs...)</code>: user-defined unpooling</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L288-L295">source</a></section></article><h2 id="Recurrent"><a class="docs-heading-anchor" href="#Recurrent">Recurrent</a><a id="Recurrent-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrent" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.RSeqClassifier" href="#NNHelferlein.RSeqClassifier"><code>NNHelferlein.RSeqClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct RSeqClassifier &lt;: Layer</code></pre><p>One layer RNN sequence classifyer that works with minimatches of (time) series data. minibatch can be a 2- or 3-dimensional Array. If 2-d, inputs for one step are in one column and the Array has as many colums as steps. If 3-d, the last dimension iterates the samples of the minibatch.</p><p>Result is always a 2-d matrix with the output of the units of the last step in each column and one column per sample of the minibatch.</p><p><strong>Constructors:</strong></p><ul><li><code>RSeqClassifer(n_inputs::Int, n_units::Int; u_type=:lstm, o...)</code>: with   number of inputs, number of units and unit type.   Internally the type <code>Knet.RNN</code> is used and all keyword arguments   of <code>Knet.RNN</code> may be provided.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L641-L658">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.RSeqTagger" href="#NNHelferlein.RSeqTagger"><code>NNHelferlein.RSeqTagger</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct RSeqTagger &lt;: Layer</code></pre><p>One layer RNN sequence classifyer that works with minimatches of (time) series data. minibatch can be a 2- or 3-dimensional Array. If 2-d, inputs for one step are in one column and the Array has as many colums as steps. If 3-d, the last dimension iterates the samples of the minibatch.</p><p>Result is an array matrix with the output of the units of all steps for all smaples of the minibatch (with model depth as first and samples of the minimatch as last dimension).</p><p><strong>Constructors:</strong></p><ul><li><code>RSeqTagger(n_inputs::Int, n_units::Int; u_type=:lstm, o...)</code>: with   number of inputs, number of units and unit type.   Internally the type <code>Knet.RNN</code> is used and all keyword arguments   of <code>Knet.RNN</code> may be provided.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L598-L615">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.hidden_states" href="#NNHelferlein.hidden_states"><code>NNHelferlein.hidden_states</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function hidden_states(l::&lt;RNN_Type&gt;)</code></pre><p>Return the hidden states of one or more layers of an RNN. <code>&lt;RNN_Type&gt;</code> is one of <code>RSeqClassifier</code>, <code>RSeqTagger</code>, <code>Knet.RNN</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L686-L692">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.cell_states" href="#NNHelferlein.cell_states"><code>NNHelferlein.cell_states</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function cell_states(l::&lt;RNN_Type&gt;)</code></pre><p>Return the cell states of one or more layers of an RNN only if it is a LSTM. <code>&lt;RNN_Type&gt;</code> is one of <code>RSeqClassifier</code>, <code>RSeqTagger</code>, <code>Knet.RNN</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L703-L710">source</a></section></article><h2 id="Others"><a class="docs-heading-anchor" href="#Others">Others</a><a id="Others-1"></a><a class="docs-heading-anchor-permalink" href="#Others" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Flat" href="#NNHelferlein.Flat"><code>NNHelferlein.Flat</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Flat &lt;: Layer</code></pre><p>Default flatten layer.</p><p><strong>Constructors:</strong></p><ul><li><code>Flat()</code>: with no options.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L316-L323">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.PyFlat" href="#NNHelferlein.PyFlat"><code>NNHelferlein.PyFlat</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct PyFlat &lt;: Layer</code></pre><p>Flatten layer with optional Python-stype flattening (row-major). This layer can be used if pre-trained weight matrices from tensorflow are applied after the flatten layer.</p><p><strong>Constructors:</strong></p><ul><li><code>PyFlat(; python=true)</code>: if true, row-major flatten is performed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L338-L347">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Softmax" href="#NNHelferlein.Softmax"><code>NNHelferlein.Softmax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Softmax &lt;: Layer</code></pre><p>Simple softmax layer to compute softmax probabilities.</p><p><strong>Constructors:</strong></p><ul><li><code>Softmax()</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L405-L412">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Dropout" href="#NNHelferlein.Dropout"><code>NNHelferlein.Dropout</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Dropout &lt;: Layer</code></pre><p>Dropout layer. Implemented with help of Knet&#39;s dropout() function that evaluates AutoGrad.recording() to detect if in training or inprediction. Dropouts are applied only if prediction.</p><p><strong>Constructors:</strong></p><ul><li><code>Dropout(p)</code> with the dropout rate <em>p</em>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L425-L435">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.BatchNorm" href="#NNHelferlein.BatchNorm"><code>NNHelferlein.BatchNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct BatchNorm &lt;: Layer</code></pre><p>Batchnormalisation layer. Implemented with help of Knet&#39;s batchnorm() function that evaluates AutoGrad.recording() to detect if in training or in prediction. In training the moments are updated to record the running averages; in prediction the moments are applied, but not modified.</p><p>In addition, optional trainable factor <code>a</code> and bias <code>b</code> are applied:</p><p class="math-container">\[y = a \cdot \frac{(x - \mu)}{(\sigma + \epsilon)} + b\]</p><p><strong>Constructors:</strong></p><ul><li><code>Batchnom(; trainable=false, channels=0)</code> will initialise       the moments with <code>Knet.bnmoments()</code> and       trainable parameters <code>a</code> and <code>b</code> only if       <code>trainable==true</code> (in this case, the number of channels must       be defined - for CNNs this is the number of feature maps).</li></ul><p><strong>Details:</strong></p><p>2d, 4d and 5d inputs are supported. Mean and variance are computed over dimensions (2), (1,2,4) and (1,2,3,5) for 2d, 4d and 5d arrays, respectively.</p><p>If <code>trainable=true</code> and <code>channels != 0</code>, trainable parameters <code>a</code> and <code>b</code> will be initialised for each channel.</p><p>If <code>trainable=true</code> and <code>channels == 0</code> (i.e. <code>Batchnom(trainable=true)</code>), the params <code>a</code> and <code>b</code> are not initialised by the constructor. Instead, the number of channels is inferred when the first minibatch is normalised as: 2d: <code>size(x)[1]</code> 4d: <code>size(x)[3]</code> 5d: <code>size(x)[4]</code> or <code>0</code> otherwise.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L451-L489">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.LayerNorm" href="#NNHelferlein.LayerNorm"><code>NNHelferlein.LayerNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct LayerNorm  &lt;: Layer</code></pre><p>Simple layer normalisation (inspired by TFs LayerNormalization). Implementation is from Deniz Yuret&#39;s answer to feature request 429 (https://github.com/denizyuret/Knet.jl/issues/492).</p><p>The layer performs a normalisation within each sample, <em>not</em> batchwise. Normalisation is modified by two trainable parameters <code>a</code> and <code>b</code> (variance and mean) added to every value of the sample vector.</p><p><strong>Constructors:</strong></p><ul><li><code>LayertNorm(depth; eps=1e-6)</code>:  <code>depth</code> is the number       of activations for one sample of the layer.</li></ul><p><strong>Signatures:</strong></p><ul><li><code>function (l::LayerNorm)(x; dims=1)</code>: normalise x along the given dimensions.       The size of the specified dimension must fit with the initialised <code>depth</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/layers.jl#L550-L569">source</a></section></article><h2 id="Attention-Mechanisms"><a class="docs-heading-anchor" href="#Attention-Mechanisms">Attention Mechanisms</a><a id="Attention-Mechanisms-1"></a><a class="docs-heading-anchor-permalink" href="#Attention-Mechanisms" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttentionMechanism" href="#NNHelferlein.AttentionMechanism"><code>NNHelferlein.AttentionMechanism</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">abstract type AttentionMechanism</code></pre><p>Attention mechanisms follow the same interface and common signatures.</p><p>If possible, the algorithm allows precomputing of the projections of the context vector generated by the encoder in a encoder-decoder-architecture (i.e. in case of an RNN encoder the accumulated encoder hidden states).</p><p>By default attention scores are scaled according to Vaswani et al., 2017 <em>(Vaswani et al., Attention Is All You Need, CoRR, 2017)</em>.</p><p>All algorithms use soft attention.</p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">Attn*Mechanism*(dec_units, enc_units; scale=true)
Attn*Mechanism*(units; scale=true)</code></pre><p>The one-argument version can be used, if encoder dimensions and decoder dimensions are the same.</p><p><strong>Common Signatures:</strong></p><pre><code class="nohighlight hljs">function (attn::AttentionMechanism)(h_t, h_enc; reset=false)
function (attn::AttentionMechanism)(; reset=false)</code></pre><p><strong>Arguments:</strong></p><ul><li><code>h_t</code>:    decoder hidden state. If <span>$h_t$</span> is a vector, its length           equals the number of decoder units. If it is a matrix,           <span>$h_t$</span> includes the states for a minibatch of samples and has           the size [units, mb].</li><li><code>h_enc</code>:  encoder hidden states, 2d or 3d. If <span>$h_{enc}$</span> is a           matrix [units, steps] with the hidden states of all encoder steps.           If 3d: [units, mb, steps] encoder states for all minibatches.</li><li><code>reset=false</code>: If the keyword argument is set to <code>true</code>, projections of           the encoder states are computed. By default projections are           stored in the object and reused until the object is resetted.           For attention mechanisms that don&#39;t allow precomputation           the argument is ignored.</li></ul><p>The short form <code>(::AttentionMechanism)(reset=true)</code> can be used to reset the precomputed projections.</p><p><strong>Return values</strong></p><p>All functions return <code>c</code> and <code>α</code> where α is a matrix of size [mb,steps] with the attention factors for each step and minibatch. <code>c</code> is a matrix of size [units, mb] with the context vector for each sample of the minibatch, calculated as the α-weighted sum of all encoder hidden states <span>$h_{enc}$</span> for each minibatch.</p><p><strong>Attention Mechanisms:</strong></p><p>All attention mechanisms calculate attention factors α from scores derived from projections of the encoder hidden states:</p><p class="math-container">\[\alpha = \mathrm{softmax}(\mathrm{score}(h_{enc},h_{t}) \cdot 1/\sqrt{n}))\]</p><p>Attention mechanisms implemented:</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/attn.jl#L3-L64">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttnBahdanau" href="#NNHelferlein.AttnBahdanau"><code>NNHelferlein.AttnBahdanau</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct AttnBahdanau &lt;: AttentionMechanism</code></pre><p>Bahdanau-style (additive, concat) attention mechanism according to the paper:</p><p><em>D. Bahdanau, KH. Co, Y. Bengio, Neural Machine Translation by jointlylearning to align and translate, ICLR, 2015</em>.</p><p class="math-container">\[\mathrm{score}(h_{t},h_{enc}) = v_{a}^{\top}\cdot\tanh(W[h_{t},h_{enc}])\]</p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">AttnBahdanau(dec_units, enc_units; scale=true)
AttnBahdanau(units; scale=true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/attn.jl#L76-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttnLuong" href="#NNHelferlein.AttnLuong"><code>NNHelferlein.AttnLuong</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct AttnLuong &lt;: AttentionMechanism</code></pre><p>Luong-style (multiplicative) attention mechanism according to the paper (referred as <em>General</em>-type attention): <em>M.-T. Luong, H. Pham, C.D. Manning, Effective Approaches to Attention-based Neural Machine Translation, CoRR, 2015</em>.</p><p class="math-container">\[\mathrm{score}(h_{t},h_{enc}) = h_{t}^{\top} W h_{enc}\]</p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">AttnLuong(dec_units, enc_units; scale=true)
AttnLuong(units; scale=true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/attn.jl#L138-L153">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttnDot" href="#NNHelferlein.AttnDot"><code>NNHelferlein.AttnDot</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct AttnDot &lt;: AttentionMechanism</code></pre><p>Dot-product attention (without trainable parameters) according to the Luong, et al. (2015) paper.</p><p><span>$\mathrm{score}(h_{t},h_{enc}) = h_{t}^{\top} h_{enc}$</span></p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">AttnDot(; scale=true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/attn.jl#L193-L203">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttnLocation" href="#NNHelferlein.AttnLocation"><code>NNHelferlein.AttnLocation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct AttnLocation &lt;: AttentionMechanism</code></pre><p>Location-based attention that only depends on the current decoder state <span>$h_t$</span> and not on the encoder states, according to the Luong, et al. (2015) paper.</p><p><span>$\mathrm{score}(h_{t}) = W h_{t}$</span></p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">AttnLocation(len, dec_units; scale=true)</code></pre><ul><li><code>len</code>: maximum sequence length of the encoder to be considered       for attention. If the actual length of <span>$h_{enc}$</span> is bigger as the       length of α, attention factors for the remaining states are set to       0.0. If the actual length of h_enc is smaller than α, only the matching       attention factors are applied.</li><li><code>dec_units</code>: number of decoder units.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/attn.jl#L233-L251">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttnInFeed" href="#NNHelferlein.AttnInFeed"><code>NNHelferlein.AttnInFeed</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct AttnInFeed &lt;: AttentionMechanism</code></pre><p>Input-feeding attention that depends on the current decoder state <span>$h_t$</span> and the next input to the decoder <span>$i_{t+1}$</span>, according to the Luong, et al. (2015) paper.</p><p>Infeed attention provides a semantic attention that depends on the next input token.</p><p><span>$\mathrm{score}(h_{t}, i_{t+1}) = W_h h_{t} + W_i i_{t+1} = W [h_t, i_{t+1}]$</span></p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">AttnInFeed(len, dec_units, fan_in; scale=true)</code></pre><ul><li><code>len</code>: maximum sequence length of the encoder to be considered       for attention. If the actual length of <span>$h_{enc}$</span> is bigger as the       length of α, attention factors for the remaining states are set to       0.0. If the actual length of h_enc is smaller than α, only the matching       attention factors are applied.</li><li><code>dec_units</code>: number of decoder units.</li><li><code>fan_in</code>: size of the decoder input.</li></ul><p><strong>Signature:</strong></p><pre><code class="nohighlight hljs">function (attn::AttnInFeed)(h_t, inp, h_enc)</code></pre><ul><li><code>h_t</code>:    decoder hidden state. If <span>$h_t$</span> is a vector, its length           equals the number of decoder units. If it is a matrix,           <span>$h_t$</span> includes the states for a minibatch of samples and has           the size [units, mb].</li><li><code>inp</code>: next decoder input <span>$i_{t+1}$</span>           (e.g. next embedded token of sequence)</li><li><code>h_enc</code>:  encoder hidden states, 2d or 3d. If <span>$h_{enc}$</span> is a           matrix [units, steps] with the hidden states of all encoder steps.           If 3d: [units, mb, steps] encoder states for all minibatches.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/attn.jl#L297-L333">source</a></section></article><h2 id="Layers-for-transformers"><a class="docs-heading-anchor" href="#Layers-for-transformers">Layers for transformers</a><a id="Layers-for-transformers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers-for-transformers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.PositionalEncoding" href="#NNHelferlein.PositionalEncoding"><code>NNHelferlein.PositionalEncoding</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct PositionalEncoding &lt;: Layer</code></pre><p>Positional encoding layer. Only <em>sincos</em>-style (according to Vaswani, et al., NIPS 2017) is implemented.</p><p>The layer takes an array of any any number of dimensions (&gt;=2), calculates the Vaswani-2017-style positional encoding and adds the encoding to each plane of the array.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/transformers.jl#L27-L36">source</a></section></article><h1 id="Data-providers"><a class="docs-heading-anchor" href="#Data-providers">Data providers</a><a id="Data-providers-1"></a><a class="docs-heading-anchor-permalink" href="#Data-providers" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.DataLoader" href="#NNHelferlein.DataLoader"><code>NNHelferlein.DataLoader</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">abstract type DataLoader</code></pre><p>Mother type for minibatch iterators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/types.jl#L2-L6">source</a></section></article><h2 id="Tabular-data"><a class="docs-heading-anchor" href="#Tabular-data">Tabular data</a><a id="Tabular-data-1"></a><a class="docs-heading-anchor-permalink" href="#Tabular-data" title="Permalink"></a></h2><p>Tabular data is normally provided in table form (csv, ods) row-wise, i.e. one sample per row. The helper functions can read the tables and generate Knet compatible iterators of minibatches.</p><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.dataframe_read" href="#NNHelferlein.dataframe_read"><code>NNHelferlein.dataframe_read</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dataframe_read(fname)</code></pre><p>Read a data table from an CSV-file with one sample per row and return a DataFrame with the data. (ODS-support is removed because of PyCall compatibility issues of the OdsIO package).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/dataframes.jl#L5-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.dataframe_minibatches" href="#NNHelferlein.dataframe_minibatches"><code>NNHelferlein.dataframe_minibatches</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dataframe_minibatches(data::DataFrames.DataFrame; size=256, ignore=[], teaching=&quot;y&quot;, o...)</code></pre><p>Make Knet-conform minibatches of type <code>Knet.data</code> from a dataframe with one sample per row.</p><p><strong>Arguments:</strong></p><ul><li><code>ignore</code>: defines a list of column names to be ignored</li><li><code>teaching=&quot;y&quot;</code>: defines the column name with teaching input. Default is &quot;y&quot;.               <code>teaching</code> is handled differently, depending on its type:               If <code>Int</code>, the teaching input is interpreted as               class ids and directly used for training (this assumes that               the values range from 1..n). If type is a String, values are               interpreted as class labels and convertet to numeric class IDs               by calling <code>mk_class_ids()</code>. The list of valid lables and their               order can be created by calling <code>mk_class_ids(data.y)[2]</code>.               If teaching is a scalar value, regression context is assumed,               and the value is used unchanged for training.</li><li>other keyword arguments: all keyword arguments accepted by               <code>Knet.minibatch()</code> may be used.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/dataframes.jl#L42-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.dataframe_split" href="#NNHelferlein.dataframe_split"><code>NNHelferlein.dataframe_split</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function dataframe_split(df::DataFrames.DataFrame;
                         teaching=&quot;y&quot;, fr=0.2, balanced=true)</code></pre><p>Split data, organised row-wise in a DataFrame into train and valid sets.</p><p><strong>Arguments:</strong></p><ul><li><code>df</code>: data</li><li><code>teaching=&quot;y&quot;</code>: name or index of column with teaching input (y)</li><li><code>fr=0.2</code>: fraction of data to be used for validation</li><li><code>shuffle=true</code>: shuffle the rows of the dataframe.</li><li><code>balanced=true</code>: if <code>true</code>, result datasets will be balanced by oversampling.             Returned datasets will be bigger as expected             but include the same numbers of samples for each class.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/dataframes.jl#L156-L170">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.mk_class_ids" href="#NNHelferlein.mk_class_ids"><code>NNHelferlein.mk_class_ids</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function mk_class_ids(labels)</code></pre><p>Take a list with n class labels for n instances and return a list of n class-IDs (of type Int) and an array of lables with the array index of each label corresponds its ID.</p><p><strong>Arguments:</strong></p><ul><li><code>labels</code>: List of labels (typically Strings)</li></ul><p><strong>Result values:</strong></p><ul><li>array of class-IDs in the same order as the input</li><li>array of unique class-IDs ordered by their ID.</li></ul><p><strong>Examples:</strong></p><pre><code class="nohighlight hljs">julia&gt; labels = [&quot;blue&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;blue&quot;]
7-element Array{String,1}:
 &quot;blue&quot;
 &quot;red&quot;
 &quot;red&quot;
 &quot;red&quot;
 &quot;green&quot;
 &quot;blue&quot;
 &quot;blue&quot;

julia&gt; mk_class_ids(labels)[1]
7-element Array{Int64,1}:
 1
 3
 3
 3
 2
 1
 1

 julia&gt; mk_class_ids(labels)[2]
3-element Array{String,1}:
 &quot;blue&quot;
 &quot;green&quot;
 &quot;red&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/dataframes.jl#L102-L146">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.MBNoiser" href="#NNHelferlein.MBNoiser"><code>NNHelferlein.MBNoiser</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">type MBNoiser</code></pre><p>Iterator to wrap any Knet.Data iterator of minibatches in  order to add random noise.     Each value will be multiplied with a random value form  Gaussian noise with mean=1.0 and sd=sigma.</p><p><strong>Construtors:</strong></p><pre><code class="nohighlight hljs">MBNoiser(mbs::Knet.Data, σ=1.0)</code></pre><ul><li><code>mbs</code>: iteraor with minibatches</li><li><code>σ</code>: standard deviation for the Gaussian noise</li></ul><p><strong>Example:</strong></p><pre><code class="nohighlight hljs">trn = minibatch(x)
tb_train!(mdl, Adam, MBNoiser(trn, σ=0.1))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/dataframes.jl#L189-L209">source</a></section></article><h2 id="Image-data"><a class="docs-heading-anchor" href="#Image-data">Image data</a><a id="Image-data-1"></a><a class="docs-heading-anchor-permalink" href="#Image-data" title="Permalink"></a></h2><p>Images as data should be provided in directories with the directory names denoting the class labels. The helpers read from the root of a directory tree in which the first level of sub-dirs tell the class label. All images in the tree under a class label are read as instances of the respective class. The following tree will generate the classes <code>daisy</code>, <code>rose</code> and <code>tulip</code>:</p><pre><code class="nohighlight hljs">image_dir/
├── daisy
│   ├── 01
│   │   ├── 01
│   │   ├── 02
│   │   └── 03
│   ├── 02
│   │   ├── 01
│   │   └── 02
│   └── others
├── rose
│   ├── big
│   └── small
└── tulip</code></pre><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.ImageLoader" href="#NNHelferlein.ImageLoader"><code>NNHelferlein.ImageLoader</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct ImageLoader &lt;: DataLoader
    dir
    i_paths
    i_classes
    classes
    batchsize
    shuffle
    train
    aug_pipl
    pre_proc
    pre_load
    i_images
end</code></pre><p>Iterable image loader to provide minibatches of images as 4-d-arrays (x,y,rgb,mb).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/images.jl#L103-L120">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.mk_image_minibatch" href="#NNHelferlein.mk_image_minibatch"><code>NNHelferlein.mk_image_minibatch</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function mk_image_minibatch(dir, batchsize; split=false, fr=0.2,
                            balanced=false, shuffle=true, train=true,
                            pre_load=false,
                            aug_pipl=nothing, pre_proc=nothing)</code></pre><p>Return one or two iterable image-loader-objects that provides minibatches of images. For training each minibatch is a tupel <code>(x,y)</code> with x: 4-d-array with the minibatch of data and y: vector of class IDs as Int.</p><p><strong>Arguments:</strong></p><ul><li><code>dir</code>: base-directory of the image dataset. The first level of       sub-dirs are used as class names.</li><li><code>batchsize</code>: size of minibatches</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>split</code>: return two iterators for training and validation</li><li><code>fr</code>: split fraction</li><li><code>balanced</code>: return balanced data (i.e. same number of instances       for all classes). Balancing is achieved via oversampling</li><li><code>shuffle</code>: if true, shuffle the images everytime the iterator       restarts</li><li><code>train</code>: if true, minibatches with (x,y) Tuples are provided,       if false only x (for prediction)</li><li><code>pre_load</code>: if <code>true</code> all images are loaded in advance;       otherwise images are loaded on demand durng training.       (option is <em>not implemented yet!</em>)</li><li><code>aug_pipl</code>: augmentation pipeline for Augmentor.jl. Augmentation       is performed before the pre_proc-function is applied</li><li><code>pre_proc</code>: function with preprocessing       and augmentation algoritms of type x = f(x). In contrast       to the augmentation that modifies images, is <code>pre_proc</code>       working on Arrays{Float32}.</li><li><code>pre_load=false</code>: read all images from disk once when populating the       loader (requires loads of memory, but speeds up training).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/images.jl#L7-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.get_class_labels" href="#NNHelferlein.get_class_labels"><code>NNHelferlein.get_class_labels</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function get_class_labels(d::DataLoader)</code></pre><p>Extracts a list of class labels from a DataLoader.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/images.jl#L94-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.image2array" href="#NNHelferlein.image2array"><code>NNHelferlein.image2array</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function image2array(img)</code></pre><p>Take an image and return a 3d-array for RGB and a 2d-array for grayscale images with the colour channels as last dimension.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/images.jl#L311-L316">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.array2image" href="#NNHelferlein.array2image"><code>NNHelferlein.array2image</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function array2image(arr)</code></pre><p>Take a 3d-array with colour channels as last dimension or a 2d-array and return an array of RGB or of Gray as Image.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/images.jl#L335-L340">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.array2RGB" href="#NNHelferlein.array2RGB"><code>NNHelferlein.array2RGB</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function array2RGB(arr)</code></pre><p>Take a 3d-array with colour channels as last dimension or a 2d-array and return always an array of RGB as Image.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/images.jl#L360-L365">source</a></section></article><h2 id="Text-data"><a class="docs-heading-anchor" href="#Text-data">Text data</a><a id="Text-data-1"></a><a class="docs-heading-anchor-permalink" href="#Text-data" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.WordTokenizer" href="#NNHelferlein.WordTokenizer"><code>NNHelferlein.WordTokenizer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct WordTokenizer
    len
    w2i
    i2w
end</code></pre><p>Create a word-based vocabulary: every unique word of a String or a list of Strings is assigned to a unique number. The created object includes a list of words (<code>i2w</code>, ordered by their numbers) and a dictionary <code>w2i</code> with the words as keys.</p><p><strong>Constructor:</strong></p><pre><code class="nohighlight hljs">function WordTokenizer(texts; len=nothing, add_ctls=true)</code></pre><p>With arguments:</p><ul><li><code>texts</code>: <code>AbstractArray</code> or iterable collection of <code>AbstractArray</code>s to be       analysed.</li><li><code>len=nothing</code>: maximum number of different words in the vocabulary.       Additional words in texts will be encoded as unknown. If <code>nothing</code>,       all words of the texts are included.</li><li><code>add_ctls=true</code>: if true, control words are added in front of the vocabulary       (extending the maximum length by 4): <code>&quot;&lt;start&gt;&quot;=&gt;1</code>, <code>&quot;&lt;end&gt;&quot;=&gt;2</code>,       <code>&quot;&lt;pad&gt;&quot;=&gt;3</code> and <code>&quot;&lt;unknown&gt;&quot;=&gt;4</code>.</li></ul><p><strong>Signatures:</strong></p><pre><code class="nohighlight hljs">function (t::WordTokenizer)(w::T; split_words=false, add_ctls=false)
                            where {T &lt;: AbstractString}</code></pre><p>Encode a word and return the corresponding number in the vocabulary or the highest number (i.e. <code>&quot;&lt;unknown&gt;&quot;</code>) if the word is not in the vocabulary.</p><p>The encode-signature accepts the keyword arguments <code>split_words</code> and <code>add_ctls</code>. If <code>split_words==true</code>, the input is treated as a sentence and splitted into single words and an array of integer with the encoded sequence is returned. If <code>add_ctls==true</code> the sequence will be framed by <code>&lt;start&gt;</code> and <code>&lt;end&gt;</code> tokens.</p><pre><code class="nohighlight hljs">function (t::WordTokenizer)(i::Int)</code></pre><p>Decode a word by returning the word corresponding to <code>i</code> or &quot;&lt;unknown&gt;&quot; if the number is out of range of the vocabulary.</p><pre><code class="nohighlight hljs">function (t::WordTokenizer)(s::AbstractArray{T}; add_ctls=false)
                           where {T &lt;: AbstractString}</code></pre><p>Called with an Array of Strings the tokeniser splits the strings into words and returns an Array of <code>Array{Int}</code> with each of the input strings represented by a sequence of Integer values.</p><pre><code class="nohighlight hljs">function (t::WordTokenizer)(seq::AbstractArray{T}; add_ctls=false)
                                 where {T &lt;: Int}</code></pre><p>Called with an Array of Integer values a single string  is returned with the decoded token-IDs as words (space-separated).</p><p><strong>Base Signatures:</strong></p><pre><code class="nohighlight hljs">    function length(t::WordTokenizer)</code></pre><p>Return the length of the vocab.</p><p><strong>Examples:</strong></p><pre><code class="nohighlight hljs">julia&gt; vocab = WordTokenizer([&quot;I love Julia&quot;, &quot;They love Python&quot;]);
Julia&gt; vocab(8)
&quot;Julia&quot;

julia&gt; vocab(&quot;love&quot;)
5

julia&gt; vocab.(split(&quot;I love Julia&quot;))
3-element Array{Int64,1}:
 5
 6
 8

julia&gt; vocab.i2w
9-element Array{String,1}:
 &quot;&lt;start&gt;&quot;
 &quot;&lt;end&gt;&quot;
 &quot;&lt;pad&gt;&quot;
 &quot;&lt;unknown&gt;&quot;
 &quot;love&quot;
 &quot;I&quot;
 &quot;They&quot;
 &quot;Julia&quot;
 &quot;Python&quot;

julia&gt; vocab.w2i
Dict{String,Int64} with 9 entries:
  &quot;I&quot;         =&gt; 6
  &quot;&lt;end&gt;&quot;     =&gt; 2
  &quot;&lt;pad&gt;&quot;     =&gt; 3
  &quot;They&quot;      =&gt; 7
  &quot;Julia&quot;     =&gt; 8
  &quot;love&quot;      =&gt; 5
  &quot;Python&quot;    =&gt; 9
  &quot;&lt;start&gt;&quot;   =&gt; 1
  &quot;&lt;unknown&gt;&quot; =&gt; 4

julia&gt; vocab.([7,5,8])
3-element Array{String,1}:
 &quot;They&quot;
 &quot;love&quot;
 &quot;Julia

julia&gt; vocab.(&quot;I love Scala&quot;, split_words=true)
3-element Array{Int64,1}:
 6
 5
 4

julia&gt; vocab.([6,5,4])
3-element Array{String,1}:
 &quot;I&quot;
 &quot;love&quot;
 &quot;&lt;unknown&gt;&quot;

julia&gt; vocab(&quot;I love Python&quot;, split_words=true, add_ctls=true)
5-element Array{Int64,1}:
 1
 6
 5
 9
 2

julia&gt; vocab([&quot;They love Julia&quot;, &quot;I love Julia&quot;])
2-element Array{Array{Int64,1},1}:
 [7, 5, 8]
 [6, 5, 8]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/texts.jl#L5-L142">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.get_tatoeba_corpus" href="#NNHelferlein.get_tatoeba_corpus"><code>NNHelferlein.get_tatoeba_corpus</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function get_tatoeba_corpus(lang; force=false,
            url=&quot;https://www.manythings.org/anki/&quot;)</code></pre><p>Download and read a bilingual text corpus from Tatoeba (privided) by ManyThings (https://www.manythings.org). All corpi are English-<em>Language</em>-pairs with different size and quality. Considerable languages include:</p><ul><li><code>fra</code>: French-English, 180 000 sentences</li><li><code>deu</code>: German-English, 227 000 sentences</li><li><code>heb</code>: Hebrew-English, 126 000 sentences</li><li><code>por</code>: Portuguese-English, 170 000 sentences</li><li><code>tur</code>: Turkish-English, 514 000 sentences</li></ul><p>The function returns two lists with corresponding sentences in both languages. Sentences are are <em>not</em> processed/normalised/cleaned, but exactly as provided by Tatoeba.</p><p>The data is stored in the package directory and only downloaded once.</p><p><strong>Arguments:</strong></p><ul><li><code>lang</code>: languagecode</li><li><code>force=false</code>: if <code>true</code>, the corpus is downloaded even if       a data file is already saved.</li><li><code>url</code>: base url of ManyThings.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/texts.jl#L276-L301">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.seq_minibatch" href="#NNHelferlein.seq_minibatch"><code>NNHelferlein.seq_minibatch</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function seq_minibatch(x, [y,] batchsize; seq_len=nothing, pad=3, o...)</code></pre><p>Return an iterator of type <code>Knet.Data</code> with sequence minibatches from a list of sequences. all keyword args of <a href="https://denizyuret.github.io/Knet.jl/latest/reference/#Knet.Train20.minibatch"><code>Knet.minibatch()</code></a> can be used.</p><p>All sequences in x are brought to the same length by truncating (if too long) or padding with the token provided as <code>pad</code>.</p><p>If <code>y</code> is defined, the minibatches include the sequences for x and training targets <code>y</code>, given as n-dimensional array (as for <code>Knet.minibach()</code>). For sequence-2-sequence minibatches the function <code>seq2seq_minibatch()</code> must be used.</p><p><strong>Arguments:</strong></p><ul><li><code>x</code>: An iterable object of sequences.</li><li><code>y</code>: vector or array with training targets</li><li><code>batchsize</code>: size of minibatches</li><li><code>seq_len=nothing</code>: demanded length of sequences in the minibatches.       If <code>nothing</code>, all sequences are padded to match with the longest       sequence.</li><li><code>pad=3</code>: token, used for padding. The default (3) is the token set by       the <code>WordRTokenizer</code>. The token must be compatible       with the type of the sequence elements.</li><li><code>o...</code>: any other keyword arguments of <code>Knet.minibatch()</code>, such as       <code>shuffle=true</code> or <code>partial=true</code> can be provided.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/texts.jl#L355-L383">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.seq2seq_minibatch" href="#NNHelferlein.seq2seq_minibatch"><code>NNHelferlein.seq2seq_minibatch</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function seq2seq_minibatch(x, y, batchsize; seq_len=nothing,
            pad_x=3, pad_y=3, o...)</code></pre><p>Return an iterator of type <code>Knet.Data</code> with (x,y) sequence minibatches from two lists of sequences. all keyword args of <a href="https://denizyuret.github.io/Knet.jl/latest/reference/#Knet.Train20.minibatch"><code>Knet.minibatch()</code></a> can be used.</p><p>All sequences in x and y are brought to the same length by truncating (if too long) or padding with the token provided as <code>pad</code>.</p><p><strong>Arguments:</strong></p><ul><li><code>x</code>: An iterable object of sequences.</li><li><code>y</code>: An iterable object of target sequences.</li><li><code>batchsize</code>: size of minibatches</li><li><code>seq_len=nothing</code>: demanded length of sequences in the minibatches.       If <code>nothing</code>, all sequences are padded to match with the longest       sequence.</li><li><code>pad_x=3</code>,</li><li><code>pad_y=3</code>: token, used for padding. The token must be compatible       with the type of the sequence elements.</li><li><code>o...</code>: any other keyword arguments of <code>Knet.minibatch()</code>, such as       <code>shuffle=true</code> or <code>partial=true</code> can be provided.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/texts.jl#L406-L431">source</a></section></article><h1 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.tb_train!" href="#NNHelferlein.tb_train!"><code>NNHelferlein.tb_train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function tb_train!(mdl, opti, trn, vld=nothing; epochs=1,
                  lr_decay=nothing, lrd_steps=5, lrd_linear=false,
                  l2=0.0,
                  eval_size=0.2, eval_freq=1,
                  acc_fun=nothing,
                  mb_loss_freq=100,
                  cp_freq=nothing, cp_dir=&quot;checkpoints&quot;,
                  tb_dir=&quot;logs&quot;, tb_name=&quot;run&quot;,
                  tb_text=&quot;&quot;&quot;Description of tb_train!() run.&quot;&quot;&quot;,
                  opti_args...)</code></pre><p>Train function with TensorBoard integration. TB logs are written with the TensorBoardLogger.jl package. The model is updated (in-place) and the trained model is returned.</p><p><strong>Arguments:</strong></p><ul><li><code>mdl</code>: model; i.e. forward-function for the net</li><li><code>opti</code>: Knet-stype optimiser type</li><li><code>trn</code>: training data; iterator to provide (x,y)-tuples with       minibatches</li><li><code>vld</code>: validation data; iterator to provide (x,y)-tuples with       minibatches. Set to <code>nothing</code>, if not defined.</li></ul><p><strong>Keyword arguments:</strong></p><p><strong>Optimiser:</strong></p><ul><li><code>epochs=1</code>: number of epochs to train</li><li><code>lr_decay=nothing</code>: do a leraning rate decay if not <code>nothing</code>:       the value given is the final learning rate after <code>lrd_steps</code>       steps of decay (<code>lr_decay</code> may be bigger than <code>lr</code>; in this case       the leraning rate is increased).        <code>lr_decay</code> is only applied if both start learning rate       <code>lr</code> and final learning rate <code>lr_decay</code> are defined explicitly.       Example: <code>lr=0.01, lr_decay=0.001</code> will reduce the lr from       0.01 to 0.001 during the training (by default in 5 steps).</li><li><code>lrd_steps=5</code>: number of learning rate decay steps. Default is       to modify the lr 5 times during the training.</li><li><code>lrd_linear=false</code>: type of learning rate decay;       If <code>false</code>, lr is modified       by a constant factor (e.g. 0.9) resulting in an exponential decay.       If <code>true</code>, lr is modified by the same step size, i.e. linearly.</li><li><code>l2=0.0</code>: L2 regularisation; implemented as weight decay per       parameter</li><li><code>opti_args...</code>: optional keyword arguments for the optimiser can be specified       (i.e. <code>lr</code>, <code>gamma</code>, ...).</li></ul><p><strong>Model evaluation:</strong></p><ul><li><code>eval_size=0.2</code>: fraction of validation data to be used for calculating       loss and accuracy for train and validation data during training.</li><li><code>eval_freq=1</code>: frequency of evaluation; default=1 means evaluation is       calculated after each epoch. With eval_freq=10 eveluation is       calculated 10 times per epoch.</li><li><code>acc_fun=nothing</code>: function to calculate accuracy. The function       must implement the following signature: <code>fun(model; data)</code> where       data is an iterator that provides (x,y)-tuples of minibatches.       For classification tasks, <code>accuracy</code> from the Knet package is       a good choice. For regression a correlation or mean error       may be preferred.</li><li><code>mb_loss_freq=100</code>: frequency of training loss reporting. default=100       means that 100 loss-values per epoch will be logged to TensorBoard.       If mb<em>loss</em>freq is greater then the number of minibatches,       loss is logged for each minibatch.</li><li><code>cp_freq=nothing</code>: frequency of model checkpoints written to disk.       Default is <code>nothing</code>, i.e. no checkpoints are written.       To write the model after each epoch with       name <code>model</code> use freq=1; to write every 2 epochs freq=0.5.</li><li><code>cp_dir=&quot;checkpoints&quot;</code>: directory for checkpoints</li></ul><p><strong>TensorBoard:</strong></p><p>TensorBoard log-directory is created from 3 parts: <code>tb_dir/tb_name/&lt;current date time&gt;</code>.</p><ul><li><code>tb_dir=&quot;logs&quot;</code>: root directory for tensorborad logs.</li><li><code>tb_name=&quot;run&quot;</code>: name of training run. <code>tb_name</code> will be used as       directory name and should not include whitespace</li><li><code>tb_text</code>:  description       to be included in the TensorBoard log as <em>text</em> log.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/train.jl#L1-L78">source</a></section></article><h1 id="Evaluation"><a class="docs-heading-anchor" href="#Evaluation">Evaluation</a><a id="Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluation" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.predict" href="#NNHelferlein.predict"><code>NNHelferlein.predict</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function predict(mdl, x; softmax=false)</code></pre><p>Return the prediction for x.</p><p><strong>Arguments:</strong></p><ul><li><code>mdl</code>: executable network model</li><li><code>x</code>: iterator providing minibatches       of input data</li><li><code>softmax</code>: if true or if model is of type <code>Classifier</code> the predicted       softmax probabilities are returned instead of raw       activations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/train.jl#L421-L433">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.predict_top5" href="#NNHelferlein.predict_top5"><code>NNHelferlein.predict_top5</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function predict_top5(mdl, x; top_n=5, classes=nothing)</code></pre><p>Run the model <code>mdl</code> for data in <code>x</code> and print the top 5 predictions as softmax probabilities.</p><p><strong>Arguments:</strong></p><ul><li><code>top_n</code>: print top <em>n</em> hits</li><li><code>classes</code>: optional list of human readable class labels.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/train.jl#L390-L399">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.hamming_dist" href="#NNHelferlein.hamming_dist"><code>NNHelferlein.hamming_dist</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function hamming_dist(p, t; accuracy=false, 
                            ignore_ctls=false, vocab=nothing, 
                            start=nothing, stop=nothing, pad=nothing, unk=nothing)


function hamming_acc(p, t; o...)


function hamming_acc(mdl; data=data, o...)</code></pre><p>Return the Hamming distance between two sequences or two minibatches of sequences. Predicted sequences <code>p</code> and teaching input sequences <code>t</code> may be of different length but the number of sequences in the minibatch must be the same.</p><p><strong>Arguments:</strong></p><ul><li><code>p</code>, <code>t</code>: n-dimensional arrays of type <code>Int</code> with predictions       and teaching input for a minibatch of sequences.       Shape of the arrays must be identical except of the first dimension       (i.e. the sequence length) that may differ between <code>p</code> and <code>t</code>.</li><li><code>accuracy=false</code>: if <code>false</code>, the mean Hamming distance in the minibatch       is returned (i.e. the average number of differences in the sequences).       If <code>true</code>, the accuracy is returned       for all not padded positions in a range (0.0 - 1.0).</li><li><code>ignore_ctls=false</code>: a vocab is used to replace all &#39;&lt;start&gt;, &lt;end&gt;, &lt;unknwon&gt;, &lt;pad&gt;&#39;       tokens by <code>&lt;pad&gt;</code>. If true, padding and other control tokens are treated as       normal codes and are not ignored.</li><li><code>vocab=nothing</code>: target laguage vocabulary of type <code>NNHelferlein.WordTokenizer</code>.       If defined,       the padding token of <code>vocab</code> is used to mask all control tokens in the       sequences (i.e. &#39;&lt;start&gt;, &lt;end&gt;, &lt;unknwon&gt;, &lt;pad&gt;&#39;).</li><li><code>start, stop, pad, unk</code>: may be used to define individual control tokens.       default is <code>nothing</code>.</li></ul><p><strong>Details:</strong></p><p>The function <code>hamming_acc()</code> is a shortcut to return the accuracy instead of the distance. The signature <code>hamming_acc(mdl; data=data; o...)</code> is for compatibility with acc functions called by train.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/train.jl#L452-L494">source</a></section></article><h1 id="ImageNet-tools"><a class="docs-heading-anchor" href="#ImageNet-tools">ImageNet tools</a><a id="ImageNet-tools-1"></a><a class="docs-heading-anchor-permalink" href="#ImageNet-tools" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.preproc_imagenet" href="#NNHelferlein.preproc_imagenet"><code>NNHelferlein.preproc_imagenet</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function preproc_imagenet(img)</code></pre><p>Image preprocessing for pre-trained ImageNet examples. Preprocessing includes</p><ul><li>bring RGB colour values into a range 0-255</li><li>standardise of colour values by substracting mean colour values   (103.939, 116.779, 123.68) from RGB</li><li>changing colour channel sequence from RGB to BGR</li></ul><p>Resize is <strong>not</strong> done, because this may be part of the augmentation pipeline.</p><p><strong>Examples:</strong></p><p>The function can be used with the image loader; for prediction with a trained model as:</p><pre><code class="language-julia hljs">pipl = CropRatio(ratio=1.0) |&gt; Resize(224,224)
images = mk_image_minibatch(&quot;./example_pics&quot;, 16;
                    shuffle=false, train=false,
                    aug_pipl=pipl,
                    pre_proc=preproc_imagenet)</code></pre><p>And for training something like:</p><pre><code class="language-julia hljs">pipl = Either(1=&gt;FlipX(), 1=&gt;FlipY(), 2=&gt;NoOp()) |&gt;
       Rotate(-5:5) |&gt;
       ShearX(-5:5) * ShearY(-5:5) |&gt;
       RCropSize(224,224)

dtrn, dvld = mk_image_minibatch(&quot;./example_pics&quot;, 16;
                    split=true, fr=0.2, balanced=false,
                    shuffle=true, train=true,
                    aug_pipl=pipl,
                    pre_proc=preproc_imagenet)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/imagenet.jl#L2-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.predict_imagenet" href="#NNHelferlein.predict_imagenet"><code>NNHelferlein.predict_imagenet</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function predict_imagenet(mdl, x; top_n=5)</code></pre><p>Predict the ImageNet-class of images from the predefined list of class labels.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/imagenet.jl#L77-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.get_imagenet_classes" href="#NNHelferlein.get_imagenet_classes"><code>NNHelferlein.get_imagenet_classes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function get_imagenet_classes()</code></pre><p>Return a list of all 1000 ImageNet class labels.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/imagenet.jl#L53-L57">source</a></section></article><h1 id="Other-utils"><a class="docs-heading-anchor" href="#Other-utils">Other utils</a><a id="Other-utils-1"></a><a class="docs-heading-anchor-permalink" href="#Other-utils" title="Permalink"></a></h1><h2 id="Utils-for-transformers"><a class="docs-heading-anchor" href="#Utils-for-transformers">Utils for transformers</a><a id="Utils-for-transformers-1"></a><a class="docs-heading-anchor-permalink" href="#Utils-for-transformers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.positional_encoding_sincos" href="#NNHelferlein.positional_encoding_sincos"><code>NNHelferlein.positional_encoding_sincos</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function positional_encoding_sincos(n_embed, n_seq)</code></pre><p>Calculate and return a matrix of size <code>[n_embed, n_seq]</code> of positional encoding values following the sin and cos style in the paper <em>Vaswani, A. et al.; Attention Is All You Need; 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA, 2017.</em></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/transformers.jl#L7-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.mk_padding_mask" href="#NNHelferlein.mk_padding_mask"><code>NNHelferlein.mk_padding_mask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function mk_padding_mask(x; pad=3)</code></pre><p>Make a padding mask; i.e. return an Array of type <code>KnetArray{Float32}</code> (or <code>Array{Float32}</code>) similar to <code>x</code> but with two additional dimension of size 1 in teh middle (this will represent the 2nd seq_len and the number of heads) in multi-head attention and the value <code>1.0</code> at each position where <code>x</code> is <code>pad</code> and <code>0.0</code> otherwise.</p><p>The function can be used for creating padding masks for attention mechanisms.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/transformers.jl#L51-L63">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.mk_peek_ahead_mask" href="#NNHelferlein.mk_peek_ahead_mask"><code>NNHelferlein.mk_peek_ahead_mask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function mk_peek_ahead_mask(x; dim=1)</code></pre><p>Return a matrix of size <code>[n_seq, n_seq]</code> filled with 1.0 and the <em>uppper triangle</em> set to 0.0. Type is <code>KnetArray{Float32}</code> in GPU context, <code>Array{Float32}</code> otherwise. The matrix can be used as peek-ahead mask in transformers.</p><p><code>dim=1</code> specifies the dimension in which the sequence length is represented. For un-embedded data this is normally <code>1</code>, i.e. the shape of x is [n<em>seq, n</em>mb]. After embedding the shape probably is [depth, n<em>seq, n</em>mb].</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/transformers.jl#L70-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.dot_prod_attn" href="#NNHelferlein.dot_prod_attn"><code>NNHelferlein.dot_prod_attn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function dot_prod_attn(q, k, v; mask=nothing)</code></pre><p>Generic scaled dot product attention following the paper of Vaswani et al., (2017), <em>Attention Is All You Need</em>.</p><p><strong>Arguments:</strong></p><ul><li><code>q</code>: query of size <code>[depth, n_seq_q, ...]</code></li><li><code>k</code>: key of size <code>[depth, n_seq_v, ...]</code></li><li><code>v</code>: value of size <code>[depth, n_seq_v, ...]</code></li><li><code>mask</code>: mask for attention factors may have different shapes but must be       broadcastable for addition to the scores tensor (which as the same size as       alpha <code>[n_seq_v, n_seq_q, ...]</code>). In transformer context typical masks are one of:       padding mask of size <code>[n_seq_v, ...]</code> or a peek-ahead mask of size <code>[n_seq_v, n_seq_v]</code>       (which is only possible in case of self-attention when all seqencee lengths       are identical).</li></ul><p><code>q, k, v</code> must have matching leading dimensions (i.e. same depth or embedding). <code>k</code> and <code>v</code> must have the same sequence length.</p><p><strong>Return values:</strong></p><ul><li><code>c</code>: context as alpha-weighted sum of values with size [depth, n<em>seq</em>v, ...]</li><li><code>alpha</code>: attention factors of size [n<em>seq</em>v, n<em>seq</em>q, ...]</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/transformers.jl#L91-L114">source</a></section></article><h2 id="Utils-for-array-manipulation"><a class="docs-heading-anchor" href="#Utils-for-array-manipulation">Utils for array manipulation</a><a id="Utils-for-array-manipulation-1"></a><a class="docs-heading-anchor-permalink" href="#Utils-for-array-manipulation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.crop_array" href="#NNHelferlein.crop_array"><code>NNHelferlein.crop_array</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function crop_array(x, crop_sizes)</code></pre><p>Crop a n-dimensional array to the given size. Cropping is always centered (i.e. a margin is removed).</p><p><strong>Arguments:</strong></p><ul><li><code>x</code>: n-dim AbstractArray</li><li><code>crop_sizes</code>: Tuple of target sizes to which the array is cropped.       Allowed values are Int or <code>:</code>. If <code>crop_sizes</code> defines less       dims as x has, the remaining dims will not be cropped (assuming <code>:</code>).       If a demanded crop size is bigger as the actual size of x,       it is ignored.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/util.jl#L23-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.blowup_array" href="#NNHelferlein.blowup_array"><code>NNHelferlein.blowup_array</code></a> — <span class="docstring-category">Function</span></header><section><div><p>function blowup_array(x, n)</p><p>Blow up an array <code>x</code> with an additional dimension and repeat the content of the array <code>n</code> times.</p><p><strong>Arguments:</strong></p><ul><li><code>x</code>: Array of any dimension</li><li><code>n</code>: number of repeats. ´n == 1´ will return an</li></ul><p>array with an addritional dimension of size 1.</p><p><strong>Examples:</strong></p><pre><code class="language-Julia hljs">julia&gt; x = [1,2,3,4]; blowup_array(x, 3)
4×3 Array{Int64,2}:
 1  1  1
 2  2  2
 3  3  3
 4  4  4

julia&gt; x = [1 2; 3 4]; blowup_array(x, 3)
2×2×3 Array{Int64,3}:
[:, :, 1] =
 1  2
 3  4

[:, :, 2] =
 1  2
 3  4

[:, :, 3] =
 1  2
 3  4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/util.jl#L119-L155">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.recycle_array" href="#NNHelferlein.recycle_array"><code>NNHelferlein.recycle_array</code></a> — <span class="docstring-category">Function</span></header><section><div><p>function recycle_array(x, n)</p><p>Recycle an array <code>x</code> along the last dimension and repeat the content of the array <code>n</code> times. The number of dims stays unchanged, but the array valueas are repeated <code>n</code> times.</p><p><strong>Arguments:</strong></p><ul><li><code>x</code>: Array of any dimension</li><li><code>n</code>: number of repeats. ´n == 1´ will return an unchanged       array.</li></ul><p><strong>Examples:</strong></p><pre><code class="language-Julia hljs">julia&gt; recycle_array([1,2],3)
6-element Array{Int64,1}:
 1
 2
 1
 2
 1
 2

julia&gt; x = [1 2; 3 4]
2×2 Array{Int64,2}:
 1  2
 3  4

julia&gt; recycle_array(x,3)
2×6 Array{Int64,2}:
 1  2  1  2  1  2
 3  4  3  4  3  4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/util.jl#L169-L204">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.de_embed" href="#NNHelferlein.de_embed"><code>NNHelferlein.de_embed</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function de_embed(x; remove_dim=true)</code></pre><p>Replace the maximum of the first dimension of an n-dimensional array by its index (aka argmax()). If <code>remove_dim=false</code> the first dim is preserved with size=1; otherwise the returned array has the first dimension removed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/util.jl#L230-L237">source</a></section></article><h2 id="Utils-for-fixing-types-in-GPU-context"><a class="docs-heading-anchor" href="#Utils-for-fixing-types-in-GPU-context">Utils for fixing types in GPU context</a><a id="Utils-for-fixing-types-in-GPU-context-1"></a><a class="docs-heading-anchor-permalink" href="#Utils-for-fixing-types-in-GPU-context" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.init0" href="#NNHelferlein.init0"><code>NNHelferlein.init0</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function init0(siz...)</code></pre><p>Initialise a vector or array of size <code>siz</code> with zeros. If a GPU is detected type of the returned value is <code>KnetArray{Float32}</code>, otherwise <code>Array{Float32}</code>.</p><p><strong>Examples:</strong></p><pre><code class="nohighlight hljs">julia&gt; init0(2,10)
2×10 Array{Float32,2}:
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0

 julia&gt; init0(0,10)
 0×10 Array{Float32,2}</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/util.jl#L61-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.convert2KnetArray" href="#NNHelferlein.convert2KnetArray"><code>NNHelferlein.convert2KnetArray</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function convert2KnetArray(x)</code></pre><p>Convert an array <code>x</code> to a <code>KnetArray{Float32}</code> or <code>KnetArray{Int32}</code><code>only in GPU context (if</code>CUDA.functional()<code>) or to an</code>Array{Float32}<code>/</code>Array{Int32}` otherwise.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/57df8c06740feb6b6eb00c990e1e18e8d154fb4d/src/util.jl#L93-L99">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../license/">License »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Wednesday 22 December 2021 10:13">Wednesday 22 December 2021</span>. Using Julia version 1.7.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
