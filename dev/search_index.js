var documenterSearchIndex = {"docs":
[{"location":"api/#Chains","page":"API Reference","title":"Chains","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"DNN","category":"page"},{"location":"api/#NNHelferlein.DNN","page":"API Reference","title":"NNHelferlein.DNN","text":"abstract type DNN end\n\nMother type for DNN hierarchy with implementation for a chain of layers.\n\nSignatures:\n\n(m::DNN)(x) = (for l in m.layers; x = l(x); end; x)\n(m::DNN)(x,y) = m(x,y)\n(m::DNN)(d::Knet.Data) = mean( m(x,y) for (x,y) in d)\n(m::DNN)(d::Tuple) = mean( m(x,y) for (x,y) in d)\n(m::DNN)(d::NNHelferlein.DataLoader) = mean( m(x,y) for (x,y) in d)\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Classifier\nRegressor\nChain","category":"page"},{"location":"api/#NNHelferlein.Classifier","page":"API Reference","title":"NNHelferlein.Classifier","text":"struct Classifier <: DNN\n\nClassifyer with nll loss.\n\nSignatures:\n\n(m::Classifier)(x,y) = nll(m(x), y)\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.Regressor","page":"API Reference","title":"NNHelferlein.Regressor","text":"struct Regressor\n\nRegression network with square loss.\n\nSignatures:\n\n(m::Regression)(x,y) = sum(abs2.( m(x) - y))\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.Chain","page":"API Reference","title":"NNHelferlein.Chain","text":"struct Chain\n\nSimple wrapper to chain layers and execute them one after another.\n\n\n\n\n\n","category":"type"},{"location":"api/#Layers","page":"API Reference","title":"Layers","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Layer","category":"page"},{"location":"api/#NNHelferlein.Layer","page":"API Reference","title":"NNHelferlein.Layer","text":"abstract type Layer end\n\nMother type for layers hierarchy.\n\n\n\n\n\n","category":"type"},{"location":"api/#Fully-connected-layers","page":"API Reference","title":"Fully connected layers","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Dense\nLinear\nEmbed\nPredictions","category":"page"},{"location":"api/#NNHelferlein.Dense","page":"API Reference","title":"NNHelferlein.Dense","text":"struct Dense  <: Layer\n\nDefault Dense layer.\n\nConstructors:\n\nDense(w, b, actf): default constructor\nDense(i::Int, j::Int; actf=sigm): layer of j neurons with       i inputs.\nDense(h5::HDF5.File, group::String; trainable=false, actf=sigm):\nDense(h5::HDF5.File, kernel::String, bias::String;       trainable=false, actf=sigm): layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.Linear","page":"API Reference","title":"NNHelferlein.Linear","text":"struct Linear  <: Layer\n\nAlmost standard dense layer, but functionality inspired by the TensorFlow-layer:\n\ncapable to work with input tensors of any number of dimensions\ndefault activation function indetity\noptionally without biases.\n\nThe shape of the input tensor is preserved; only the size of the first dim is changed from in to out.\n\nConstructors:\n\nLinear(i::Int, j::Int; bias=true, actf=identity) weher i is fan-in       and j is fan-out.\n\nKeyword arguments:\n\nbias=true: if false biases are fixed to 0.0\nactf=identity: activation function.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.Embed","page":"API Reference","title":"NNHelferlein.Embed","text":"struct Embed <: Layer\n\nSimple type for an embedding layer to embed a virtual onehot-vector into a smaller number of neurons by linear combination. The onehot-vector is virtual, because not the vector, but only the index of the \"one\" in the vector has to be provided as Integer value (or a minibatch of integers).\n\nFields:\n\nw\nactf\n\nConstructors:\n\nEmbed(v,d; actf=identity): with   vocab size v, embedding depth d and default activation function idendity.\n\nSignatures:\n\n(l::Embed)(x) = l.actf.(w[:,x]) default embedding of input tensor x.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.Predictions","page":"API Reference","title":"NNHelferlein.Predictions","text":"struct Predictions <: Layer\n\nSimple wrapper around a Dense layer without activation function that can used as output layer (because all loss-functions of the package assume raw output activations).\n\nConstructors:\n\nPredictions(i::Int, j:Int): with   input size i, output size j activation function idendity.\nPredictions(h5::HDF5.File, group::String; trainable=false):\nPredictions(h5::HDF5.File, kernel::String, bias::String;              trainable=false): with   an hdf5-object and group name of the output layer.\n\n\n\n\n\n","category":"type"},{"location":"api/#Convolutional","page":"API Reference","title":"Convolutional","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Conv\nDeConv\nPool\nUnPool","category":"page"},{"location":"api/#NNHelferlein.Conv","page":"API Reference","title":"NNHelferlein.Conv","text":"struct Conv  <: Layer\n\nDefault Conv layer.\n\nConstructors:\n\nConv(w, b, padding, actf): default constructor\nConv(w1::Int, w2::Int,  i::Int, o::Int; actf=relu; kwargs...): layer with   o kernels of size (w1,w2) for an input of i layers.\nConv(h5::HDF5.File, group::String; trainable=false, actf=relu):\nConv(h5::HDF5.File, group::String; trainable=false, actf=relu): layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.\n\nKeyword arguments:\n\npadding=0: the number of extra zeros implicitly concatenated       at the start and end of each dimension.\nstride=1: the number of elements to slide to reach the next filtering window.\ndilation=1: dilation factor for each dimension.\n... See the Knet documentation for Details:       https://denizyuret.github.io/Knet.jl/latest/reference/#Convolution-and-Pooling.       All keywords to the Knet function conv4() are supported.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.DeConv","page":"API Reference","title":"NNHelferlein.DeConv","text":"struct DeConv  <: Layer\n\nDefault deconvolution layer.\n\nConstructors:\n\nDeConv(w, b, actf, kwargs...): default constructor\nConv(w1::Int, w2::Int,  i::Int, o::Int; actf=relu, kwargs...): layer with   o kernels of size (w1,w2) for an input of i channels.\nConv(h5::HDF5.File, group::String; trainable=false, actf=relu):\nConv(h5::HDF5.File, group::String; trainable=false, actf=relu): layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.\n\nKeyword arguments:\n\npadding=0: the number of extra zeros implicitly concatenated       at the start and end of each dimension (applied to the output).\nstride=1: the number of elements to slide to reach the next filtering window       (applied to the output).\n... See the Knet documentation for Details:       https://denizyuret.github.io/Knet.jl/latest/reference/#Convolution-and-Pooling.       All keywords to the Knet function deconv4() are supported.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.Pool","page":"API Reference","title":"NNHelferlein.Pool","text":"struct Pool <: Layer\n\nPooling layer.\n\nConstructors:\n\nPool(;kwargs...): max pooling; without kwargs, 2x2-pooling       is performed.\n\nKeyword arguments:\n\nwindow=2: pooling window size (same for both directions)\n...: See the Knet documentation for Details:       https://denizyuret.github.io/Knet.jl/latest/reference/#Convolution-and-Pooling.       All keywords to the Knet function pool are supported.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.UnPool","page":"API Reference","title":"NNHelferlein.UnPool","text":"struct UnPool <: Layer\n\nUnpooling layer.\n\nConstructors:\n\nUnPool(;kwargs...): user-defined unpooling\n\n\n\n\n\n","category":"type"},{"location":"api/#Recurrent","page":"API Reference","title":"Recurrent","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"RSeqClassifier\nRSeqTagger\nhidden_states\ncell_states","category":"page"},{"location":"api/#NNHelferlein.RSeqClassifier","page":"API Reference","title":"NNHelferlein.RSeqClassifier","text":"struct RSeqClassifier <: Layer\n\nOne layer RNN sequence classifyer that works with minimatches of (time) series data. minibatch can be a 2- or 3-dimensional Array. If 2-d, inputs for one step are in one column and the Array has as many colums as steps. If 3-d, the last dimension iterates the samples of the minibatch.\n\nResult is always a 2-d matrix with the output of the units of the last step in each column and one column per sample of the minibatch.\n\nConstructors:\n\nRSeqClassifer(n_inputs::Int, n_units::Int; u_type=:lstm, o...): with   number of inputs, number of units and unit type.   Internally the type Knet.RNN is used and all keyword arguments   of Knet.RNN may be provided.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.RSeqTagger","page":"API Reference","title":"NNHelferlein.RSeqTagger","text":"struct RSeqTagger <: Layer\n\nOne layer RNN sequence classifyer that works with minimatches of (time) series data. minibatch can be a 2- or 3-dimensional Array. If 2-d, inputs for one step are in one column and the Array has as many colums as steps. If 3-d, the last dimension iterates the samples of the minibatch.\n\nResult is an array matrix with the output of the units of all steps for all smaples of the minibatch (with model depth as first and samples of the minimatch as last dimension).\n\nConstructors:\n\nRSeqTagger(n_inputs::Int, n_units::Int; u_type=:lstm, o...): with   number of inputs, number of units and unit type.   Internally the type Knet.RNN is used and all keyword arguments   of Knet.RNN may be provided.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.hidden_states","page":"API Reference","title":"NNHelferlein.hidden_states","text":"function hidden_states(l::<RNN_Type>)\n\nReturn the hidden states of one or more layers of an RNN. <RNN_Type> is one of RSeqClassifier, RSeqTagger, Knet.RNN.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.cell_states","page":"API Reference","title":"NNHelferlein.cell_states","text":"function cell_states(l::<RNN_Type>)\n\nReturn the cell states of one or more layers of an RNN only if it is a LSTM. <RNN_Type> is one of RSeqClassifier, RSeqTagger, Knet.RNN.\n\n\n\n\n\n","category":"function"},{"location":"api/#Others","page":"API Reference","title":"Others","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Flat\nPyFlat\nSoftmax\nDropout\nBatchNorm\nLayerNorm","category":"page"},{"location":"api/#NNHelferlein.Flat","page":"API Reference","title":"NNHelferlein.Flat","text":"struct Flat <: Layer\n\nDefault flatten layer.\n\nConstructors:\n\nFlat(): with no options.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.PyFlat","page":"API Reference","title":"NNHelferlein.PyFlat","text":"struct PyFlat <: Layer\n\nFlatten layer with optional Python-stype flattening (row-major). This layer can be used if pre-trained weight matrices from tensorflow are applied after the flatten layer.\n\nConstructors:\n\nPyFlat(; python=true): if true, row-major flatten is performed.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.Softmax","page":"API Reference","title":"NNHelferlein.Softmax","text":"struct Softmax <: Layer\n\nSimple softmax layer to compute softmax probabilities.\n\nConstructors:\n\nSoftmax()\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.Dropout","page":"API Reference","title":"NNHelferlein.Dropout","text":"struct Dropout <: Layer\n\nDropout layer. Implemented with help of Knet's dropout() function that evaluates AutoGrad.recording() to detect if in training or inprediction. Dropouts are applied only if prediction.\n\nConstructors:\n\nDropout(p) with the dropout rate p.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.BatchNorm","page":"API Reference","title":"NNHelferlein.BatchNorm","text":"struct BatchNorm <: Layer\n\nBatchnormalisation layer. Implemented with help of Knet's batchnorm() function that evaluates AutoGrad.recording() to detect if in training or in prediction. In training the moments are updated to record the running averages; in prediction the moments are applied, but not modified.\n\nIn addition, optional trainable factor a and bias b are applied:\n\ny = a cdot frac(x - mu)(sigma + epsilon) + b\n\nConstructors:\n\nBatchnom(; trainable=false, channels=0) will initialise       the moments with Knet.bnmoments() and       trainable parameters a and b only if       trainable==true (in this case, the number of channels must       be defined - for CNNs this is the number of feature maps).\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.LayerNorm","page":"API Reference","title":"NNHelferlein.LayerNorm","text":"struct LayerNorm  <: Layer\n\nSimple layer normalisation (inspired by TFs LayerNormalization). Implementation is from Deniz Yuret's answer to feature request 429 (https://github.com/denizyuret/Knet.jl/issues/492).\n\nThe layer performs a normalisation within each sample, not batchwise. Normalisation is modified by two trainable parameters a and b (variance and mean) added to every value of the sample vector.\n\nConstructors:\n\nLayertNorm(depth; eps=1e-6):  depth is the number       of activations for one sample of the layer.\n\nSignatures:\n\nfunction (l::LayerNorm)(x; dims=1): normalise x along the given dimensions.       The size of the specified dimension must fit with the initialised depth.\n\n\n\n\n\n","category":"type"},{"location":"api/#Attention-Mechanisms","page":"API Reference","title":"Attention Mechanisms","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"AttentionMechanism\nAttnBahdanau\nAttnLuong\nAttnDot\nAttnLocation\nAttnInFeed","category":"page"},{"location":"api/#NNHelferlein.AttentionMechanism","page":"API Reference","title":"NNHelferlein.AttentionMechanism","text":"abstract type AttentionMechanism\n\nAttention mechanisms follow the same interface and common signatures.\n\nIf possible, the algorithm allows precomputing of the projections of the context vector generated by the encoder in a encoder-decoder-architecture (i.e. in case of an RNN encoder the accumulated encoder hidden states).\n\nBy default attention scores are scaled according to Vaswani et al., 2017 (Vaswani et al., Attention Is All You Need, CoRR, 2017).\n\nAll algorithms use soft attention.\n\nConstructors:\n\nAttn*Mechanism*(dec_units, enc_units; scale=true)\nAttn*Mechanism*(units; scale=true)\n\nThe one-argument version can be used, if encoder dimensions and decoder dimensions are the same.\n\nCommon Signatures:\n\nfunction (attn::AttentionMechanism)(h_t, h_enc; reset=false)\nfunction (attn::AttentionMechanism)(; reset=false)\n\nArguments:\n\nh_t:    decoder hidden state. If h_t is a vector, its length           equals the number of decoder units. If it is a matrix,           h_t includes the states for a minibatch of samples and has           the size [units, mb].\nh_enc:  encoder hidden states, 2d or 3d. If h_enc is a           matrix [units, steps] with the hidden states of all encoder steps.           If 3d: [units, mb, steps] encoder states for all minibatches.\nreset=false: If the keyword argument is set to true, projections of           the encoder states are computed. By default projections are           stored in the object and reused until the object is resetted.           For attention mechanisms that don't allow precomputation           the argument is ignored.\n\nThe short form (::AttentionMechanism)(reset=true) can be used to reset the precomputed projections.\n\nReturn values\n\nAll functions return c and α where α is a matrix of size [mb,steps] with the attention factors for each step and minibatch. c is a matrix of size [units, mb] with the context vector for each sample of the minibatch, calculated as the α-weighted sum of all encoder hidden states h_enc for each minibatch.\n\nAttention Mechanisms:\n\nAll attention mechanisms calculate attention factors α from scores derived from projections of the encoder hidden states:\n\nalpha = mathrmsoftmax(mathrmscore(h_ench_t) cdot 1sqrtn))\n\nAttention mechanisms implemented:\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.AttnBahdanau","page":"API Reference","title":"NNHelferlein.AttnBahdanau","text":"mutable struct AttnBahdanau <: AttentionMechanism\n\nBahdanau-style (additive, concat) attention mechanism according to the paper:\n\nD. Bahdanau, KH. Co, Y. Bengio, Neural Machine Translation by jointlylearning to align and translate, ICLR, 2015.\n\nmathrmscore(h_th_enc) = v_a^topcdottanh(Wh_th_enc)\n\nConstructors:\n\nAttnBahdanau(dec_units, enc_units; scale=true)\nAttnBahdanau(units; scale=true)\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.AttnLuong","page":"API Reference","title":"NNHelferlein.AttnLuong","text":"mutable struct AttnLuong <: AttentionMechanism\n\nLuong-style (multiplicative) attention mechanism according to the paper (referred as General-type attention): M.-T. Luong, H. Pham, C.D. Manning, Effective Approaches to Attention-based Neural Machine Translation, CoRR, 2015.\n\nmathrmscore(h_th_enc) = h_t^top W h_enc\n\nConstructors:\n\nAttnLuong(dec_units, enc_units; scale=true)\nAttnLuong(units; scale=true)\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.AttnDot","page":"API Reference","title":"NNHelferlein.AttnDot","text":"mutable struct AttnDot <: AttentionMechanism\n\nDot-product attention (without trainable parameters) according to the Luong, et al. (2015) paper.\n\nmathrmscore(h_th_enc) = h_t^top h_enc\n\nConstructors:\n\nAttnDot(; scale=true)\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.AttnLocation","page":"API Reference","title":"NNHelferlein.AttnLocation","text":"mutable struct AttnLocation <: AttentionMechanism\n\nLocation-based attention that only depends on the current decoder state h_t and not on the encoder states, according to the Luong, et al. (2015) paper.\n\nmathrmscore(h_t) = W h_t\n\nConstructors:\n\nAttnLocation(len, dec_units; scale=true)\n\nlen: maximum sequence length of the encoder to be considered       for attention. If the actual length of h_enc is bigger as the       length of α, attention factors for the remaining states are set to       0.0. If the actual length of h_enc is smaller than α, only the matching       attention factors are applied.\ndec_units: number of decoder units.\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.AttnInFeed","page":"API Reference","title":"NNHelferlein.AttnInFeed","text":"mutable struct AttnInFeed <: AttentionMechanism\n\nInput-feeding attention that depends on the current decoder state h_t and the next input to the decoder i_t+1, according to the Luong, et al. (2015) paper.\n\nInfeed attention provides a semantic attention that depends on the next input token.\n\nmathrmscore(h_t i_t+1) = W_h h_t + W_i i_t+1 = W h_t i_t+1\n\nConstructors:\n\nAttnInFeed(len, dec_units, fan_in; scale=true)\n\nlen: maximum sequence length of the encoder to be considered       for attention. If the actual length of h_enc is bigger as the       length of α, attention factors for the remaining states are set to       0.0. If the actual length of h_enc is smaller than α, only the matching       attention factors are applied.\ndec_units: number of decoder units.\nfan_in: size of the decoder input.\n\nSignature:\n\nfunction (attn::AttnInFeed)(h_t, inp, h_enc)\n\nh_t:    decoder hidden state. If h_t is a vector, its length           equals the number of decoder units. If it is a matrix,           h_t includes the states for a minibatch of samples and has           the size [units, mb].\ninp: next decoder input i_t+1           (e.g. next embedded token of sequence)\nh_enc:  encoder hidden states, 2d or 3d. If h_enc is a           matrix [units, steps] with the hidden states of all encoder steps.           If 3d: [units, mb, steps] encoder states for all minibatches.\n\n\n\n\n\n","category":"type"},{"location":"api/#Layers-for-transformers","page":"API Reference","title":"Layers for transformers","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"PositionalEncoding","category":"page"},{"location":"api/#NNHelferlein.PositionalEncoding","page":"API Reference","title":"NNHelferlein.PositionalEncoding","text":"struct PositionalEncoding <: Layer\n\nPositional encoding layer. Only sincos-style (according to Vaswani, et al., NIPS 2017) is implemented.\n\nThe layer takes an array of any any number of dimensions (>=2), calculates the Vaswani-2017-style positional encoding and adds the encoding to each plane of the array.\n\n\n\n\n\n","category":"type"},{"location":"api/#Data-providers","page":"API Reference","title":"Data providers","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"DataLoader","category":"page"},{"location":"api/#NNHelferlein.DataLoader","page":"API Reference","title":"NNHelferlein.DataLoader","text":"abstract type DataLoader\n\nMother type for minibatch iterators.\n\n\n\n\n\n","category":"type"},{"location":"api/#Tabular-data","page":"API Reference","title":"Tabular data","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Tabular data is normally provided in table form (csv, ods) row-wise, i.e. one sample per row. The helper functions can read the tables and generate Knet compatible iterators of minibatches.","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"dataframe_read\ndataframe_minibatches\ndataframe_split\nmk_class_ids","category":"page"},{"location":"api/#NNHelferlein.dataframe_read","page":"API Reference","title":"NNHelferlein.dataframe_read","text":"dataframe_read(fname)\n\nRead a data table from an CSV-file with one sample per row and return a DataFrame with the data. (ODS-support is removed because of PyCall compatibility issues of the OdsIO package).\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.dataframe_minibatches","page":"API Reference","title":"NNHelferlein.dataframe_minibatches","text":"dataframe_minibatches(data::DataFrames.DataFrame; size=256, ignore=[], teaching=\"y\", o...)\n\nMake Knet-conform minibatches of type Knet.data from a dataframe with one sample per row.\n\nArguments:\n\nignore: defines a list of column names to be ignored\nteaching=\"y\": defines the column name with teaching input. Default is \"y\".               teaching is handled differently, depending on its type:               If Int, the teaching input is interpreted as               class ids and directly used for training (this assumes that               the values range from 1..n). If type is a String, values are               interpreted as class labels and convertet to numeric class IDs               by calling mk_class_ids(). The list of valid lables and their               order can be created by calling mk_class_ids(data.y)[2].               If teaching is a scalar value, regression context is assumed,               and the value is used unchanged for training.\nother keyword arguments: all keyword arguments accepted by               Knet.minibatch() may be used.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.dataframe_split","page":"API Reference","title":"NNHelferlein.dataframe_split","text":"function dataframe_split(df::DataFrames.DataFrame;\n                         teaching=\"y\", fr=0.2, balanced=true)\n\nSplit data, organised row-wise in a DataFrame into train and valid sets.\n\nArguments:\n\ndf: data\nteaching=\"y\": name or index of column with teaching input (y)\nfr=0.2: fraction of data to be used for validation\nshuffle=true: shuffle the rows of the dataframe.\nbalanced=true: if true, result datasets will be balanced by oversampling.             Returned datasets will be bigger as expected             but include the same numbers of samples for each class.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.mk_class_ids","page":"API Reference","title":"NNHelferlein.mk_class_ids","text":"function mk_class_ids(labels)\n\nTake a list with n class labels for n instances and return a list of n class-IDs (of type Int) and an array of lables with the array index of each label corresponds its ID.\n\nArguments:\n\nlabels: List of labels (typically Strings)\n\nResult values:\n\narray of class-IDs in the same order as the input\narray of unique class-IDs ordered by their ID.\n\nExamples:\n\njulia> labels = [\"blue\", \"red\", \"red\", \"red\", \"green\", \"blue\", \"blue\"]\n7-element Array{String,1}:\n \"blue\"\n \"red\"\n \"red\"\n \"red\"\n \"green\"\n \"blue\"\n \"blue\"\n\njulia> mk_class_ids(labels)[1]\n7-element Array{Int64,1}:\n 1\n 3\n 3\n 3\n 2\n 1\n 1\n\n julia> mk_class_ids(labels)[2]\n3-element Array{String,1}:\n \"blue\"\n \"green\"\n \"red\"\n\n\n\n\n\n","category":"function"},{"location":"api/#Image-data","page":"API Reference","title":"Image data","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Images as data should be provided in directories with the directory names denoting the class labels. The helpers read from the root of a directory tree in which the first level of sub-dirs tell the class label. All images in the tree under a class label are read as instances of the respective class. The following tree will generate the classes daisy, rose and tulip:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"image_dir/\n├── daisy\n│   ├── 01\n│   │   ├── 01\n│   │   ├── 02\n│   │   └── 03\n│   ├── 02\n│   │   ├── 01\n│   │   └── 02\n│   └── others\n├── rose\n│   ├── big\n│   └── small\n└── tulip","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"ImageLoader\nmk_image_minibatch\nget_class_labels\nimage2array\narray2image\narray2RGB","category":"page"},{"location":"api/#NNHelferlein.ImageLoader","page":"API Reference","title":"NNHelferlein.ImageLoader","text":"struct ImageLoader <: DataLoader\n    dir\n    i_paths\n    i_classes\n    classes\n    batchsize\n    shuffle\n    train\n    aug_pipl\n    pre_proc\n    pre_load\n    i_images\nend\n\nIterable image loader to provide minibatches of images as 4-d-arrays (x,y,rgb,mb).\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.mk_image_minibatch","page":"API Reference","title":"NNHelferlein.mk_image_minibatch","text":"function mk_image_minibatch(dir, batchsize; split=false, fr=0.2,\n                            balanced=false, shuffle=true, train=true,\n                            aug_pipl=nothing, pre_proc=nothing)\n\nReturn one or two iterable image-loader-objects that provides minibatches of images. For training each minibatch is a tupel (x,y) with x: 4-d-array with the minibatch of data and y: vector of class IDs as Int.\n\nArguments:\n\ndir: base-directory of the image dataset. The first level of       sub-dirs are used as class names.\nbatchsize: size of minibatches\n\nKeyword arguments:\n\nsplit: return two iterators for training and validation\nfr: split fraction\nbalanced: return balanced data (i.e. same number of instances       for all classes). Balancing is achieved via oversampling\nshuffle: if true, shuffle the images everytime the iterator       restarts\ntrain: if true, minibatches with (x,y) Tuples are provided,       if false only x (for prediction)\npre_load: if true all images are loaded in advance;       otherwise images are loaded on demand durng training.       (option is not implemented yet!)\naug_pipl: augmentation pipeline for Augmentor.jl. Augmentation       is performed before the pre_proc-function is applied\npre_proc: function with preprocessing       and augmentation algoritms of type x = f(x). In contrast       to the augmentation that modifies images, is pre_proc       working on Arrays{Float32}.\npre_load=false: read all images from disk once when populating the       loader (requires loads of memory, but speeds up training).\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.get_class_labels","page":"API Reference","title":"NNHelferlein.get_class_labels","text":"function get_class_labels(d::DataLoader)\n\nExtracts a list of class labels from a DataLoader.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.image2array","page":"API Reference","title":"NNHelferlein.image2array","text":"function image2array(img)\n\nTake an image and return a 3d-array for RGB and a 2d-array for grayscale images with the colour channels as last dimension.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.array2image","page":"API Reference","title":"NNHelferlein.array2image","text":"function array2image(arr)\n\nTake a 3d-array with colour channels as last dimension or a 2d-array and return an array of RGB or of Gray as Image.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.array2RGB","page":"API Reference","title":"NNHelferlein.array2RGB","text":"function array2RGB(arr)\n\nTake a 3d-array with colour channels as last dimension or a 2d-array and return alwasy an array of RGB as Image.\n\n\n\n\n\n","category":"function"},{"location":"api/#Text-data","page":"API Reference","title":"Text data","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"WordTokenizer\nget_tatoeba_corpus\nseq_minibatch\nseq2seq_minibatch","category":"page"},{"location":"api/#NNHelferlein.WordTokenizer","page":"API Reference","title":"NNHelferlein.WordTokenizer","text":"mutable struct WordTokenizer\n    len\n    w2i\n    i2w\nend\n\nCreate a word-based vocabulary: every unique word of a String or a list of Strings is assigned to a unique number. The created object includes a list of words (i2w, ordered by their numbers) and a dictionary w2i with the words as keys.\n\nConstructor:\n\nfunction WordTokenizer(texts; len=nothing, add_ctls=true)\n\nWith arguments:\n\ntexts: AbstractArray or iterable collection of AbstractArrays to be       analysed.\nlen=nothing: maximum number of different words in the vocabulary.       Additional words in texts will be encoded as unknown. If nothing,       all words of the texts are included.\nadd_ctls=true: if true, control words are added to the vocabulary       (extending the maximum length by 4): \"<start>\", \"<end>\",       \"<pad>\" and \"<unknown>\". \"<unknown>\" will allways be encoded       with the largest number in the vocab (i.e. i2w[end]).\n\nSignatures:\n\nfunction (t::WordTokenizer)(w::AbstractString)\n\nEncode a word and return the corresponding number in the vocabulary or the highest number (i.e. \"<unknown>\") if the word is not in the vocabulary.\n\nThe encode-signature accepts the keyword arguments split_words and add_ctl. If split_words==true, the input is treated as a sentence and splitted into single words and an array of integer with the encoded sequence is returned. If add_ctl==true the sequence will be framed by <start> and <end> tokens.\n\nfunction (t::WordTokenizer)(i::Int)\n\nDecode a word by returning the word corresponding to i or \"<unknown>\" if the number is out of range of the vocabulary.\n\nExamples:\n\njulia> vocab = WordTokenizer([\"I love Julia\", \"They love Python\"]);\nJulia> vocab(4)\n\"Julia\"\n\njulia> vocab(\"love\")\n1\n\njulia> vocab.(split(\"I love Julia\"))\n3-element Array{Int64,1}:\n 2\n 1\n 4\n\njulia> vocab.([3,1,4])\n3-element Array{String,1}:\n \"They\"\n \"love\"\n \"Julia\n\n julia> vocab.(split(\"I love Scala\"))\n3-element Array{Int64,1}:\n 2\n 1\n 9\n\njulia> vocab.([2,1,9])\n3-element Array{String,1}:\n \"I\"\n \"love\"\n \"<unknown>\"\n\n julia> vocab(\"Ich liebe Python\", split_words=true, add_ctl=true)\n5-element Array{Int64,1}:\n 6\n 9\n 9\n 5\n 7\n\n\n\n\n\n","category":"type"},{"location":"api/#NNHelferlein.get_tatoeba_corpus","page":"API Reference","title":"NNHelferlein.get_tatoeba_corpus","text":"function get_tatoeba_corpus(lang; force=false,\n            url=\"https://www.manythings.org/anki/\")\n\nDownload and read a bilingual text corpus from Tatoeba (privided) by ManyThings (https://www.manythings.org). All corpi are English-Language-pairs with different size and quality. Considerable languages include:\n\nfra: French-English, 180 000 sentences\ndeu: German-English, 227 000 sentences\nheb: Hebrew-English, 126 000 sentences\npor: Portuguese-English, 170 000 sentences\ntur: Turkish-English, 514 000 sentences\n\nThe function returns two lists with corresponding sentences in both languages. Sentences are are not processed/normalised/cleaned, but exactly as provided by Tatoeba.\n\nThe data is stored in the package directory and only downloaded once.\n\nArguments:\n\nlang: languagecode\nforce=false: if true, the corpus is downloaded even if       a data file is already saved.\nurl: base url of ManyThings.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.seq_minibatch","page":"API Reference","title":"NNHelferlein.seq_minibatch","text":"function seq_minibatch(x, [y,] batchsize; seq_len=nothing, pad=0, o...)\n\nReturn an iterator of type Knet.Data with sequence minibatches from a list of sequences. all keyword args of Knet.minibatch() can be used.\n\nAll sequences in x are brought to the same length by truncating (if too long) or padding with the token provided as pad.\n\nIf y is defined, the minibatches include the sequences for x and training targets y, given as n-dimensional array (as for Knet.minibach()). For sequence-2-sequence minibatches the function seq2seq_minibatch() must be used.\n\nArguments:\n\nx: An iterable object of sequences.\ny: vector or array with training targets\nbatchsize: size of minibatches\nseq_len=nothing: demanded length of sequences in the minibatches.       If nothing, all sequences are padded to match with the longest       sequence.\npad=0: token, used for padding. The token must be compatible       with the type of the sequence elements.\no...: any other keyword arguments of Knet.minibatch(), such as       shuffle=true or partial=true can be provided.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.seq2seq_minibatch","page":"API Reference","title":"NNHelferlein.seq2seq_minibatch","text":"function seq2seq_minibatch(x, y, batchsize; seq_len=nothing, pad=0, o...)\n\nReturn an iterator of type Knet.Data with (x,y) sequence minibatches from two lists of sequences. all keyword args of Knet.minibatch() can be used.\n\nAll sequences in x and y are brought to the same length by truncating (if too long) or padding with the token provided as pad.\n\nArguments:\n\nx: An iterable object of sequences.\ny: An iterable object of target sequences.\nbatchsize: size of minibatches\nseq_len=nothing: demanded length of sequences in the minibatches.       If nothing, all sequences are padded to match with the longest       sequence.\npad_x=0,\npad_y=0: token, used for padding. The token must be compatible       with the type of the sequence elements.\no...: any other keyword arguments of Knet.minibatch(), such as       shuffle=true or partial=true can be provided.\n\n\n\n\n\n","category":"function"},{"location":"api/#Training","page":"API Reference","title":"Training","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"tb_train!","category":"page"},{"location":"api/#NNHelferlein.tb_train!","page":"API Reference","title":"NNHelferlein.tb_train!","text":"function tb_train!(mdl, opti, trn, vld=nothing; epochs=1,\n                  lr_decay=nothing, lrd_freq=1, l2=0.0,\n                  eval_size=0.2, eval_freq=1,\n                  acc_fun=nothing,\n                  mb_loss_freq=100,\n                  cp_freq=nothing, cp_dir=\"checkpoints\",\n                  tb_dir=\"logs\", tb_name=\"run\",\n                  tb_text=\"\"\"Description of tb_train!() run.\"\"\",\n                  opti_args...)\n\nTrain function with TensorBoard integration. TB logs are written with the TensorBoardLogger.jl package. The model is updated (in-place) and the trained model is returned.\n\nArguments:\n\nmdl: model; i.e. forward-function for the net\nopti: Knet-stype optimiser type\ntrn: training data; iterator to provide (x,y)-tuples with       minibatches\nvld: validation data; iterator to provide (x,y)-tuples with       minibatches. Set to nothing, if not defined.\n\nKeyword arguments:\n\nOptimiser:\n\nepochs=1: number of epochs to train\nlr_decay=nothing: Leraning rate decay if not nothing:       factor (<1) to reduce the       lr every epoch as lr  *= lr_decay.\nlrd_freq=1: frequency of learning rate decay steps. Default is       to modify the lr after every epoch\nl2=0.0: L2 regularisation; implemented as weight decay per       parameter\nopti_args...: optional keyword arguments for the optimiser can be specified       (i.e. lr, gamma, ...).\n\nModel evaluation:\n\neval_size=0.2: fraction of validation data to be used for calculating       loss and accuracy for train and validation data during training.\neval_freq=1: frequency of evaluation; default=1 means evaluation is       calculated after each epoch. With eval_freq=10 eveluation is       calculated 10 times per epoch.\nacc_fun=nothing: function to calculate accuracy. The function       is called with 2 arguments: fun(predictions, teaching) where       predictions is the output of a model call and a matrix and       teaching is the teaching input (y).       For classification tasks, accuracy from the Knet package is       a good choice. For regression a correlation or mean error       may be used (i.e. acc_fun=(x,y)->sum(abs, x.-y)).\nmb_loss_freq=100: frequency of training loss reporting. default=100       means that 100 loss-values per epoch will be logged to TensorBoard.       If mblossfreq is greater then the number of minibatches,       loss is logged for each minibatch.\ncp_freq=nothing: frequency of model checkpoints written to disk.       Default is nothing, i.e. no checkpoints are written.       To write the model after each epoch with       name model use freq=1; to write every 2 epochs freq=0.5.\ncp_dir=\"checkpoints\": directory for checkpoints\n\nTensorBoard:\n\nTensorBoard log-directory is created from 3 parts: tb_dir/tb_name/<current date time>.\n\ntb_dir=\"logs\": root directory for tensorborad logs.\ntb_name=\"run\": name of training run. tb_name will be used as       directory name and should not include whitespace\ntb_text:  description       to be included in the TensorBoard log as text log.\n\n\n\n\n\n","category":"function"},{"location":"api/#Evaluation","page":"API Reference","title":"Evaluation","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"predict\npredict_top5","category":"page"},{"location":"api/#NNHelferlein.predict","page":"API Reference","title":"NNHelferlein.predict","text":"function predict(mdl, x; softmax=false)\n\nReturn the prediction for x.\n\nArguments:\n\nmdl: executable network model\nx: iterator providing minibatches       of input data\nsoftmax: if true and if model is a ::Classifier the predicted       softmax probabilities are returned instead of raw       activations.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.predict_top5","page":"API Reference","title":"NNHelferlein.predict_top5","text":"function predict_top5(mdl, x; top_n=5, classes=nothing)\n\nRun the model mdl for data in x and print the top 5 predictions as softmax probabilities.\n\nArguments:\n\ntop_n: print top n hits\nclasses: optional list of human readable class labels.\n\n\n\n\n\n","category":"function"},{"location":"api/#ImageNet-tools","page":"API Reference","title":"ImageNet tools","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"preproc_imagenet\npredict_imagenet\nget_imagenet_classes","category":"page"},{"location":"api/#NNHelferlein.preproc_imagenet","page":"API Reference","title":"NNHelferlein.preproc_imagenet","text":"function preproc_imagenet(img)\n\nImage preprocessing for pre-trained ImageNet examples. Preprocessing includes\n\nbring RGB colour values into a range 0-255\nstandardise of colour values by substracting mean colour values   (103.939, 116.779, 123.68) from RGB\nchanging colour channel sequence from RGB to BGR\n\nResize is not done, because this may be part of the augmentation pipeline.\n\nExamples:\n\nThe function can be used with the image loader; for prediction with a trained model as:\n\npipl = CropRatio(ratio=1.0) |> Resize(224,224)\nimages = mk_image_minibatch(\"./example_pics\", 16;\n                    shuffle=false, train=false,\n                    aug_pipl=pipl,\n                    pre_proc=preproc_imagenet)\n\nAnd for training something like:\n\npipl = Either(1=>FlipX(), 1=>FlipY(), 2=>NoOp()) |>\n       Rotate(-5:5) |>\n       ShearX(-5:5) * ShearY(-5:5) |>\n       RCropSize(224,224)\n\ndtrn, dvld = mk_image_minibatch(\"./example_pics\", 16;\n                    split=true, fr=0.2, balanced=false,\n                    shuffle=true, train=true,\n                    aug_pipl=pipl,\n                    pre_proc=preproc_imagenet)\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.predict_imagenet","page":"API Reference","title":"NNHelferlein.predict_imagenet","text":"function predict_imagenet(mdl, x; top_n=5)\n\nPredict the ImageNet-class of images from the predefined list of class labels.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.get_imagenet_classes","page":"API Reference","title":"NNHelferlein.get_imagenet_classes","text":"function get_imagenet_classes()\n\nReturn a list of all 1000 ImageNet class labels.\n\n\n\n\n\n","category":"function"},{"location":"api/#Other-utils","page":"API Reference","title":"Other utils","text":"","category":"section"},{"location":"api/#Utils-for-transformers","page":"API Reference","title":"Utils for transformers","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"positional_encoding_sincos\nmk_padding_mask\nmk_peek_ahead_mask\ndot_prod_attn","category":"page"},{"location":"api/#NNHelferlein.positional_encoding_sincos","page":"API Reference","title":"NNHelferlein.positional_encoding_sincos","text":"function positional_encoding_sincos(n_embed, n_seq)\n\nCalculate and return a matrix of size [n_embed, n_seq] of positional encoding values following the sin and cos style in the paper Vaswani, A. et al.; Attention Is All You Need; 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA, 2017.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.mk_padding_mask","page":"API Reference","title":"NNHelferlein.mk_padding_mask","text":"function mk_padding_mask(x; pad=0)\n\nMake a padding mask; i.e. return an Array of type KnetArray{Float32} (or Array{Float32}) similar to x but with two additional dimension of size 1 in teh middle (this will represent the 2nd seq_len and the number of heads) in multi-head attention and the value 1.0 at each position where x is pad and 0.0 otherwise.\n\nThe function can be used for creating padding masks for attention mechanisms.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.mk_peek_ahead_mask","page":"API Reference","title":"NNHelferlein.mk_peek_ahead_mask","text":"function mk_peek_ahead_mask(n_seq)\n\nReturn a matrix of size [n_seq, n_seq] filled with 1.0 and the lower triangle set to 0.0. Type is KnetArray{Float32} in GPU context, Array{Float32} otherwise. The matrix can be used as peek-ahead mask in transformers.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.dot_prod_attn","page":"API Reference","title":"NNHelferlein.dot_prod_attn","text":"function dot_prod_attn(q, k, v; mask=nothing)\n\nGeneric scaled dot product attention following the paper of Vaswani et al., (2017), Attention Is All You Need.\n\nArguments:\n\nq: query of size [depth, n_seq_q, ...]\nk: key of size [depth, n_seq_v, ...]\nv: value of size [depth, n_seq_v, ...]\nmask: mask for attention factors may have different shapes but must be       broadcastable for addition to the scores tensor (which as the same size as       alpha [n_seq_v, n_seq_q, ...]). In transformer context typical masks are one of:       padding mask of size [n_seq_v, ...] or a peek-ahead mask of size [n_seq_v, n_seq_v]       (which is only possible in case of self-attention when all seqencee lengths       are identical).\n\nq, k, v must have matching leading dimensions (i.e. same depth or embedding). k and v must have the same sequence length.\n\nReturn values:\n\nc: context as alpha-weighted sum of values with size [depth, nseqv, ...]\nalpha: attention factors of size [nseqv, nseqq, ...]\n\n\n\n\n\n","category":"function"},{"location":"api/#Utils-for-array-manipulation","page":"API Reference","title":"Utils for array manipulation","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"crop_array\nblowup_array\nrecycle_array\nde_embed","category":"page"},{"location":"api/#NNHelferlein.crop_array","page":"API Reference","title":"NNHelferlein.crop_array","text":"function crop_array(x, crop_sizes)\n\nCrop a n-dimensional array to the given size. Cropping is always centered (i.e. a margin is removed).\n\nArguments:\n\nx: n-dim AbstractArray\ncrop_sizes: Tuple of target sizes to which the array is cropped.       Allowed values are Int or :. If crop_sizes defines less       dims as x has, the remaining dims will not be cropped (assuming :).       If a demanded crop size is bigger as the actual size of x,       it is ignored.\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.blowup_array","page":"API Reference","title":"NNHelferlein.blowup_array","text":"function blowup_array(x, n)\n\nBlow up an array x with an additional dimension and repeat the content of the array n times.\n\nArguments:\n\nx: Array of any dimension\nn: number of repeats. ´n == 1´ will return an\n\narray with an addritional dimension of size 1.\n\nExamples:\n\njulia> x = [1,2,3,4]; blowup_array(x, 3)\n4×3 Array{Int64,2}:\n 1  1  1\n 2  2  2\n 3  3  3\n 4  4  4\n\njulia> x = [1 2; 3 4]; blowup_array(x, 3)\n2×2×3 Array{Int64,3}:\n[:, :, 1] =\n 1  2\n 3  4\n\n[:, :, 2] =\n 1  2\n 3  4\n\n[:, :, 3] =\n 1  2\n 3  4\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.recycle_array","page":"API Reference","title":"NNHelferlein.recycle_array","text":"function recycle_array(x, n)\n\nRecycle an array x along the last dimension and repeat the content of the array n times. The number of dims stays unchanged, but the array valueas are repeated n times.\n\nArguments:\n\nx: Array of any dimension\nn: number of repeats. ´n == 1´ will return an unchanged       array.\n\nExamples:\n\njulia> recycle_array([1,2],3)\n6-element Array{Int64,1}:\n 1\n 2\n 1\n 2\n 1\n 2\n\njulia> x = [1 2; 3 4]\n2×2 Array{Int64,2}:\n 1  2\n 3  4\n\njulia> recycle_array(x,3)\n2×6 Array{Int64,2}:\n 1  2  1  2  1  2\n 3  4  3  4  3  4\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.de_embed","page":"API Reference","title":"NNHelferlein.de_embed","text":"function de_embed(x; remove_dim=true)\n\nReplace the maximum of the first dimension of an n-dimensional array by its index (aka argmax()). If remove_dim=false the first dim is preserved with size=1; otherwise the returned array has the first dimension removed.\n\n\n\n\n\n","category":"function"},{"location":"api/#Utils-for-fixing-types-in-GPU-context","page":"API Reference","title":"Utils for fixing types in GPU context","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"init0\nconvert2KnetArray","category":"page"},{"location":"api/#NNHelferlein.init0","page":"API Reference","title":"NNHelferlein.init0","text":"function init0(siz...)\n\nInitialise a vector or array of size siz with zeros. If a GPU is detected type of the returned value is KnetArray{Float32}, otherwise Array{Float32}.\n\nExamples:\n\njulia> init0(2,10)\n2×10 Array{Float32,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n\n julia> init0(0,10)\n 0×10 Array{Float32,2}\n\n\n\n\n\n","category":"function"},{"location":"api/#NNHelferlein.convert2KnetArray","page":"API Reference","title":"NNHelferlein.convert2KnetArray","text":"function convert2KnetArray(x)\n\nConvert an array x to a KnetArray{Float32} only in GPU context (if CUDA.functional()) or to an Array{Float32} otherwise.\n\n\n\n\n\n","category":"function"},{"location":"license/","page":"License","title":"License","text":"The NNHelferlein.jl package is licensed under the MIT License:","category":"page"},{"location":"license/","page":"License","title":"License","text":"Copyright (c) 2021 Andreas Dominik, THM, Gießen, Germany","category":"page"},{"location":"license/","page":"License","title":"License","text":"Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:","category":"page"},{"location":"license/","page":"License","title":"License","text":"The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.","category":"page"},{"location":"license/","page":"License","title":"License","text":"THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Examples may be used as templates for new projects...","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"MNIST LeNet: A simple LeNet build with help of the Helferlein layers   in just two (ok: long) lines of code.\nPretrained VGG16: The notebook shows import of a pretrained VGG16 model   from Tensorflow/Keras into a Knet-style CNN   and its application to example images utilising the   Helferlein imagenet-utilities.","category":"page"},{"location":"overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"The section provides a brief overview of the functionality provided by NNHelferlen. For more details, please visit the API-Section.","category":"page"},{"location":"overview/#Neural-network-definitions","page":"Overview","title":"Neural network definitions","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"The abstract type DNN provides signatures to be called as","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"(m::DNN)(x): evaluate x (sample or minibatch)\n(m::DNN)(x,y): evaluate x and calculate the loss\n(m::DNN)(d): return the mean loss for a dataset, if d is an iterator               of type Knet.Data or NNHelferlen.DataLoader\n(m::DNN)((x,y)): return the mean loss for a x,y-tuple.","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"Explicit signatures exist for types Classifier and Regressor with negative log-likelihood and square loss as loss, respectively. The type Chain wraps a list of layers that are executed sequentially.","category":"page"},{"location":"overview/#Layer-definitions","page":"Overview","title":"Layer definitions","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"Several layers are predefined with executable signatures:","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"MLPs: different flavours of the simple layer:       Dense: default layer for a vector (i.e. sample)          or matrix (i.e. mininbatch) as input with logistic          actvation as default.               Predictions: Dense layer without activation function.       Linear: TensorFlow-style layer to process high-dimensional         arrays and identity as default activation.       Embed: embedding layer that adds a first dimension with the          embeddings to the input.\nConvolutional NNs: to build CNNs Conv, DeConv, Pool       UnPool and Flat             layers are provided with standard functionality.       The utilitys include methods for array manipulation, such as       clipping arrays or adding dimensions.\nOthers: additional layers include (please see the API-section for       a complete list!):       Softmax, Dropout, trainable BatchNorm, trainable LayerNorm.","category":"page"},{"location":"overview/#Attention-Mechanisms","page":"Overview","title":"Attention Mechanisms","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"Some attention mechanisms are implemented for use in sequence-to-sequence networks. If possible projections of values are  precomputed to reduce computational cost:","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"AttnBahdanau: concat- or additive-style attention according to       Bahdanau, 2015.\nAttnLuong: multiplicative-or general-stype attention according to       Luong, 2015.\nAttnDot: dot-product-style attention according to       Luong, 2015.\nAttnLocation: dot-product-style attention according to       Luong, 2015.\nAttnInFeed: input-feeding attention according to       Luong, 2015.","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"A generalised dot-product attention can be computed from (Query, Key, Value) tuple: dot_prod_attn(q, k, v).","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"Helpers for transformer networks include functions for positional encoding, generating padding- and peek-akead-masks and computing scaled multi-headed attention, according to Vaswani, 2017.","category":"page"},{"location":"overview/#Data-provider","page":"Overview","title":"Data provider","text":"","category":"section"},{"location":"overview/#Image-data","page":"Overview","title":"Image data","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"The function mk_image_minibatch() can be used to create an iterator over images, organised in directories, with the first directory-level as class labels.","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"Helper functions (such as image2array(), array2image(), array2RGB()) can be used to transform image data to arrays. Imagenet-style preprocessing can be achieved with preproc_imagenet(), readable Imagenet class labels of the top predictions are printed by predict_imagenet().","category":"page"},{"location":"overview/#DataFrames","page":"Overview","title":"DataFrames","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"Helpers for tabular date include:","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"dataframe_read: read a csv-file and return a DataFrame\ndataframe_split: split tabular data in a DataFrame into train and               validation data; optionally with balancing.\ndataframe_minibatches: data provider to turn tabular data from               a DataFrame (with one sample per row)               into a Knet-like iterator of minibatches of type Knet.Data.\nmk_class_ids(labels): may be used to turn class label strings into               class-IDs.","category":"page"},{"location":"overview/#Texts-and-NLP","page":"Overview","title":"Texts and NLP","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"Some utilities are provided for NLP data handling:","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"WordTokenizer: a simple tool to encode words as ids.       The type comes with signatures to en- and decode in both directions.\nget_tatoeba_corpus: download dual-language corpi and provide       corresponding lists of sentences in two languages.","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"seq_minibatch() and seq2seq_minibatch() functions return iterators to sequences (incl. truncating or padding of the sequences to a common length). The functions are not restricted to tokenised strings as sequence elements can be on any type.","category":"page"},{"location":"overview/#Working-with-pretrained-networks","page":"Overview","title":"Working with pretrained networks","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"Layers of pre-trained models can be created from TensorFlow HDF5-parameter files. It is possible to build a network from any pretrained TensorFlow model by importing the parameters by HDF5-constructors for the layers Dense, Conv. The flatten-layer PyFlat allows for Python-like row-major-flattening, necessary to make sure, that the parameters of an imported layer after flattening are in the correct order.","category":"page"},{"location":"overview/#Training","page":"Overview","title":"Training","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"Although Knet-style is to avoid havyweight interfaces and train networks with lightweight and flexible optimisers, a train interface is added that provides TensorBoard logs with online reporting of minibatch loss, training and validation loss and accuracy.","category":"page"},{"location":"overview/#Utilities","page":"Overview","title":"Utilities","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"A number of additional utilities is included. Please have a look at the utilities section of the API documentation.","category":"page"},{"location":"#NNHelferlein-ADo's-Neural-Networks-Little-Helpers","page":"Introduction","title":"NNHelferlein - ADo's Neural Networks Little Helpers","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The package provides helpers and utilities mainly to be used with the Knet package to build artificial neural networks.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The package follows mainly the Knet-style; i.e. all networks can be trained with the Knet-iterators, all layers can be used together with Knet-style quickly-self-written layers, all Knet-networks can be trained with tb_train(), all data providers can be used together, ...","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"For installation please refer to the README @github: https://github.com/andreasdominik/NNHelferlein.jl","category":"page"},{"location":"#Quick-Start","page":"Introduction","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Have a look at the examples:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Pages = [\n    \"examples.md\"\n    ]\nDepth = 2","category":"page"},{"location":"#Overview","page":"Introduction","title":"Overview","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Pages = [\n    \"overview.md\"\n    ]\nDepth = 2","category":"page"},{"location":"#API-Reference","page":"Introduction","title":"API Reference","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Pages = [\n    \"api.md\"\n    ]\nDepth = 2","category":"page"},{"location":"#Index","page":"Introduction","title":"Index","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"}]
}
