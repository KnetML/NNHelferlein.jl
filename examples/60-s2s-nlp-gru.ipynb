{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence RNN for machine translation\n",
    "\n",
    "The notebook shows how to implement a recurrent neural network for machine translation \n",
    "with help of Knet and NNHelferlein.\n",
    "The net uses a Tatoeba-corpus to train a one-layer gru network. \n",
    "The resulting network demostrates the abilities of such an architecture - however the \n",
    "training corpus ist much too small to be sufficient for a professional\n",
    "translator; and the network should have more layers and more units per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using Random, StatsBase\n",
    "using Knet, AutoGrad\n",
    "using NNHelferlein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The seq-2-seq-model\n",
    "\n",
    "The sequence-to-sequence model is simple. We need\n",
    "+ the type\n",
    "+ a constructor\n",
    "+ signatures for training (with 2 sequences as arguments) and for prediction (with only the \n",
    "  source signature as arg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type and constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct S2S\n",
    "    embed_enc       # embed layer for encoder\n",
    "    embed_dec       # embed layer for decoder\n",
    "    encoder         # encoder rnn\n",
    "    decoder         # decoder rnn\n",
    "    predict         # predict layer (Linear w/o actf)\n",
    "    drop            # dropout layer\n",
    "    voc_in; voc_out # vocab sizes\n",
    "    embed           # embedding depth\n",
    "    units           # number of lstm units in layers\n",
    "\n",
    "    function S2S(n_embed, n_units, n_vocab_in, n_vocab_out)\n",
    "        embed_enc = Embed(n_vocab_in, n_embed)\n",
    "        drop = Dropout(0.1)\n",
    "        embed_dec = Embed(n_vocab_out, n_embed)\n",
    "        encoder = Recurrent(n_embed, n_units, u_type=:gru)\n",
    "        decoder = Recurrent(n_embed, n_units, u_type=:gru)\n",
    "        predict = Linear(n_units, n_vocab_out)\n",
    "\n",
    "        return new(embed_enc, embed_dec, encoder, decoder,\n",
    "            predict, drop,\n",
    "            n_vocab_in, n_vocab_out, n_embed, n_units)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training signature\n",
    "\n",
    "includes the following steps:\n",
    "+ run the source sequence througth a rnn layer\n",
    "+ transfer hidden states from encoder to decoder\n",
    "+ start the decoder with the embedded target sequence (and return all states from all steps)\n",
    "+ calculate and return loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s2s::S2S)(i, o)\n",
    "\n",
    "    seqlen_i = size(i)[1]\n",
    "    seqlen_o = size(o)[1]\n",
    "    i = reshape(i, seqlen_i, :)\n",
    "    o = reshape(o, seqlen_o, :)\n",
    "    \n",
    "    x = s2s.embed_enc(i)    # no <start>/<end> tags\n",
    "    x = s2s.drop(x)\n",
    "    h = s2s.encoder(x, hidden_states=0)\n",
    "    #c = get_cell_states(s2s.encoder)\n",
    " \n",
    "    y = s2s.embed_dec(o[1:end-1,:])\n",
    "    h_dec = s2s.decoder(y, hidden_states=h, return_all=true)\n",
    "    p = s2s.predict(h_dec)\n",
    "    loss = nll(p, o[2:end,:])\n",
    "    \n",
    "    return loss\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict signature\n",
    "\n",
    "is very similar to the trainin signature, except of the decoder part\n",
    "that now generates a step of the output sequence in every turn \n",
    "until the `<end>`-token is detected:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s2s::S2S)(i)\n",
    "\n",
    "    seqlen_i = size(i)[1]\n",
    "    i = reshape(i, seqlen_i, :)\n",
    "    \n",
    "    mb = size(i)[end]\n",
    "    \n",
    "    x = s2s.embed_enc(i)\n",
    "    h = s2s.encoder(x, hidden_states=0)\n",
    "    set_hidden_states(s2s.decoder, h)\n",
    "\n",
    "    output = blowup_array([TOKEN_START], mb)\n",
    "    outstep = blowup_array([TOKEN_START], mb)\n",
    "\n",
    "    MAX_LEN = 16\n",
    "    step = 0\n",
    "    while !all(outstep .== TOKEN_END) && step < MAX_LEN\n",
    "        step += 1\n",
    "        dec_in = s2s.embed_dec(outstep)\n",
    "        h = s2s.decoder(dec_in, hidden_states=nothing)\n",
    "        \n",
    "        y = softmax(s2s.predict(h), dims=1)\n",
    "        outstep = de_embed(y)\n",
    "        output = vcat(output, outstep)\n",
    "    end\n",
    "\n",
    "    return output\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example data\n",
    "Just to test the signatures, we will translate 4 (most?) important sentences from \n",
    "German to English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = AbstractString[]\n",
    "push!(de, \"Ich programmiere immer in Julia\")\n",
    "push!(de, \"Peter liebt Python\")\n",
    "push!(de, \"Wir alle lieben Julia\")\n",
    "push!(de, \"Ich liebe Julia\")\n",
    "\n",
    "en = AbstractString[]\n",
    "push!(en, \"I always code Julia\")\n",
    "push!(en, \"Peter loves Python\")\n",
    "push!(en, \"We all love Julia\")\n",
    "push!(en, \"I love Julia\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en = AbstractString[\"I always code Julia\", \"Peter loves Python\", \"We all love Julia\", \"I love Julia\"]\n",
      "de = AbstractString[\"Ich programmiere immer in Julia\", \"Peter liebt Python\", \"Wir alle lieben Julia\", \"Ich liebe Julia\"]\n"
     ]
    }
   ],
   "source": [
    "@show en\n",
    "@show de;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minibatch is a tuple of 2 matrices x and y with one column per sequence.    \n",
    "`prepare_corpus()` does some cleaning and calls the *NNHelferlein*-Function\n",
    "`secuence_minibatch()` which returns an iterator over the (x,y)-tuples and teh vocabularies \n",
    "for source and target language.\n",
    "\n",
    "The argument combination `partial=true, x_padding=false` prevents x-sequences to be padded\n",
    "and constructs smaller minibatches instead if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_corpus (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function prepare_corpus(source, target; batchsize=128, \n",
    "                        vocab_size=nothing)\n",
    "    source = clean_sentence.(source)\n",
    "    target = clean_sentence.(target)\n",
    "    \n",
    "    src_vocab = WordTokenizer(source, len=vocab_size)\n",
    "    trg_vocab = WordTokenizer(target, len=vocab_size)\n",
    "    \n",
    "    src = src_vocab(source, add_ctls=false)\n",
    "    trg = trg_vocab(target, add_ctls=true)\n",
    "\n",
    "    src = truncate_sequence.(src, 10, end_token=nothing)\n",
    "    trg = truncate_sequence.(trg, 10, end_token=TOKEN_END)\n",
    "    \n",
    "    return sequence_minibatch(src, trg, batchsize, shuffle=true, seq2seq=true, \n",
    "                              pad=TOKEN_END, partial=true, x_padding=false), \n",
    "           src_vocab, trg_vocab\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SequenceData(Any[(Int32[10 6; 9 8; 13 5], Int32[1 1; 8 6; … ; 10 5; 2 2]), (Int32[15; 11; 16; 5;;], Int32[1; 9; … ; 5; 2;;]), (Int32[6; 12; … ; 14; 5;;], Int32[1; 6; … ; 5; 2;;])], 3, [1, 2, 3], true), WordTokenizer(16, Dict{String, Int32}(\"immer\" => 7, \"liebe\" => 8, \"liebt\" => 9, \"<start>\" => 1, \"Peter\" => 10, \"alle\" => 11, \"programmiere\" => 12, \"Julia\" => 5, \"Python\" => 13, \"in\" => 14…), [\"<start>\", \"<end>\", \"<pad>\", \"<unknown>\", \"Julia\", \"Ich\", \"immer\", \"liebe\", \"liebt\", \"Peter\", \"alle\", \"programmiere\", \"Python\", \"in\", \"Wir\", \"lieben\"]), WordTokenizer(14, Dict{String, Int32}(\"We\" => 9, \"code\" => 11, \"<start>\" => 1, \"Peter\" => 8, \"Julia\" => 5, \"love\" => 7, \"Python\" => 10, \"<unknown>\" => 4, \"<pad>\" => 3, \"loves\" => 13…), [\"<start>\", \"<end>\", \"<pad>\", \"<unknown>\", \"Julia\", \"I\", \"love\", \"Peter\", \"We\", \"Python\", \"code\", \"always\", \"loves\", \"all\"]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfun, de_vocab, en_vocab = prepare_corpus(de, en, batchsize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train:\n",
    "\n",
    "For this simple toy-problem, a tiny rnn may be sufficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,16)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,14)), identity), Recurrent(6, 16, :gru, GRU(input=6,hidden=16)), Recurrent(6, 16, :gru, GRU(input=6,hidden=16)), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(14,16)), P(Knet.KnetArrays.KnetVector{Float32}(14)), identity), Dropout(0.1), 16, 14, 6, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EMBED = 6\n",
    "N_UNITS = 16\n",
    "s2s = S2S(N_EMBED, N_UNITS, length(de_vocab), length(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 200 epochs with 3 minibatches/epoch.\n",
      "Evaluation is performed every 1 minibatches with 1 mbs.\n",
      "Watch the progress with TensorBoard at:\n",
      "/data/aNN/Helferlein/logs/de-en-gru/2022-02-06T09-02-36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:21\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with:\n",
      "Training loss:       0.12291465451320012\n",
      "Training accuracy:   1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,16)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,14)), identity), Recurrent(6, 16, :gru, GRU(input=6,hidden=16)), Recurrent(6, 16, :gru, GRU(input=6,hidden=16)), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(14,16)), P(Knet.KnetArrays.KnetVector{Float32}(14)), identity), Dropout(0.1), 16, 14, 6, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_train!(s2s, Adam, dfun, split=nothing, epochs=200, tb_name=\"de-en-gru\",\n",
    "    acc_fun=hamming_acc,\n",
    "    mb_loss_freq=100, checkpoints=nothing, eval_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train for some seconds and define a last function, that helps to translate directly and test the RNN:   \n",
    "The function does:\n",
    "+ transform a sentence in the source language into a list of word-tokens, using\n",
    "  the source vocab.\n",
    "+ run the sequence througth the RNN\n",
    "+ use the target vocab to transform the sequence of tokens back into a sentence\n",
    "  in the target language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translate (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function translate(inp::T; mdl=s2s, sv=de_vocab, tv=en_vocab) where {T <: AbstractString}\n",
    "    \n",
    "    in_seq = sv(inp, split_words=true, add_ctls=false)\n",
    "    in_seq = reshape(in_seq, (:,1))\n",
    "    out_seq = mdl(in_seq)\n",
    "    return tv(out_seq)\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I love Julia <end>\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich liebe Julia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I always code Julia <end>\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich programmiere immer in Julia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Peter loves Python <end>\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Peter liebt Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> We all love Julia <end>\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Wir alle lieben Julia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More realistic data from Tatoeba:\n",
    "\n",
    "It is not at all surprising that our rnn is able to memorise 4 sentences - the example \n",
    "is just a check for the s2s-network and the tools.\n",
    "\n",
    "As *NNHelferlein* provides direct access to Tatoeba data, we can train a rnn on a larger\n",
    "dataset. The Tatoeba German-English corpus includes about 250000 sentences an can be \n",
    "easily accesses as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir = normpath(joinpath(dirname(pathof(#= /root/.julia/packages/NNHelferlein/poh1A/src/texts.jl:314 =# @__MODULE__())), \"..\", \"data\", \"Tatoeba\")) = \"/root/.julia/packages/NNHelferlein/poh1A/data/Tatoeba\"\n",
      "pathname = joinpath(dir, fname) = \"/root/.julia/packages/NNHelferlein/poh1A/data/Tatoeba/deu-eng.zip\"\n",
      "Corpus for language deu is already downloaded.\n",
      "Reading Tatoeba corpus for languages en-deu\n",
      "\r\n",
      "importing sentences: 1000\r\n",
      "importing sentences: 2000\r\n",
      "importing sentences: 3000\r\n",
      "importing sentences: 4000\r\n",
      "importing sentences: 5000\r\n",
      "importing sentences: 6000\r\n",
      "importing sentences: 7000\r\n",
      "importing sentences: 8000\r\n",
      "importing sentences: 9000\r\n",
      "importing sentences: 10000\r\n",
      "importing sentences: 11000\r\n",
      "importing sentences: 12000\r\n",
      "importing sentences: 13000\r\n",
      "importing sentences: 14000\r\n",
      "importing sentences: 15000\r\n",
      "importing sentences: 16000\r\n",
      "importing sentences: 17000\r\n",
      "importing sentences: 18000\r\n",
      "importing sentences: 19000\r\n",
      "importing sentences: 20000\r\n",
      "importing sentences: 21000\r\n",
      "importing sentences: 22000\r\n",
      "importing sentences: 23000\r\n",
      "importing sentences: 24000\r\n",
      "importing sentences: 25000\r\n",
      "importing sentences: 26000\r\n",
      "importing sentences: 27000\r\n",
      "importing sentences: 28000\r\n",
      "importing sentences: 29000\r\n",
      "importing sentences: 30000\r\n",
      "importing sentences: 31000\r\n",
      "importing sentences: 32000\r\n",
      "importing sentences: 33000\r\n",
      "importing sentences: 34000\r\n",
      "importing sentences: 35000\r\n",
      "importing sentences: 36000\r\n",
      "importing sentences: 37000\r\n",
      "importing sentences: 38000\r\n",
      "importing sentences: 39000\r\n",
      "importing sentences: 40000\r\n",
      "importing sentences: 41000\r\n",
      "importing sentences: 42000\r\n",
      "importing sentences: 43000\r\n",
      "importing sentences: 44000\r\n",
      "importing sentences: 45000\r\n",
      "importing sentences: 46000\r\n",
      "importing sentences: 47000\r\n",
      "importing sentences: 48000\r\n",
      "importing sentences: 49000\r\n",
      "importing sentences: 50000\r\n",
      "importing sentences: 51000\r\n",
      "importing sentences: 52000\r\n",
      "importing sentences: 53000\r\n",
      "importing sentences: 54000\r\n",
      "importing sentences: 55000\r\n",
      "importing sentences: 56000\r\n",
      "importing sentences: 57000\r\n",
      "importing sentences: 58000\r\n",
      "importing sentences: 59000\r\n",
      "importing sentences: 60000\r\n",
      "importing sentences: 61000\r\n",
      "importing sentences: 62000\r\n",
      "importing sentences: 63000\r\n",
      "importing sentences: 64000\r\n",
      "importing sentences: 65000\r\n",
      "importing sentences: 66000\r\n",
      "importing sentences: 67000\r\n",
      "importing sentences: 68000\r\n",
      "importing sentences: 69000\r\n",
      "importing sentences: 70000\r\n",
      "importing sentences: 71000\r\n",
      "importing sentences: 72000\r\n",
      "importing sentences: 73000\r\n",
      "importing sentences: 74000\r\n",
      "importing sentences: 75000\r\n",
      "importing sentences: 76000\r\n",
      "importing sentences: 77000\r\n",
      "importing sentences: 78000\r\n",
      "importing sentences: 79000\r\n",
      "importing sentences: 80000\r\n",
      "importing sentences: 81000\r\n",
      "importing sentences: 82000\r\n",
      "importing sentences: 83000\r\n",
      "importing sentences: 84000\r\n",
      "importing sentences: 85000\r\n",
      "importing sentences: 86000\r\n",
      "importing sentences: 87000\r\n",
      "importing sentences: 88000\r\n",
      "importing sentences: 89000\r\n",
      "importing sentences: 90000\r\n",
      "importing sentences: 91000\r\n",
      "importing sentences: 92000\r\n",
      "importing sentences: 93000\r\n",
      "importing sentences: 94000\r\n",
      "importing sentences: 95000\r\n",
      "importing sentences: 96000\r\n",
      "importing sentences: 97000\r\n",
      "importing sentences: 98000\r\n",
      "importing sentences: 99000\r\n",
      "importing sentences: 100000\r\n",
      "importing sentences: 101000\r\n",
      "importing sentences: 102000\r\n",
      "importing sentences: 103000\r\n",
      "importing sentences: 104000\r\n",
      "importing sentences: 105000\r\n",
      "importing sentences: 106000\r\n",
      "importing sentences: 107000\r\n",
      "importing sentences: 108000\r\n",
      "importing sentences: 109000\r\n",
      "importing sentences: 110000\r\n",
      "importing sentences: 111000\r\n",
      "importing sentences: 112000\r\n",
      "importing sentences: 113000\r\n",
      "importing sentences: 114000\r\n",
      "importing sentences: 115000\r\n",
      "importing sentences: 116000\r\n",
      "importing sentences: 117000\r\n",
      "importing sentences: 118000\r\n",
      "importing sentences: 119000\r\n",
      "importing sentences: 120000\r\n",
      "importing sentences: 121000\r\n",
      "importing sentences: 122000\r\n",
      "importing sentences: 123000\r\n",
      "importing sentences: 124000\r\n",
      "importing sentences: 125000\r\n",
      "importing sentences: 126000\r\n",
      "importing sentences: 127000\r\n",
      "importing sentences: 128000\r\n",
      "importing sentences: 129000\r\n",
      "importing sentences: 130000\r\n",
      "importing sentences: 131000\r\n",
      "importing sentences: 132000\r\n",
      "importing sentences: 133000\r\n",
      "importing sentences: 134000\r\n",
      "importing sentences: 135000\r\n",
      "importing sentences: 136000\r\n",
      "importing sentences: 137000\r\n",
      "importing sentences: 138000\r\n",
      "importing sentences: 139000\r\n",
      "importing sentences: 140000\r\n",
      "importing sentences: 141000\r\n",
      "importing sentences: 142000\r\n",
      "importing sentences: 143000\r\n",
      "importing sentences: 144000\r\n",
      "importing sentences: 145000\r\n",
      "importing sentences: 146000\r\n",
      "importing sentences: 147000\r\n",
      "importing sentences: 148000\r\n",
      "importing sentences: 149000\r\n",
      "importing sentences: 150000\r\n",
      "importing sentences: 151000\r\n",
      "importing sentences: 152000\r\n",
      "importing sentences: 153000\r\n",
      "importing sentences: 154000\r\n",
      "importing sentences: 155000\r\n",
      "importing sentences: 156000\r\n",
      "importing sentences: 157000\r\n",
      "importing sentences: 158000\r\n",
      "importing sentences: 159000\r\n",
      "importing sentences: 160000\r\n",
      "importing sentences: 161000\r\n",
      "importing sentences: 162000\r\n",
      "importing sentences: 163000\r\n",
      "importing sentences: 164000\r\n",
      "importing sentences: 165000\r\n",
      "importing sentences: 166000\r\n",
      "importing sentences: 167000\r\n",
      "importing sentences: 168000\r\n",
      "importing sentences: 169000\r\n",
      "importing sentences: 170000\r\n",
      "importing sentences: 171000\r\n",
      "importing sentences: 172000\r\n",
      "importing sentences: 173000\r\n",
      "importing sentences: 174000\r\n",
      "importing sentences: 175000\r\n",
      "importing sentences: 176000\r\n",
      "importing sentences: 177000\r\n",
      "importing sentences: 178000\r\n",
      "importing sentences: 179000\r\n",
      "importing sentences: 180000\r\n",
      "importing sentences: 181000\r\n",
      "importing sentences: 182000\r\n",
      "importing sentences: 183000\r\n",
      "importing sentences: 184000\r\n",
      "importing sentences: 185000\r\n",
      "importing sentences: 186000\r\n",
      "importing sentences: 187000\r\n",
      "importing sentences: 188000\r\n",
      "importing sentences: 189000\r\n",
      "importing sentences: 190000\r\n",
      "importing sentences: 191000\r\n",
      "importing sentences: 192000\r\n",
      "importing sentences: 193000\r\n",
      "importing sentences: 194000\r\n",
      "importing sentences: 195000\r\n",
      "importing sentences: 196000\r\n",
      "importing sentences: 197000\r\n",
      "importing sentences: 198000\r\n",
      "importing sentences: 199000\r\n",
      "importing sentences: 200000\r\n",
      "importing sentences: 201000\r\n",
      "importing sentences: 202000\r\n",
      "importing sentences: 203000\r\n",
      "importing sentences: 204000\r\n",
      "importing sentences: 205000\r\n",
      "importing sentences: 206000\r\n",
      "importing sentences: 207000\r\n",
      "importing sentences: 208000\r\n",
      "importing sentences: 209000\r\n",
      "importing sentences: 210000\r\n",
      "importing sentences: 211000\r\n",
      "importing sentences: 212000\r\n",
      "importing sentences: 213000\r\n",
      "importing sentences: 214000\r\n",
      "importing sentences: 215000\r\n",
      "importing sentences: 216000\r\n",
      "importing sentences: 217000\r\n",
      "importing sentences: 218000\r\n",
      "importing sentences: 219000\r\n",
      "importing sentences: 220000\r\n",
      "importing sentences: 221000\r\n",
      "importing sentences: 222000\r\n",
      "importing sentences: 223000\r\n",
      "importing sentences: 224000\r\n",
      "importing sentences: 225000\r\n",
      "importing sentences: 226000\r\n",
      "importing sentences: 227000\r\n",
      "importing sentences: 228000\r\n",
      "importing sentences: 229000\r\n",
      "importing sentences: 230000\r\n",
      "importing sentences: 231000\r\n",
      "importing sentences: 232000\r\n",
      "importing sentences: 233000\r\n",
      "importing sentences: 234000\r\n",
      "importing sentences: 235000\r\n",
      "importing sentences: 236000\r\n",
      "importing sentences: 237000\r\n",
      "importing sentences: 238000\r\n",
      "importing sentences: 239000\r\n",
      "importing sentences: 240000\r\n",
      "importing sentences: 241000\r\n",
      "importing sentences: 242000\r\n",
      "importing sentences: 243000\r\n",
      "importing sentences: 244000\r\n",
      "importing sentences: 245000\r\n",
      "importing sentences: 246000\r\n",
      "importing sentences: 247000\r\n",
      "importing sentences: 248000\r\n",
      "importing sentences: 249000"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SequenceData(Any[(Int32[765 2169 … 3604 37402], Int32[1 1 … 1 1; 642 2952 … 673 10459; 2 2 … 71 2; 2 2 … 2 2]), (Int32[40981 18026 … 9596 5149], Int32[1 1 … 1 1; 10459 10459 … 543 164; … ; 2 2 … 2 134; 2 2 … 2 2]), (Int32[31449 24262 … 25915 15237], Int32[1 1 … 1 1; 1141 5769 … 68 21; … ; 2 2 … 748 920; 2 2 … 2 2]), (Int32[6427 737 … 10948 10948; 1584 2708 … 213 213], Int32[1 1 … 1 1; 2952 14335 … 3924 3924; 2 2 … 19 19; 2 2 … 2 2]), (Int32[2140 1201 … 13878 6; 5 1200 … 29 150], Int32[1 1 … 1 1; 543 543 … 6858 6800; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[3202 5514 … 4282 4282; 16 16 … 2820 423], Int32[1 1 … 1 1; 1494 1494 … 160 160; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[837 25 … 5 1246; 375 768 … 1539 313], Int32[1 1 … 1 1; 160 26 … 6 5189; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[4834 4834 … 778 1997; 22 22 … 3570 12], Int32[1 1 … 1 1; 3372 3372 … 1615 1615; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[3314 70 … 3734 1565; 12 1510 … 3065 1336], Int32[1 1 … 1 1; 1615 70 … 1236 1236; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[1565 1565 … 1495 37856; 1336 2939 … 4283 4283], Int32[1 1 … 1 1; 1236 1236 … 7593 7593; … ; 2 2 … 2 2; 2 2 … 2 2])  …  (Int32[68 326 … 5 5; 6312 249 … 186 111; … ; 29394 12 … 17 202; 20 5946 … 36 266], Int32[1 1 … 1 1; 25 86 … 6 6; … ; 3063 999 … 33 228; 2 2 … 2 2]), (Int32[5 35 … 6281 530; 354 2069 … 950 20; … ; 30 14 … 57 5264; 630 319 … 11 44], Int32[1 1 … 1 1; 6 6 … 3944 679; … ; 7 40 … 929 162; 2 2 … 2 2]), (Int32[53 10 … 784 10; 32901 785 … 3137 44; … ; 25212 1563 … 38341 12; 57 1768 … 155 43], Int32[1 1 … 1 1; 496 55 … 1492 21; … ; 127 79 … 3164 113; 2 2 … 2 2]), (Int32[37 281 … 5 5; 483 30035 … 129 390; … ; 14 21524 … 22609 3742; 265 20646 … 18127 56], Int32[1 1 … 1 1; 21 206 … 6 6; … ; 127 388 … 2879 56; 2 2 … 2 2]), (Int32[117 82 … 5 5; 2150 13 … 32 32; … ; 56 29 … 7 1060; 36 12 … 79 138], Int32[1 1 … 1 1; 6 83 … 6 6; … ; 35 8 … 63 34; 2 2 … 2 2]), (Int32[5 5 … 6 6; 970 8 … 390 41; … ; 756 88 … 2798 647; 27 712 … 29 6123], Int32[1 1 … 1 1; 6 6 … 5 5; … ; 7550 141 … 7 14; 2 2 … 2 2]), (Int32[33 53 … 557 6; 191 647 … 15 368; … ; 3550 17 … 35 13; 2065 14 … 8 113], Int32[1 1 … 1 1; 24 982 … 664 5; … ; 81 9 … 748 39; 2 2 … 2 2]), (Int32[6 32172 … 18284 5; 502 13273 … 17653 8; … ; 15 10238 … 3953 1119; 128 26 … 297 7969], Int32[1 1 … 1 1; 5 19002 … 11066 6; … ; 2316 36 … 352 35; 2 2 … 2 2]), (Int32[5 34 … 211 6; 28 295 … 1222 141; … ; 970 433 … 219 47; 324 6512 … 407 3424], Int32[1 1 … 1 1; 6 30 … 452 5520; … ; 975 23 … 16 144; 2 2 … 2 2]), (Int32[37 1795 … 6 804; 74 103 … 71 1524; … ; 867 12217 … 36459 1167; 54 33568 … 18 208], Int32[1 1 … 1 1; 21 843 … 5 13328; … ; 69 12 … 32 9; 2 2 … 2 2])], 3900, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900], true), WordTokenizer(41035, Dict{String, Int32}(\"hereinbat\" => 22997, \"Egozentriker\" => 16950, \"Wörterbuches\" => 22998, \"hundertzwanzig\" => 22999, \"null\" => 8561, \"schmerzlicher\" => 23000, \"Zweiten\" => 6246, \"kandidiert\" => 16951, \"gleichseitiges\" => 23001, \"Bio-Lebensmittel\" => 13823…), [\"<start>\", \"<end>\", \"<pad>\", \"<unknown>\", \"Tom\", \"Ich\", \"nicht\", \"ist\", \"zu\", \"Sie\"  …  \"Wohnheim\", \"schnorcheln\", \"gewürztes\", \"Zirkel\", \"Löffelchen\", \"Spritzer\", \"Kokons\", \"sträubt\", \"verzweifeln\", \"Lampenschirm\"]), WordTokenizer(19306, Dict{String, Int32}(\"irreplaceable\" => 5063, \"waster\" => 8893, \"inattentive\" => 13116, \"frowning\" => 13117, \"sleepwalking\" => 10479, \"dumber\" => 10480, \"Secure\" => 13118, \"Tuberculosis\" => 13119, \"melons\" => 7844, \"gout\" => 13120…), [\"<start>\", \"<end>\", \"<pad>\", \"<unknown>\", \"I\", \"Tom\", \"to\", \"you\", \"the\", \"t\"  …  \"forties\", \"non-violent\", \"Nozomi\", \"Phase\", \"roaches\", \"petting\", \"smuggled\", \"immunization\", \"devotes\", \"wavering\"]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en, de = get_tatoeba_corpus(\"deu\")\n",
    "# en = en[1000:end]; de = de[1000:end]\n",
    "dtato, de_vocab, en_vocab = prepare_corpus(de, en, batchsize=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the more realistic training data still single layer of 512 LSTM units is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,41035)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,19306)), identity), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512)), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512)), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(19306,512)), P(Knet.KnetArrays.KnetVector{Float32}(19306)), identity), Dropout(0.1), 41035, 19306, 1024, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EMBED = 1024\n",
    "N_UNITS = 512\n",
    "s2s = S2S(N_EMBED, N_UNITS, length(de_vocab), length(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 20 epochs with 3900 minibatches/epoch.\n",
      "Evaluation is performed every 390 minibatches with 780 mbs.\n",
      "Watch the progress with TensorBoard at:\n",
      "/data/aNN/Helferlein/logs/de-en-gru/2022-02-06T09-09-37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 3:32:33\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with:\n",
      "Training loss:       0.23689512871134166\n",
      "Training accuracy:   0.8662990285224409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,41035)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,19306)), identity), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512)), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512)), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(19306,512)), P(Knet.KnetArrays.KnetVector{Float32}(19306)), identity), Dropout(0.1), 41035, 19306, 1024, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_train!(s2s, Adam, dtato, split=nothing, epochs=20, tb_name=\"de-en-gru\",\n",
    "    acc_fun=hamming_acc,\n",
    "    mb_loss_freq=1000, checkpoints=5, eval_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Tom usually listens to classical music while <end>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Tom hört gewöhnlich klassische Musik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Tom almost always wears dark clothes <end>\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Tom trägt fast immer dunkle Kleidung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> How much beer should I buy <end>\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Wie viel Bier soll ich kaufen?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I need some hot water <end>\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich brauche eine Mütze voll Schlaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I have to drink more coffee <end>\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich muss mehr Kaffee trinken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Tom needs to drink more coffee <end>\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Tom muss mehr Kaffee trinken\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (32 threads) 1.7.0",
   "language": "julia",
   "name": "julia-(32-threads)-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
