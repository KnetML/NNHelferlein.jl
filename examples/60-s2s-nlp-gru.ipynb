{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62357945",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence RNN for machine translation\n",
    "\n",
    "The notebook shows how to implement a recurrent neural network for machine translation \n",
    "with help of Knet and NNHelferlein.\n",
    "The net uses a Tatoeba-corpus to train a one-layer gru network. \n",
    "The resulting network demostrates the abilities of such an architecture - however the \n",
    "training corpus ist much too small to be sufficient for a professional\n",
    "translator; and the network should have more layers and more units per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2ecf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, StatsBase\n",
    "using Knet, AutoGrad\n",
    "using NNHelferlein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbdc17",
   "metadata": {},
   "source": [
    "### The seq-2-seq-model\n",
    "\n",
    "The sequence-to-sequence model is simple. We need\n",
    "+ the type\n",
    "+ a constructor\n",
    "+ signatures for training (with 2 sequences as arguments) and for prediction (with only the \n",
    "  source signature as arg)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50cabd3",
   "metadata": {},
   "source": [
    "#### Type and constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78c1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct S2S\n",
    "    embed_enc       # embed layer for encoder\n",
    "    embed_dec       # embed layer for decoder\n",
    "    encoder         # encoder rnn\n",
    "    decoder         # decoder rnn\n",
    "    predict         # predict layer (Linear w/o actf)\n",
    "    drop            # dropout layer\n",
    "    voc_in; voc_out # vocab sizes\n",
    "    embed           # embedding depth\n",
    "    units           # number of lstm units in layers\n",
    "\n",
    "    function S2S(n_embed, n_units, n_vocab_in, n_vocab_out)\n",
    "        embed_enc = Embed(n_vocab_in, n_embed)\n",
    "        drop = Dropout(0.1)\n",
    "        embed_dec = Embed(n_vocab_out, n_embed)\n",
    "        encoder = Recurrent(n_embed, n_units, u_type=:gru)\n",
    "        decoder = Recurrent(n_embed, n_units, u_type=:gru)\n",
    "        predict = Linear(n_units, n_vocab_out)\n",
    "\n",
    "        return new(embed_enc, embed_dec, encoder, decoder,\n",
    "            predict, drop,\n",
    "            n_vocab_in, n_vocab_out, n_embed, n_units)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec0ac0",
   "metadata": {},
   "source": [
    "#### Training signature\n",
    "\n",
    "includes the following steps:\n",
    "+ run the source sequence througth a rnn layer\n",
    "+ transfer hidden states from encoder to decoder\n",
    "+ start the decoder with the embedded target sequence (and return all states from all steps)\n",
    "+ calculate and return loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0628a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s2s::S2S)(i, o)\n",
    "\n",
    "    seqlen_i = size(i)[1]\n",
    "    seqlen_o = size(o)[1]\n",
    "    i = reshape(i, seqlen_i, :)\n",
    "    o = reshape(o, seqlen_o, :)\n",
    "    \n",
    "    x = s2s.embed_enc(i)    # no <start>/<end> tags\n",
    "    x = s2s.drop(x)\n",
    "    h = s2s.encoder(x, h=0)\n",
    " \n",
    "    y = s2s.embed_dec(o[1:end-1,:])\n",
    "    h_dec = s2s.decoder(y, h=h, return_all=true)\n",
    "    p = s2s.predict(h_dec)\n",
    "    loss = nll(p, o[2:end,:])\n",
    "    \n",
    "    return loss\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e329e08",
   "metadata": {},
   "source": [
    "#### Predict signature\n",
    "\n",
    "is very similar to the trainin signature, except of the decoder part\n",
    "that now generates a step of the output sequence in every turn \n",
    "until the `<end>`-token is detected:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04bdb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s2s::S2S)(i)\n",
    "\n",
    "    seqlen_i = size(i)[1]\n",
    "    i = reshape(i, seqlen_i, :)\n",
    "    \n",
    "    mb = size(i)[end]\n",
    "    \n",
    "    x = s2s.embed_enc(i)\n",
    "    h = s2s.encoder(x, h=0)\n",
    "    set_hidden_states!(s2s.decoder, h)\n",
    "\n",
    "    output = blowup_array([TOKEN_START], mb)\n",
    "    outstep = blowup_array([TOKEN_START], mb)\n",
    "\n",
    "    MAX_LEN = 16\n",
    "    step = 0\n",
    "    while !all(outstep .== TOKEN_END) && step < MAX_LEN\n",
    "        step += 1\n",
    "        dec_in = s2s.embed_dec(outstep)\n",
    "        h = s2s.decoder(dec_in, h=nothing)\n",
    "        \n",
    "        y = softmax(s2s.predict(h), dims=1)\n",
    "        outstep = de_embed(y)\n",
    "        output = vcat(output, outstep)\n",
    "    end\n",
    "\n",
    "    return output\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2faf8c",
   "metadata": {},
   "source": [
    "### Example data\n",
    "Just to test the signatures, we will translate 4 (most?) important sentences from \n",
    "German to English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908ce950",
   "metadata": {},
   "outputs": [],
   "source": [
    "de = AbstractString[]\n",
    "push!(de, \"Ich programmiere immer in Julia\")\n",
    "push!(de, \"Peter liebt Python\")\n",
    "push!(de, \"Wir alle lieben Julia\")\n",
    "push!(de, \"Ich liebe Julia\")\n",
    "\n",
    "en = AbstractString[]\n",
    "push!(en, \"I always code Julia\")\n",
    "push!(en, \"Peter loves Python\")\n",
    "push!(en, \"We all love Julia\")\n",
    "push!(en, \"I love Julia\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3565dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en = AbstractString[\"I always code Julia\", \"Peter loves Python\", \"We all love Julia\", \"I love Julia\"]\n",
      "de = AbstractString[\"Ich programmiere immer in Julia\", \"Peter liebt Python\", \"Wir alle lieben Julia\", \"Ich liebe Julia\"]\n"
     ]
    }
   ],
   "source": [
    "@show en\n",
    "@show de;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0d271",
   "metadata": {},
   "source": [
    "The minibatch is a tuple of 2 matrices x and y with one column per sequence.    \n",
    "`prepare_corpus()` does some cleaning and calls the *NNHelferlein*-Function\n",
    "`secuence_minibatch()` which returns an iterator over the (x,y)-tuples and teh vocabularies \n",
    "for source and target language.\n",
    "\n",
    "The argument combination `partial=true, x_padding=false` prevents x-sequences to be padded\n",
    "and constructs smaller minibatches instead if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d6d9cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_corpus (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function prepare_corpus(source, target; batchsize=128, \n",
    "                        vocab_size=nothing)\n",
    "    source = clean_sentence.(source)\n",
    "    target = clean_sentence.(target)\n",
    "    \n",
    "    src_vocab = WordTokenizer(source, len=vocab_size)\n",
    "    trg_vocab = WordTokenizer(target, len=vocab_size)\n",
    "    \n",
    "    src = src_vocab(source, add_ctls=false)\n",
    "    trg = trg_vocab(target, add_ctls=true)\n",
    "\n",
    "    src = truncate_sequence.(src, 10, end_token=nothing)\n",
    "    trg = truncate_sequence.(trg, 10, end_token=TOKEN_END)\n",
    "    \n",
    "    return sequence_minibatch(src, trg, batchsize, shuffle=true, seq2seq=true, \n",
    "                              pad=TOKEN_END, partial=true, x_padding=true), \n",
    "           src_vocab, trg_vocab\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fca00e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SequenceData(Any[(Int32[10 6; 9 8; 13 5], Int32[1 1; 8 6; … ; 10 5; 2 2]), (Int32[15 6; 11 12; … ; 5 14; 2 5], Int32[1 1; 9 6; … ; 5 5; 2 2])], 2, [1, 2], true), WordTokenizer(16, Dict{String, Int32}(\"immer\" => 7, \"liebe\" => 8, \"liebt\" => 9, \"<start>\" => 1, \"Peter\" => 10, \"alle\" => 11, \"programmiere\" => 12, \"Julia\" => 5, \"Python\" => 13, \"in\" => 14…), [\"<start>\", \"<end>\", \"<pad>\", \"<unknown>\", \"Julia\", \"Ich\", \"immer\", \"liebe\", \"liebt\", \"Peter\", \"alle\", \"programmiere\", \"Python\", \"in\", \"Wir\", \"lieben\"]), WordTokenizer(14, Dict{String, Int32}(\"We\" => 9, \"code\" => 11, \"<start>\" => 1, \"Peter\" => 8, \"Julia\" => 5, \"love\" => 7, \"Python\" => 10, \"<unknown>\" => 4, \"<pad>\" => 3, \"loves\" => 13…), [\"<start>\", \"<end>\", \"<pad>\", \"<unknown>\", \"Julia\", \"I\", \"love\", \"Peter\", \"We\", \"Python\", \"code\", \"always\", \"loves\", \"all\"]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfun, de_vocab, en_vocab = prepare_corpus(de, en, batchsize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7a5d4",
   "metadata": {},
   "source": [
    "### Train:\n",
    "\n",
    "For this simple toy-problem, a tiny rnn may be sufficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d58223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,16)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,14)), identity), Recurrent(6, 16, :gru, GRU(input=6,hidden=16), true), Recurrent(6, 16, :gru, GRU(input=6,hidden=16), true), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(14,16)), P(Knet.KnetArrays.KnetVector{Float32}(14)), identity), Dropout(0.1), 16, 14, 6, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EMBED = 6\n",
    "N_UNITS = 16\n",
    "s2s = S2S(N_EMBED, N_UNITS, length(de_vocab), length(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa6c7d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 200 epochs with 2 minibatches/epoch.\n",
      "Evaluation is performed every 1 minibatches with 1 mbs.\n",
      "Watch the progress with TensorBoard at:\n",
      "/data/aNN/Helferlein/logs/de-en-gru/2022-02-17T18-14-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with:\n",
      "Training loss:       0.1967436671257019\n",
      "Training accuracy:   1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,16)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,14)), identity), Recurrent(6, 16, :gru, GRU(input=6,hidden=16), true), Recurrent(6, 16, :gru, GRU(input=6,hidden=16), true), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(14,16)), P(Knet.KnetArrays.KnetVector{Float32}(14)), identity), Dropout(0.1), 16, 14, 6, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_train!(s2s, Adam, dfun, split=nothing, epochs=200, tb_name=\"de-en-gru\",\n",
    "    acc_fun=hamming_acc,\n",
    "    mb_loss_freq=100, checkpoints=nothing, eval_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd52f1",
   "metadata": {},
   "source": [
    "We train for some seconds and define a last function, that helps to translate directly and test the RNN:   \n",
    "The function does:\n",
    "+ transform a sentence in the source language into a list of word-tokens, using\n",
    "  the source vocab.\n",
    "+ run the sequence througth the RNN\n",
    "+ use the target vocab to transform the sequence of tokens back into a sentence\n",
    "  in the target language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f904521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translate (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function translate(inp::T; mdl=s2s, sv=de_vocab, tv=en_vocab) where {T <: AbstractString}\n",
    "    \n",
    "    in_seq = sv(inp, split_words=true, add_ctls=false)\n",
    "    in_seq = reshape(in_seq, (:,1))\n",
    "    out_seq = mdl(in_seq)\n",
    "    return tv(out_seq)\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e9d5ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I love Julia <end>\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich liebe Julia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "679e2f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I always code Julia <end>\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich programmiere immer in Julia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6eecbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Peter loves Python <end>\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Peter liebt Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1614d601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I love Julia <end>\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Wir alle lieben Julia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d76bfd",
   "metadata": {},
   "source": [
    "### More realistic data from Tatoeba:\n",
    "\n",
    "It is not at all surprising that our rnn is able to memorise 4 sentences - the example \n",
    "is just a check for the s2s-network and the tools.\n",
    "\n",
    "As *NNHelferlein* provides direct access to Tatoeba data, we can train a rnn on a larger\n",
    "dataset. The Tatoeba German-English corpus includes about 250000 sentences an can be \n",
    "easily accesses as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca0d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en, de = get_tatoeba_corpus(\"deu\")\n",
    "en = en[1000:end]; de = de[1000:end]\n",
    "dtato, de_vocab, en_vocab = prepare_corpus(de, en, batchsize=128);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b1f48",
   "metadata": {},
   "source": [
    "For the more realistic training data still single layer of 512 LSTM units is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cac0724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,40982)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,19288)), identity), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512), true), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512), true), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(19288,512)), P(Knet.KnetArrays.KnetVector{Float32}(19288)), identity), Dropout(0.1), 40982, 19288, 1024, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EMBED = 1024\n",
    "N_UNITS = 512\n",
    "s2s = S2S(N_EMBED, N_UNITS, length(de_vocab), length(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4a8a778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset for training (90%) and validation (10%).\n",
      "Training 20 epochs with 1746 minibatches/epoch and 194 validation mbs.\n",
      "Evaluation is performed every 350 minibatches with 39 mbs.\n",
      "Watch the progress with TensorBoard at:\n",
      "/data/aNN/Helferlein/logs/de-en-gru/2022-02-17T18-20-34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:22:41\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with:\n",
      "Training loss:       0.15276523946516668\n",
      "Training accuracy:   0.9048155519348606\n",
      "Validation loss:     0.1482027831208921\n",
      "Validation accuracy: 0.9093793786307477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,40982)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,19288)), identity), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512), true), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512), true), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(19288,512)), P(Knet.KnetArrays.KnetVector{Float32}(19288)), identity), Dropout(0.1), 40982, 19288, 1024, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_train!(s2s, Adam, dtato, epochs=20, tb_name=\"de-en-gru\",\n",
    "    split=0.9, eval_freq=5, eval_size=0.2, \n",
    "    acc_fun=hamming_acc, mb_loss_freq=1000, checkpoints=nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a884bd80",
   "metadata": {},
   "source": [
    "## Results:\n",
    "After about 20 minutes of training we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc149ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Tom usually listens to classical music <end>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Tom hört gewöhnlich klassische Musik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b59f5a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Tom almost always wears dark clothes <end>\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Tom trägt fast immer dunkle Kleidung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8130660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> How much beer should I buy <end>\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Wie viel Bier soll ich kaufen?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d54dc1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I need to get some shut-eye <end>\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich brauche eine Mütze voll Schlaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "419994be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I need to drink more coffee <end>\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich muss mehr Kaffee trinken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3f8e9bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Tom needs to drink more coffee <end>\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Tom muss mehr Kaffee trinken\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (8 threads) 1.7.0",
   "language": "julia",
   "name": "julia-(8-threads)-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
