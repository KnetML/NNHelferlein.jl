{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62357945",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence RNN for machine translation\n",
    "\n",
    "The notebook shows how to implement a recurrent neural network for machine translation \n",
    "with help of Knet and NNHelferlein.\n",
    "The net uses a Tatoeba-corpus to train a one-layer gru network. \n",
    "The resulting network demostrates the abilities of such an architecture - however the \n",
    "training corpus ist much too small to be sufficient for a professional\n",
    "translator; and the network should have more layers and more units per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2ecf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, StatsBase\n",
    "using Knet, AutoGrad\n",
    "using NNHelferlein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbdc17",
   "metadata": {},
   "source": [
    "### The seq-2-seq-model\n",
    "\n",
    "The sequence-to-sequence model is simple. We need\n",
    "+ the type\n",
    "+ a constructor\n",
    "+ signatures for training (with 2 sequences as arguments) and for prediction (with only the \n",
    "  source signature as arg)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50cabd3",
   "metadata": {},
   "source": [
    "#### Type and constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78c1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct S2S\n",
    "    embed_enc       # embed layer for encoder\n",
    "    embed_dec       # embed layer for decoder\n",
    "    encoder         # encoder rnn\n",
    "    decoder         # decoder rnn\n",
    "    predict         # predict layer (Linear w/o actf)\n",
    "    drop            # dropout layer\n",
    "    voc_in; voc_out # vocab sizes\n",
    "    embed           # embedding depth\n",
    "    units           # number of lstm units in layers\n",
    "\n",
    "    function S2S(n_embed, n_units, n_vocab_in, n_vocab_out)\n",
    "        embed_enc = Embed(n_vocab_in, n_embed)\n",
    "        drop = Dropout(0.1)\n",
    "        embed_dec = Embed(n_vocab_out, n_embed)\n",
    "        encoder = Recurrent(n_embed, n_units, u_type=:gru)\n",
    "        decoder = Recurrent(n_embed, n_units, u_type=:gru)\n",
    "        predict = Linear(n_units, n_vocab_out)\n",
    "\n",
    "        return new(embed_enc, embed_dec, encoder, decoder,\n",
    "            predict, drop,\n",
    "            n_vocab_in, n_vocab_out, n_embed, n_units)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec0ac0",
   "metadata": {},
   "source": [
    "#### Training signature\n",
    "\n",
    "includes the following steps:\n",
    "+ run the source sequence througth a rnn layer\n",
    "+ transfer hidden states from encoder to decoder\n",
    "+ start the decoder with the embedded target sequence (and return all states from all steps)\n",
    "+ calculate and return loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0628a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s2s::S2S)(i, o)\n",
    "\n",
    "    seqlen_i = size(i)[1]\n",
    "    seqlen_o = size(o)[1]\n",
    "    i = reshape(i, seqlen_i, :)\n",
    "    o = reshape(o, seqlen_o, :)\n",
    "    \n",
    "    x = s2s.embed_enc(i)    # no <start>/<end> tags\n",
    "    x = s2s.drop(x)\n",
    "    h = s2s.encoder(x, h=0)\n",
    " \n",
    "    y = s2s.embed_dec(o[1:end-1,:])\n",
    "    h_dec = s2s.decoder(y, h=h, return_all=true)\n",
    "    p = s2s.predict(h_dec)\n",
    "    loss = nll(p, o[2:end,:])\n",
    "    \n",
    "    return loss\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e329e08",
   "metadata": {},
   "source": [
    "#### Predict signature\n",
    "\n",
    "is very similar to the trainin signature, except of the decoder part\n",
    "that now generates a step of the output sequence in every turn \n",
    "until the `<end>`-token is detected:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04bdb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s2s::S2S)(i)\n",
    "\n",
    "    seqlen_i = size(i)[1]\n",
    "    i = reshape(i, seqlen_i, :)\n",
    "    \n",
    "    mb = size(i)[end]\n",
    "    \n",
    "    x = s2s.embed_enc(i)\n",
    "    h = s2s.encoder(x, h=0)\n",
    "    set_hidden_states!(s2s.decoder, h)\n",
    "\n",
    "    output = blowup_array([TOKEN_START], mb)\n",
    "    outstep = blowup_array([TOKEN_START], mb)\n",
    "\n",
    "    MAX_LEN = 16\n",
    "    step = 0\n",
    "    while !all(outstep .== TOKEN_END) && step < MAX_LEN\n",
    "        step += 1\n",
    "        dec_in = s2s.embed_dec(outstep)\n",
    "        h = s2s.decoder(dec_in, h=nothing)\n",
    "        \n",
    "        y = softmax(s2s.predict(h), dims=1)\n",
    "        outstep = de_embed(y)\n",
    "        output = vcat(output, outstep)\n",
    "    end\n",
    "\n",
    "    return output\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2faf8c",
   "metadata": {},
   "source": [
    "### Example data\n",
    "Just to test the signatures, we will translate 4 (most?) important sentences from \n",
    "German to English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908ce950",
   "metadata": {},
   "outputs": [],
   "source": [
    "de = AbstractString[]\n",
    "push!(de, \"Ich programmiere immer in Julia\")\n",
    "push!(de, \"Peter liebt Python\")\n",
    "push!(de, \"Wir alle lieben Julia\")\n",
    "push!(de, \"Ich liebe Julia\")\n",
    "\n",
    "en = AbstractString[]\n",
    "push!(en, \"I always code Julia\")\n",
    "push!(en, \"Peter loves Python\")\n",
    "push!(en, \"We all love Julia\")\n",
    "push!(en, \"I love Julia\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3565dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en = AbstractString[\"I always code Julia\", \"Peter loves Python\", \"We all love Julia\", \"I love Julia\"]\n",
      "de = AbstractString[\"Ich programmiere immer in Julia\", \"Peter liebt Python\", \"Wir alle lieben Julia\", \"Ich liebe Julia\"]\n"
     ]
    }
   ],
   "source": [
    "@show en\n",
    "@show de;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0d271",
   "metadata": {},
   "source": [
    "The minibatch is a tuple of 2 matrices x and y with one column per sequence.    \n",
    "`prepare_corpus()` does some cleaning and calls the *NNHelferlein*-Function\n",
    "`secuence_minibatch()` which returns an iterator over the (x,y)-tuples and teh vocabularies \n",
    "for source and target language.\n",
    "\n",
    "The argument combination `partial=true, x_padding=false` prevents x-sequences to be padded\n",
    "and constructs smaller minibatches instead if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d6d9cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_corpus (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function prepare_corpus(source, target; batchsize=128, \n",
    "                        vocab_size=nothing)\n",
    "    source = clean_sentence.(source)\n",
    "    target = clean_sentence.(target)\n",
    "    \n",
    "    src_vocab = WordTokenizer(source, len=vocab_size)\n",
    "    trg_vocab = WordTokenizer(target, len=vocab_size)\n",
    "    \n",
    "    src = src_vocab(source, add_ctls=false)\n",
    "    trg = trg_vocab(target, add_ctls=true)\n",
    "\n",
    "    src = truncate_sequence.(src, 10, end_token=nothing)\n",
    "    trg = truncate_sequence.(trg, 10, end_token=TOKEN_END)\n",
    "    \n",
    "    return sequence_minibatch(src, trg, batchsize, shuffle=true, seq2seq=true, \n",
    "                              pad=TOKEN_END, partial=true, x_padding=true), \n",
    "           src_vocab, trg_vocab\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fca00e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SequenceData(Any[(Int32[10 6; 9 8; 13 5], Int32[1 1; 8 6; … ; 10 5; 2 2]), (Int32[15 6; 11 12; … ; 5 14; 2 5], Int32[1 1; 9 6; … ; 5 5; 2 2])], 2, [1, 2], true), WordTokenizer(16, Dict{String, Int32}(\"immer\" => 7, \"liebe\" => 8, \"liebt\" => 9, \"<start>\" => 1, \"Peter\" => 10, \"alle\" => 11, \"programmiere\" => 12, \"Julia\" => 5, \"Python\" => 13, \"in\" => 14…), [\"<start>\", \"<end>\", \"<pad>\", \"<unknown>\", \"Julia\", \"Ich\", \"immer\", \"liebe\", \"liebt\", \"Peter\", \"alle\", \"programmiere\", \"Python\", \"in\", \"Wir\", \"lieben\"]), WordTokenizer(14, Dict{String, Int32}(\"We\" => 9, \"code\" => 11, \"<start>\" => 1, \"Peter\" => 8, \"Julia\" => 5, \"love\" => 7, \"Python\" => 10, \"<unknown>\" => 4, \"<pad>\" => 3, \"loves\" => 13…), [\"<start>\", \"<end>\", \"<pad>\", \"<unknown>\", \"Julia\", \"I\", \"love\", \"Peter\", \"We\", \"Python\", \"code\", \"always\", \"loves\", \"all\"]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfun, de_vocab, en_vocab = prepare_corpus(de, en, batchsize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7a5d4",
   "metadata": {},
   "source": [
    "### Train:\n",
    "\n",
    "For this simple toy-problem, a tiny rnn may be sufficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d58223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,16)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,14)), identity), Recurrent(6, 16, :gru, GRU(input=6,hidden=16), true), Recurrent(6, 16, :gru, GRU(input=6,hidden=16), true), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(14,16)), P(Knet.KnetArrays.KnetVector{Float32}(14)), identity), Dropout(0.1), 16, 14, 6, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EMBED = 6\n",
    "N_UNITS = 16\n",
    "s2s = S2S(N_EMBED, N_UNITS, length(de_vocab), length(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa6c7d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 200 epochs with 2 minibatches/epoch.\n",
      "Evaluation is performed every 1 minibatches with 1 mbs.\n",
      "Watch the progress with TensorBoard at:\n",
      "/data/aNN/Helferlein/logs/de-en-gru/2022-02-17T18-14-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:16\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with:\n",
      "Training loss:       0.1967436671257019\n",
      "Training accuracy:   1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,16)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(6,14)), identity), Recurrent(6, 16, :gru, GRU(input=6,hidden=16), true), Recurrent(6, 16, :gru, GRU(input=6,hidden=16), true), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(14,16)), P(Knet.KnetArrays.KnetVector{Float32}(14)), identity), Dropout(0.1), 16, 14, 6, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_train!(s2s, Adam, dfun, split=nothing, epochs=200, tb_name=\"de-en-gru\",\n",
    "    acc_fun=hamming_acc,\n",
    "    mb_loss_freq=100, checkpoints=nothing, eval_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd52f1",
   "metadata": {},
   "source": [
    "We train for some seconds and define a last function, that helps to translate directly and test the RNN:   \n",
    "The function does:\n",
    "+ transform a sentence in the source language into a list of word-tokens, using\n",
    "  the source vocab.\n",
    "+ run the sequence througth the RNN\n",
    "+ use the target vocab to transform the sequence of tokens back into a sentence\n",
    "  in the target language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f904521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translate (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function translate(inp::T; mdl=s2s, sv=de_vocab, tv=en_vocab) where {T <: AbstractString}\n",
    "    \n",
    "    in_seq = sv(inp, split_words=true, add_ctls=false)\n",
    "    in_seq = reshape(in_seq, (:,1))\n",
    "    out_seq = mdl(in_seq)\n",
    "    return tv(out_seq)\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e9d5ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I love Julia <end>\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich liebe Julia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "679e2f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I always code Julia <end>\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich programmiere immer in Julia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6eecbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Peter loves Python <end>\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Peter liebt Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1614d601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I love Julia <end>\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Wir alle lieben Julia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d76bfd",
   "metadata": {},
   "source": [
    "### More realistic data from Tatoeba:\n",
    "\n",
    "It is not at all surprising that our rnn is able to memorise 4 sentences - the example \n",
    "is just a check for the s2s-network and the tools.\n",
    "\n",
    "As *NNHelferlein* provides direct access to Tatoeba data, we can train a rnn on a larger\n",
    "dataset. The Tatoeba German-English corpus includes about 250000 sentences an can be \n",
    "easily accesses as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eca0d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir = normpath(joinpath(dirname(pathof(#= /root/.julia/packages/NNHelferlein/GEtSz/src/texts.jl:314 =# @__MODULE__())), \"..\", \"data\", \"Tatoeba\")) = \"/root/.julia/packages/NNHelferlein/GEtSz/data/Tatoeba\"\n",
      "pathname = joinpath(dir, fname) = \"/root/.julia/packages/NNHelferlein/GEtSz/data/Tatoeba/deu-eng.zip\"\n",
      "Corpus for language deu is already downloaded.\n",
      "Reading Tatoeba corpus for languages en-deu\n",
      "\r",
      "importing sentences: 1000\r",
      "importing sentences: 2000\r",
      "importing sentences: 3000\r",
      "importing sentences: 4000\r",
      "importing sentences: 5000\r",
      "importing sentences: 6000\r",
      "importing sentences: 7000\r",
      "importing sentences: 8000\r",
      "importing sentences: 9000\r",
      "importing sentences: 10000\r",
      "importing sentences: 11000\r",
      "importing sentences: 12000\r",
      "importing sentences: 13000\r",
      "importing sentences: 14000\r",
      "importing sentences: 15000\r",
      "importing sentences: 16000\r",
      "importing sentences: 17000\r",
      "importing sentences: 18000\r",
      "importing sentences: 19000\r",
      "importing sentences: 20000\r",
      "importing sentences: 21000\r",
      "importing sentences: 22000\r",
      "importing sentences: 23000\r",
      "importing sentences: 24000\r",
      "importing sentences: 25000\r",
      "importing sentences: 26000\r",
      "importing sentences: 27000\r",
      "importing sentences: 28000\r",
      "importing sentences: 29000\r",
      "importing sentences: 30000\r",
      "importing sentences: 31000\r",
      "importing sentences: 32000\r",
      "importing sentences: 33000\r",
      "importing sentences: 34000\r",
      "importing sentences: 35000\r",
      "importing sentences: 36000\r",
      "importing sentences: 37000\r",
      "importing sentences: 38000\r",
      "importing sentences: 39000\r",
      "importing sentences: 40000\r",
      "importing sentences: 41000\r",
      "importing sentences: 42000\r",
      "importing sentences: 43000\r",
      "importing sentences: 44000\r",
      "importing sentences: 45000\r",
      "importing sentences: 46000\r",
      "importing sentences: 47000\r",
      "importing sentences: 48000\r",
      "importing sentences: 49000\r",
      "importing sentences: 50000\r",
      "importing sentences: 51000\r",
      "importing sentences: 52000\r",
      "importing sentences: 53000\r",
      "importing sentences: 54000\r",
      "importing sentences: 55000\r",
      "importing sentences: 56000\r",
      "importing sentences: 57000\r",
      "importing sentences: 58000\r",
      "importing sentences: 59000\r",
      "importing sentences: 60000\r",
      "importing sentences: 61000\r",
      "importing sentences: 62000\r",
      "importing sentences: 63000\r",
      "importing sentences: 64000\r",
      "importing sentences: 65000\r",
      "importing sentences: 66000\r",
      "importing sentences: 67000\r",
      "importing sentences: 68000\r",
      "importing sentences: 69000\r",
      "importing sentences: 70000\r",
      "importing sentences: 71000\r",
      "importing sentences: 72000\r",
      "importing sentences: 73000\r",
      "importing sentences: 74000\r",
      "importing sentences: 75000\r",
      "importing sentences: 76000\r",
      "importing sentences: 77000\r",
      "importing sentences: 78000\r",
      "importing sentences: 79000\r",
      "importing sentences: 80000\r",
      "importing sentences: 81000\r",
      "importing sentences: 82000\r",
      "importing sentences: 83000\r",
      "importing sentences: 84000\r",
      "importing sentences: 85000\r",
      "importing sentences: 86000\r",
      "importing sentences: 87000\r",
      "importing sentences: 88000\r",
      "importing sentences: 89000\r",
      "importing sentences: 90000\r",
      "importing sentences: 91000\r",
      "importing sentences: 92000\r",
      "importing sentences: 93000\r",
      "importing sentences: 94000\r",
      "importing sentences: 95000\r",
      "importing sentences: 96000\r",
      "importing sentences: 97000\r",
      "importing sentences: 98000\r",
      "importing sentences: 99000\r",
      "importing sentences: 100000\r",
      "importing sentences: 101000\r",
      "importing sentences: 102000\r",
      "importing sentences: 103000\r",
      "importing sentences: 104000\r",
      "importing sentences: 105000\r",
      "importing sentences: 106000\r",
      "importing sentences: 107000\r",
      "importing sentences: 108000\r",
      "importing sentences: 109000\r",
      "importing sentences: 110000\r",
      "importing sentences: 111000\r",
      "importing sentences: 112000\r",
      "importing sentences: 113000\r",
      "importing sentences: 114000\r",
      "importing sentences: 115000\r",
      "importing sentences: 116000\r",
      "importing sentences: 117000\r",
      "importing sentences: 118000\r",
      "importing sentences: 119000\r",
      "importing sentences: 120000\r",
      "importing sentences: 121000\r",
      "importing sentences: 122000\r",
      "importing sentences: 123000\r",
      "importing sentences: 124000\r",
      "importing sentences: 125000\r",
      "importing sentences: 126000\r",
      "importing sentences: 127000\r",
      "importing sentences: 128000\r",
      "importing sentences: 129000\r",
      "importing sentences: 130000\r",
      "importing sentences: 131000\r",
      "importing sentences: 132000\r",
      "importing sentences: 133000\r",
      "importing sentences: 134000\r",
      "importing sentences: 135000\r",
      "importing sentences: 136000\r",
      "importing sentences: 137000\r",
      "importing sentences: 138000\r",
      "importing sentences: 139000\r",
      "importing sentences: 140000\r",
      "importing sentences: 141000\r",
      "importing sentences: 142000\r",
      "importing sentences: 143000\r",
      "importing sentences: 144000\r",
      "importing sentences: 145000\r",
      "importing sentences: 146000\r",
      "importing sentences: 147000\r",
      "importing sentences: 148000\r",
      "importing sentences: 149000\r",
      "importing sentences: 150000\r",
      "importing sentences: 151000\r",
      "importing sentences: 152000\r",
      "importing sentences: 153000\r",
      "importing sentences: 154000\r",
      "importing sentences: 155000\r",
      "importing sentences: 156000\r",
      "importing sentences: 157000\r",
      "importing sentences: 158000\r",
      "importing sentences: 159000\r",
      "importing sentences: 160000\r",
      "importing sentences: 161000\r",
      "importing sentences: 162000\r",
      "importing sentences: 163000\r",
      "importing sentences: 164000\r",
      "importing sentences: 165000\r",
      "importing sentences: 166000\r",
      "importing sentences: 167000\r",
      "importing sentences: 168000\r",
      "importing sentences: 169000\r",
      "importing sentences: 170000\r",
      "importing sentences: 171000\r",
      "importing sentences: 172000\r",
      "importing sentences: 173000\r",
      "importing sentences: 174000\r",
      "importing sentences: 175000\r",
      "importing sentences: 176000\r",
      "importing sentences: 177000\r",
      "importing sentences: 178000\r",
      "importing sentences: 179000\r",
      "importing sentences: 180000\r",
      "importing sentences: 181000\r",
      "importing sentences: 182000\r",
      "importing sentences: 183000\r",
      "importing sentences: 184000\r",
      "importing sentences: 185000\r",
      "importing sentences: 186000\r",
      "importing sentences: 187000\r",
      "importing sentences: 188000\r",
      "importing sentences: 189000\r",
      "importing sentences: 190000\r",
      "importing sentences: 191000\r",
      "importing sentences: 192000\r",
      "importing sentences: 193000\r",
      "importing sentences: 194000\r",
      "importing sentences: 195000\r",
      "importing sentences: 196000\r",
      "importing sentences: 197000\r",
      "importing sentences: 198000\r",
      "importing sentences: 199000\r",
      "importing sentences: 200000\r",
      "importing sentences: 201000\r",
      "importing sentences: 202000\r",
      "importing sentences: 203000\r",
      "importing sentences: 204000\r",
      "importing sentences: 205000\r",
      "importing sentences: 206000\r",
      "importing sentences: 207000\r",
      "importing sentences: 208000\r",
      "importing sentences: 209000\r",
      "importing sentences: 210000\r",
      "importing sentences: 211000\r",
      "importing sentences: 212000\r",
      "importing sentences: 213000\r",
      "importing sentences: 214000\r",
      "importing sentences: 215000\r",
      "importing sentences: 216000\r",
      "importing sentences: 217000\r",
      "importing sentences: 218000\r",
      "importing sentences: 219000\r",
      "importing sentences: 220000\r",
      "importing sentences: 221000\r",
      "importing sentences: 222000\r",
      "importing sentences: 223000\r",
      "importing sentences: 224000\r",
      "importing sentences: 225000\r",
      "importing sentences: 226000\r",
      "importing sentences: 227000\r",
      "importing sentences: 228000\r",
      "importing sentences: 229000\r",
      "importing sentences: 230000\r",
      "importing sentences: 231000\r",
      "importing sentences: 232000\r",
      "importing sentences: 233000\r",
      "importing sentences: 234000\r",
      "importing sentences: 235000\r",
      "importing sentences: 236000\r",
      "importing sentences: 237000\r",
      "importing sentences: 238000\r",
      "importing sentences: 239000\r",
      "importing sentences: 240000\r",
      "importing sentences: 241000\r",
      "importing sentences: 242000\r",
      "importing sentences: 243000\r",
      "importing sentences: 244000\r",
      "importing sentences: 245000\r",
      "importing sentences: 246000\r",
      "importing sentences: 247000\r",
      "importing sentences: 248000\r",
      "importing sentences: 249000"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SequenceData(Any[(Int32[3611 14859 … 5 5; 2 2 … 4029 4310], Int32[1 1 … 1 1; 1933 1933 … 6 6; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[5 5224 … 2269 487; 475 5 … 4790 3624], Int32[1 1 … 1 1; 6 3759 … 246 246; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[2269 2269 … 15159 788; 246 246 … 7 1387], Int32[1 1 … 1 1; 246 246 … 90 90; … ; 2 2 … 5883 1041; 2 2 … 2 2]), (Int32[7702 1822 … 5070 7131; 7 3259 … 29 5], Int32[1 1 … 1 1; 90 90 … 7793 4531; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[818 34 … 818 5988; 750 950 … 4046 112], Int32[1 1 … 1 1; 4266 29 … 7970 4412; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[5020 4283 … 302 302; 112 29 … 3927 727], Int32[1 1 … 1 1; 4412 165 … 426 426; … ; 2 18 … 2 2; 2 2 … 2 2]), (Int32[1954 2459 … 5 5; 14886 26 … 1618 2319], Int32[1 1 … 1 1; 16382 5777 … 6 6; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[5 2135 … 5 5; 4433 1430 … 480 8352], Int32[1 1 … 1 1; 6 1157 … 6 6; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[5 5 … 757 10; 4800 7479 … 2257 2810], Int32[1 1 … 1 1; 6 6 … 383 55; … ; 2 2 … 2 2; 2 2 … 2 2]), (Int32[10 1195 … 302 302; 15543 125 … 2134 2874], Int32[1 1 … 1 1; 55 6195 … 663 663; … ; 2 2 … 2 2; 2 2 … 2 2])  …  (Int32[6 6 … 5 5; 93 1513 … 2052 2052; … ; 51 14 … 674 674; 214 18 … 2077 2077], Int32[1 1 … 1 1; 5 5 … 6 6; … ; 27 797 … 669 669; 2 2 … 2 2]), (Int32[5 5 … 68 52; 1125 183 … 2744 2053; … ; 35669 18 … 14 3715; 105 20 … 628 20], Int32[1 1 … 1 1; 6 6 … 25 25; … ; 34 11 … 8 11; 2 2 … 2 2]), (Int32[52 68 … 6 6; 9603 9505 … 1187 1187; … ; 1379 48 … 1256 1256; 1274 5041 … 77 40], Int32[1 1 … 1 1; 25 25 … 5 5; … ; 88 9 … 4420 4420; 2 2 … 2 2]), (Int32[6 664 … 36036 52; 1187 246 … 8 1304; … ; 1256 7 … 6535 8281; 13 405 … 19060 27], Int32[1 1 … 1 1; 5 5 … 2607 25; … ; 4420 28 … 1223 7675; 2 2 … 2 2]), (Int32[68 1623 … 148 33853; 29751 36 … 13 83; … ; 49 99 … 127 20665; 155 33907 … 65 7032], Int32[1 1 … 1 1; 25 86 … 168 16300; … ; 16 20 … 34 113; 2 2 … 2 2]), (Int32[35 3562 … 148 5; 86 171 … 5 32; … ; 459 973 … 7066 102; 3759 16 … 197 280], Int32[1 1 … 1 1; 38 38 … 25 6; … ; 9 99 … 4865 96; 2 2 … 2 2]), (Int32[5 5 … 23 23; 191 544 … 24253 2417; … ; 17077 14 … 14 101; 282 18 … 265 156], Int32[1 1 … 1 1; 6 6 … 25 25; … ; 19234 1020 … 14 664; 2 2 … 2 2]), (Int32[117 34 … 16905 6; 577 1464 … 45 21; … ; 56 5 … 19846 131; 36 28 … 430 21], Int32[1 1 … 1 1; 6 29 … 18851 909; … ; 64 6 … 4104 8624; 2 2 … 2 2]), (Int32[23 35 … 148 9719; 31137 28 … 120 120; … ; 1515 1341 … 2724 60; 25849 49 … 2016 120], Int32[1 1 … 1 1; 25 25 … 168 168; … ; 12580 16 … 177 177; 2 2 … 2 2]), (Int32[1315 5 … 6 803; 139 1129 … 71 1522; … ; 27 20 … 36409 1166; 19856 27628 … 18 209], Int32[1 1 … 1 1; 168 6 … 5 13306; … ; 177 1597 … 32 9; 2 2 … 2 2])], 1940, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940], true), WordTokenizer(40982, Dict{String, Int32}(\"hereinbat\" => 22961, \"Egozentriker\" => 16918, \"Wörterbuches\" => 22962, \"hundertzwanzig\" => 22963, \"null\" => 8546, \"schmerzlicher\" => 22964, \"Zweiten\" => 6235, \"kandidiert\" => 16919, \"gleichseitiges\" => 22965, \"Bio-Lebensmittel\" => 13794…), [\"<start>\", \"<end>\", \"<pad>\", \"<unknown>\", \"Tom\", \"Ich\", \"nicht\", \"ist\", \"zu\", \"Sie\"  …  \"Wohnheim\", \"schnorcheln\", \"gewürztes\", \"Zirkel\", \"Löffelchen\", \"Spritzer\", \"Kokons\", \"sträubt\", \"verzweifeln\", \"Lampenschirm\"]), WordTokenizer(19288, Dict{String, Int32}(\"irreplaceable\" => 5057, \"waster\" => 8879, \"inattentive\" => 13094, \"frowning\" => 13095, \"sleepwalking\" => 10465, \"dumber\" => 10466, \"Secure\" => 13096, \"Tuberculosis\" => 13097, \"melons\" => 7824, \"gout\" => 13098…), [\"<start>\", \"<end>\", \"<pad>\", \"<unknown>\", \"I\", \"Tom\", \"to\", \"you\", \"the\", \"t\"  …  \"forties\", \"non-violent\", \"Nozomi\", \"Phase\", \"roaches\", \"petting\", \"smuggled\", \"immunization\", \"devotes\", \"wavering\"]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en, de = get_tatoeba_corpus(\"deu\")\n",
    "en = en[1000:end]; de = de[1000:end]\n",
    "dtato, de_vocab, en_vocab = prepare_corpus(de, en, batchsize=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b1f48",
   "metadata": {},
   "source": [
    "For the more realistic training data still single layer of 512 LSTM units is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cac0724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,40982)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,19288)), identity), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512), true), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512), true), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(19288,512)), P(Knet.KnetArrays.KnetVector{Float32}(19288)), identity), Dropout(0.1), 40982, 19288, 1024, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EMBED = 1024\n",
    "N_UNITS = 512\n",
    "s2s = S2S(N_EMBED, N_UNITS, length(de_vocab), length(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4a8a778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset for training (90%) and validation (10%).\n",
      "Training 20 epochs with 1746 minibatches/epoch and 194 validation mbs.\n",
      "Evaluation is performed every 350 minibatches with 39 mbs.\n",
      "Watch the progress with TensorBoard at:\n",
      "/data/aNN/Helferlein/logs/de-en-gru/2022-02-17T18-20-34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:22:41\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with:\n",
      "Training loss:       0.15276523946516668\n",
      "Training accuracy:   0.9048155519348606\n",
      "Validation loss:     0.1482027831208921\n",
      "Validation accuracy: 0.9093793786307477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,40982)), identity), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(1024,19288)), identity), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512), true), Recurrent(1024, 512, :gru, GRU(input=1024,hidden=512), true), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(19288,512)), P(Knet.KnetArrays.KnetVector{Float32}(19288)), identity), Dropout(0.1), 40982, 19288, 1024, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_train!(s2s, Adam, dtato, epochs=20, tb_name=\"de-en-gru\",\n",
    "    split=0.9, eval_freq=5, eval_size=0.2, \n",
    "    acc_fun=hamming_acc, mb_loss_freq=1000, checkpoints=nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc149ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Tom usually listens to classical music <end>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Tom hört gewöhnlich klassische Musik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b59f5a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Tom almost always wears dark clothes <end>\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Tom trägt fast immer dunkle Kleidung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8130660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> How much beer should I buy <end>\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Wie viel Bier soll ich kaufen?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d54dc1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I need to get some shut-eye <end>\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich brauche eine Mütze voll Schlaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "419994be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> I need to drink more coffee <end>\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Ich muss mehr Kaffee trinken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3f8e9bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> Tom needs to drink more coffee <end>\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Tom muss mehr Kaffee trinken\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (8 threads) 1.7.0",
   "language": "julia",
   "name": "julia-(8-threads)-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
