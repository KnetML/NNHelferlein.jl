{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer API\n",
    "\n",
    "Simple transformers \n",
    "can be built with *NNHelferlein* s transformer type. The Implementation follows \n",
    "the *Vaswani, 2017* paper (fig. from *Vaswani et al. NIPS (2017)* http://arxiv.org/abs/1706.03762 ) an dis wrapped into the types \n",
    "`NNHelferlein.Transformer` and `NNHelferlein.TokenTransformer`:\n",
    "\n",
    "<img src=\"assets/80-vaswani-fig-1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Knet, NNHelferlein\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground data\n",
    "\n",
    "For the experiments a tiny but endearing dataset is used and prepared with *NNHelferlein* tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Vector{Int32}}:\n",
       " [1, 7, 9, 6, 2, 3, 3, 3]\n",
       " [1, 10, 5, 13, 2, 3, 3, 3]\n",
       " [1, 16, 5, 14, 11, 2, 3, 3]\n",
       " [1, 7, 12, 8, 15, 6, 2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de = [\"Ich liebe Julia\",\n",
    "      \"Peter liebt Python\",\n",
    "      \"Susi liebt sie alle\",\n",
    "      \"Ich programmiere immer in Julia\"]\n",
    "en = [\"I love Julia\",\n",
    "      \"Peter loves Python\",\n",
    "      \"Susi loves them all\",\n",
    "      \"I always code Julia\"]\n",
    "\n",
    "de_vocab = WordTokenizer(de)\n",
    "d = de_vocab(de, add_ctls=true)\n",
    "d = pad_sequence.(d, 8)\n",
    "d = truncate_sequence.(d, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Vector{Int32}}:\n",
       " [1, 7, 10, 5, 2, 3, 3, 3]\n",
       " [1, 9, 6, 11, 2, 3, 3, 3]\n",
       " [1, 12, 6, 14, 13, 2, 3, 3]\n",
       " [1, 7, 15, 8, 5, 2, 3, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab = WordTokenizer(en)\n",
    "e = en_vocab(en, add_ctls=true)\n",
    "e = pad_sequence.(e, 8)\n",
    "e = truncate_sequence.(e, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length(mbs) = 2\n",
      "x = Int32[1 1; 7 10; 9 5; 6 13; 2 2; 3 3; 3 3; 3 3]\n",
      "y = Int32[1 1; 7 9; 10 6; 5 11; 2 2; 3 3; 3 3; 3 3]\n"
     ]
    }
   ],
   "source": [
    "mbs = sequence_minibatch(d, e, 2)\n",
    "x,y = first(mbs)\n",
    "@show length(mbs)\n",
    "@show x\n",
    "@show y;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer\n",
    "\n",
    "Transformes can be constructed with the types `NNHelferlein.Transformer` and\n",
    "`NNHelferlein.TokenTransformer`. The first is more general and expects tensors\n",
    "of embedded data as input. The `TokenTransformer` works on sequences of\n",
    "Integer tokens.\n",
    "\n",
    "We set up a `TokenTransformer` with 5 layers, an embedding depth of 128 \n",
    "and 4 heads. The size of the vocabulatory can be defined by the\n",
    "vocab-objects of type `WordTokenizer`. We briefly test it with the first minibatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt =  TokenTransformer(5, 128, 4, de_vocab, en_vocab, drop_rate=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(tt(x, y)) = (15, 8, 2)\n",
      "size(tt.α) = (8, 8, 4, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8×2 Matrix{Int64}:\n",
       " 9  9\n",
       " 9  9\n",
       " 9  9\n",
       " 9  9\n",
       " 9  9\n",
       " 9  9\n",
       " 9  9\n",
       " 9  9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show size(tt(x,y))       # raw output\n",
    "@show size(tt.α)          # attention factors (for 4 heads)\n",
    "tt(x,y, embedded=false)   # generated sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signatures for training and prediction:\n",
    "\n",
    "Addistional signatures are necessary for training and prediction. The methods\n",
    "adds functionality for\n",
    "+ creation of padding masks\n",
    "+ shifting in- and out-sequences by one, to be able to train the *next*\n",
    "  position of the sequence\n",
    "+ loss calculation for training\n",
    "\n",
    "*NNHelferlein* default encoding of the `WordTokenizer` is used for `<start>`, `<end>` and `<pad>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct AllYouNeed\n",
    "    t::TokenTransformer\n",
    "    vocab_enc\n",
    "    vocab_dec\n",
    "    \n",
    "    AllYouNeed(n_layers, depth, heads, x_vocab, y_vocab; drop_rate=0.1) = \n",
    "        new(TokenTransformer(n_layers, depth, heads, x_vocab, y_vocab; drop_rate),\n",
    "        x_vocab,\n",
    "        y_vocab)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (ayn::AllYouNeed)(x,y)   # calc loss\n",
    "    \n",
    "    y_in = y[1:end-1,:]       # shift y against teaching output\n",
    "    y_teach = y[2:end,:]\n",
    "        \n",
    "    x_mask = mk_padding_mask(x)\n",
    "    y_mask = mk_padding_mask(y_in)\n",
    "        \n",
    "    o = ayn.t(x, y_in)\n",
    "        \n",
    "    o_mask = (mk_padding_mask(y_teach) .== 0.0) |> Array{Float32}\n",
    "    y_m = y_teach .* o_mask .|> Int   # make class ID 0 for padded positions\n",
    "    loss = nll(o, y_m, average=true)  # Xentropy loss of unmasked positions only\n",
    "    \n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7183423f0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate = AllYouNeed(5, 128, 4, de_vocab, en_vocab, drop_rate=0.1)\n",
    "translate(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Calculating a meaningful accuracy is a little bit tricky for transformers, because\n",
    "target sequence *in* and *out* are shiftet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tt_acc (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tt_acc(mdl; data=nothing)\n",
    "\n",
    "    tac = Float32(0.0)\n",
    "    for (x,y) in data\n",
    "        y_in = y[1:end-1,:]\n",
    "        y_teach = y[2:end,:]\n",
    "        o = mdl.t(x, y_in, embedded=false)\n",
    "\n",
    "        tac += hamming_acc(o, y_teach, vocab=mdl.vocab_dec)\n",
    "    end\n",
    "\n",
    "    return tac / length(data)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_acc(translate, data=mbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 100 epochs with 2 minibatches/epoch.\n",
      "Evaluation is performed every 2 minibatches with 2 mbs.\n",
      "Watch the progress with TensorBoard at:\n",
      "/home/andreas/Documents/Projekte/2022-NNHelferlein_KnetML/NNHelferlein/examples/logs/I_love_WARMUP/2023-05-12T14-44-55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  21%|████████▋                                |  ETA: 0:02:56\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting learning rate to η=5.00e-05 in epoch 20.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  40%|████████████████▋                        |  ETA: 0:01:12\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting learning rate to η=1.00e-04 in epoch 40.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  60%|████████████████████████▋                |  ETA: 0:00:34\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting learning rate to η=1.50e-04 in epoch 60.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  80%|████████████████████████████████▋        |  ETA: 0:00:14\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting learning rate to η=2.00e-04 in epoch 80.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:54\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with:\n",
      "Training loss:       1.2147009\n",
      "Training accuracy:   0.4375\n",
      "Training 300 epochs with 2 minibatches/epoch.\n",
      "Evaluation is performed every 2 minibatches with 2 mbs.\n",
      "Watch the progress with TensorBoard at:\n",
      "/home/andreas/Documents/Projekte/2022-NNHelferlein_KnetML/NNHelferlein/examples/logs/I_love_TRAIN/2023-05-12T14-45-53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  20%|████████▎                                |  ETA: 0:00:24\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting learning rate to η=7.75e-05 in epoch 60.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  40%|████████████████▍                        |  ETA: 0:00:18\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting learning rate to η=5.50e-05 in epoch 120.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  60%|████████████████████████▋                |  ETA: 0:00:12\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting learning rate to η=3.25e-05 in epoch 180.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  80%|████████████████████████████████▊        |  ETA: 0:00:06\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting learning rate to η=1.00e-05 in epoch 240.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:29\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with:\n",
      "Training loss:       0.014779562\n",
      "Training accuracy:   1.0\n"
     ]
    }
   ],
   "source": [
    "translate = AllYouNeed(5, 128, 4, de_vocab, en_vocab, drop_rate=0.1)\n",
    "\n",
    "ayn = tb_train!(translate, Adam, mbs, epochs=100,\n",
    "                lr=1e-9, lr_decay=2e-4, lrd_steps=5, lrd_linear=true,\n",
    "                tb_name=\"I_love_WARMUP\",\n",
    "                acc_fun=tt_acc, eval_size=1, eval_freq=1)\n",
    "ayn = tb_train!(translate, Adam, mbs, epochs=300,\n",
    "                lr=1e-4, lr_decay=1e-5, lrd_steps=5, lrd_linear=true,\n",
    "                tb_name=\"I_love_TRAIN\",\n",
    "                acc_fun=tt_acc, eval_size=1, eval_freq=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... of course, this is just the proof, that the transformer \n",
    "can overfit a small dataset.\n",
    "\n",
    "Please have a look at the example `80-transformer.jpynb` to see how to\n",
    "work with a more realistic dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
